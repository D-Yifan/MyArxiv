<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-13T00:00:00Z">2023-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-tuning Via <span class="highlight-title">Prompt</span>s Makes NLP Models Adversarially Robust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations, such
as word-level synonym substitutions. In this work, we demonstrate surprising
gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an
alternative method of adapting to downstream tasks. Rather than modifying the
model (by appending an MLP head), MVP instead modifies the input (by appending
a prompt template). Across three classification datasets, MVP improves
performance against adversarial word-level synonym substitutions by an average
of 8% over standard methods and even outperforms adversarial training-based
state-of-art defenses by 3.5%. By combining MVP with adversarial training, we
achieve further improvements in robust accuracy while maintaining clean
accuracy. Finally, we conduct ablations to investigate the mechanism underlying
these gains. Notably, we find that the main causes of vulnerability of MLP can
be attributed to the misalignment between pre-training and fine-tuning tasks,
and the randomly initialized MLP parameters. Code is available at
https://github.com/acmi-lab/mvp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meet in the Middle: A New <span class="highlight-title">Pre-train</span>ing Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Nguyen, Nikos Karampatziakis, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based approaches to Sentiment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olumide Ebenezer Ojo, Hoang Thang Ta, Alexander Gelbukh, Hiram Calvo, Olaronke Oluwayemisi Adebanji, Grigori Sidorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of transfer learning methods is largely responsible for the present
breakthrough in Natural Learning Processing (NLP) tasks across multiple
domains. In order to solve the problem of sentiment detection, we examined the
performance of four different types of well-known state-of-the-art transformer
models for text classification. Models such as Bidirectional Encoder
Representations from Transformers (BERT), Robustly Optimized BERT Pre-training
Approach (RoBERTa), a distilled version of BERT (DistilBERT), and a large
bidirectional neural network architecture (XLNet) were proposed. The
performance of the four models that were used to detect disaster in the text
was compared. All the models performed well enough, indicating that
transformer-based models are suitable for the detection of disaster in text.
The RoBERTa transformer model performs best on the test dataset with a score of
82.6% and is highly recommended for quality predictions. Furthermore, we
discovered that the learning algorithms' performance was influenced by the
pre-processing techniques, the nature of words in the vocabulary, unbalanced
labeling, and the model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publisher: Springer Nature Switzerland AG, Gewerbestrasse 11, 6330
  Cham, Switzerland Published in Book Titled: Recent Developments and the New
  Directions of Research, Foundations, and Applications: Selected Papers of the
  8th World Conference on Soft Computing, February 03-05, 2022, Baku,
  Azerbaijan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of
  Synthetic and Compositional Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Models Trained on Indian Legal Data Fair? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Girhepuje, Anmol Goel, Gokul Krishnan, Shreya Goyal, Satyendra Pandey, Ponnurangam Kumaraguru, Balaram Ravindran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances and applications of language technology and artificial
intelligence have enabled much success across multiple domains like law,
medical and mental health. AI-based Language Models, like Judgement Prediction,
have recently been proposed for the legal sector. However, these models are
strife with encoded social biases picked up from the training data. While bias
and fairness have been studied across NLP, most studies primarily locate
themselves within a Western context. In this work, we present an initial
investigation of fairness from the Indian perspective in the legal domain. We
highlight the propagation of learnt algorithmic biases in the bail prediction
task for models trained on Hindi legal documents. We evaluate the fairness gap
using demographic parity and show that a decision tree model trained for the
bail prediction task has an overall fairness disparity of 0.237 between input
features associated with Hindus and Muslims. Additionally, we highlight the
need for further research and studies in the avenues of fairness/bias in
applying AI in the legal sector with a specific focus on the Indian context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Symposium on AI and Law (SAIL) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision-Language Models with Sparse Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Empirical Evaluation of Existing Word Embedding
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obaidullah Zaland, Muhammad Abulaish, Mohd. Fazil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-based word representations help countless Natural Language Processing
(NLP) tasks capture both semantic and syntactic regularities of the language.
In this paper, we present the characteristics of existing word embedding
approaches and analyze them with regards to many classification tasks. We
categorize the methods into two main groups - Traditional approaches mostly use
matrix factorization to produce word representations, and they are not able to
capture the semantic and syntactic regularities of the language very well.
Neural-Network based approaches, on the other hand, can capture sophisticated
regularities of the language and preserve the word relationships in the
generated word representations. We report experimental results on multiple
classification tasks and highlight the scenarios where one approach performs
better than the rest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroQL: A Neuro-Symbolic Language and <span class="highlight-title">Dataset</span> for Inter-Subjective
  <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Papoulias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new AI task and baseline solution for Inter-Subjective
Reasoning. We define inter-subjective information, to be a mixture of objective
and subjective information possibly shared by different parties. Examples may
include commodities and their objective properties as reported by IR
(Information Retrieval) systems, that need to be cross-referenced with
subjective user reviews from an online forum. For an AI system to successfully
reason about both, it needs to be able to combine symbolic reasoning of
objective facts with the shared consensus found on subjective user reviews. To
this end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as
a baseline solution for this problem. NeuroQL is a neuro-symbolic language that
extends logical unification with neural primitives for extraction and
retrieval. It can function as a target for automatic translation of
inter-subjective questions (posed in natural language) into the neuro-symbolic
code that can answer them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models in the Workplace: A Case Study on <span class="highlight-title">Prompt</span>
  Engineering for Job Type Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clavié, Alexandru Ciceu, Frederick Naylor, Guillaume Soulié, Thomas Brightwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This case study investigates the task of job classification in a real-world
setting, where the goal is to determine whether an English-language job posting
is appropriate for a graduate or entry-level position. We explore multiple
approaches to text classification, including supervised approaches such as
traditional models like Support Vector Machines (SVMs) and state-of-the-art
deep learning methods such as DeBERTa. We compare them with Large Language
Models (LLMs) used in both few-shot and zero-shot classification settings. To
accomplish this task, we employ prompt engineering, a technique that involves
designing prompts to guide the LLMs towards the desired output. Specifically,
we evaluate the performance of two commercially available state-of-the-art
GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also
conduct a detailed analysis of the impact of different aspects of prompt
engineering on the model's performance. Our results show that, with a
well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all
other models, achieving a 6% increase in Precision@95% Recall compared to the
best supervised approach. Furthermore, we observe that the wording of the
prompt is a critical factor in eliciting the appropriate "reasoning" in the
model, and that seemingly minor aspects of the prompt significantly affect the
model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating multiple-choice questions for medical question answering with
  distractors and cue-masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien Sileo, Kanimozhi Uma, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical multiple-choice question answering (MCQA) is particularly difficult.
Questions may describe patient symptoms and ask for the correct diagnosis,
which requires domain knowledge and complex reasoning. Standard language
modeling pretraining alone is not sufficient to achieve the best results.
\citet{jin2020disease} showed that focusing masked language modeling on disease
name prediction when using medical encyclopedic paragraphs as input leads to
considerable MCQA accuracy improvement. In this work, we show that (1)
fine-tuning on generated MCQA dataset outperforms the masked language modeling
based objective and (2) correctly masking the cues to the answers is critical
for good performance. We release new pretraining datasets and achieve
state-of-the-art results on 4 MCQA datasets, notably +5.7\% with base-size
model on MedQA-USMLE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Biases in the Texts using an End-to-End Pipeline Approach <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Syed Raza Bashir,  Sneha, Urooj Qamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of fairness is gaining popularity in academia and industry.
Social media is especially vulnerable to media biases and toxic language and
comments. We propose a fair ML pipeline that takes a text as input and
determines whether it contains biases and toxic content. Then, based on
pre-trained word embeddings, it suggests a set of new words by substituting the
bi-ased words, the idea is to lessen the effects of those biases by replacing
them with alternative words. We compare our approach to existing fairness
models to determine its effectiveness. The results show that our proposed
pipeline can de-tect, identify, and mitigate biases in social media data
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Bias @ ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Human Subject Study of Named Entity Recognition (NER) in
  <span class="highlight-title">Conversation</span>al Music Recommendation Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena V. Epure, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conducted a human subject study of named entity recognition on a noisy
corpus of conversational music recommendation queries, with many irregular and
novel named entities. We evaluated the human NER linguistic behaviour in these
challenging conditions and compared it with the most common NER systems
nowadays, fine-tuned transformers. Our goal was to learn about the task to
guide the design of better evaluation methods and NER algorithms. The results
showed that NER in our context was quite hard for both human and algorithms
under a strict evaluation schema; humans had higher precision, while the model
higher recall because of entity exposure especially during pre-training; and
entity types had different error patterns (e.g. frequent typing errors for
artists). The released corpus goes beyond predefined frames of interaction and
can support future work in conversational music recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually-rich human affect perception using multimodal scene
  information <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of human affect understanding involves the ability to infer
person specific emotional states from various sources including images, speech,
and language. Affect perception from images has predominantly focused on
expressions extracted from salient face crops. However, emotions perceived by
humans rely on multiple contextual cues including social settings, foreground
interactions, and ambient visual scenes. In this work, we leverage pretrained
vision-language (VLN) models to extract descriptions of foreground context from
images. Further, we propose a multimodal context fusion (MCF) module to combine
foreground cues with the visual scene and person-based contextual information
for emotion prediction. We show the effectiveness of our proposed modular
design on two datasets associated with natural scenes and TV shows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The System Description of dun_oscar team for The ICPR MSR Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Du, Rui Deng, Yingxin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the system submitted by dun_oscar team for the ICPR MSR
Challenge. Three subsystems for task1-task3 are descripted respectively. In
task1, we develop a visual system which includes a OCR model, a text tracker,
and a NLP classifier for distinguishing subtitles and non-subtitles. In task2,
we employ an ASR system which includes an AM with 18 layers and a 4-gram LM.
Semi-supervised learning on unlabeled data is also vital. In task3, we employ
the ASR system to improve the visual system, some false subtitles can be
corrected by a fusion module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing against Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of adversarial attacks, including targeted and
backdoor data poisoning attacks. Despite this vulnerability, robust contrastive
vision-language pretraining against adversarial attacks has remained
unaddressed. In this work, we propose RoCLIP, the first effective method for
robust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP
effectively breaks the association between poisoned image-caption pairs by
considering a pool of random examples, and (1) matching every image with the
text that is most similar to its caption in the pool, and (2) matching every
caption with the image that is most similar to its image in the pool. Our
extensive experiments show that our method renders state-of-the-art targeted
data poisoning and backdoor attacks ineffective during pre-training or
fine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor
attack success rates down to 0\% during pre-training and 1\%-4\% during
fine-tuning, and effectively improves the model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transductions and Alignments with RNN Seq2seq Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper studies the capabilities of Recurrent-Neural-Network sequence to
sequence (RNN seq2seq) models in learning four string-to-string transduction
tasks: identity, reversal, total reduplication, and input-specified
reduplication. These transductions are traditionally well studied under finite
state transducers and attributed with varying complexity. We find that RNN
seq2seq models are only able to approximate a mapping that fits the training or
in-distribution data. Attention helps significantly, but does not solve the
out-of-distribution generalization limitation. Task complexity and RNN variants
also play a role in the results. Our results are best understood in terms of
the complexity hierarchy of formal languages as opposed to that of string
transductions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 9 figures; 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Diarization with Non-autoregressive Intermediate Attractors <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Fujita, Tatsuya Komatsu, Robin Scheibler, Yusuke Kida, Tetsuji Ogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end neural diarization (EEND) with encoder-decoder-based attractors
(EDA) is a promising method to handle the whole speaker diarization problem
simultaneously with a single neural network. While the EEND model can produce
all frame-level speaker labels simultaneously, it disregards output label
dependency. In this work, we propose a novel EEND model that introduces the
label dependency between frames. The proposed method generates
non-autoregressive intermediate attractors to produce speaker labels at the
lower layers and conditions the subsequent layers with these labels. While the
proposed model works in a non-autoregressive manner, the speaker labels are
refined by referring to the whole sequence of intermediate labels. The
experiments with the two-speaker CALLHOME dataset show that the intermediate
labels with the proposed non-autoregressive intermediate attractors boost the
diarization performance. The proposed method with the deeper network benefits
more from the intermediate labels, resulting in better performance and training
throughput than EEND-EDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Single Items: Exploring User Preferences in Item Sets with the
  <span class="highlight-title">Conversation</span>al Playlist Curation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users in consumption domains, like music, are often able to more efficiently
provide preferences over a set of items (e.g. a playlist or radio) than over
single items (e.g. songs). Unfortunately, this is an underexplored area of
research, with most existing recommendation systems limited to understanding
preferences over single items. Curating an item set exponentiates the search
space that recommender systems must consider (all subsets of items!): this
motivates conversational approaches-where users explicitly state or refine
their preferences and systems elicit preferences in natural language-as an
efficient way to understand user needs. We call this task conversational item
set curation and present a novel data collection methodology that efficiently
collects realistic preferences about item sets in a conversational setting by
observing both item-level and set-level feedback. We apply this methodology to
music recommendation to build the Conversational Playlist Curation Dataset
(CPCD), where we show that it leads raters to express preferences that would
not be otherwise expressed. Finally, we propose a wide range of conversational
retrieval models as baselines for this task and evaluate them on the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Vision, Text, and Layout for Universal Document Processing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinical <span class="highlight-title">BERT</span>Score: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdapterSoup: Weight Averaging to Improve Generalization of <span class="highlight-title">Pretrain</span>ed
  Language Models <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, Jesse Dodge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) are trained on massive corpora, but often
need to specialize to specific domains. A parameter-efficient adaptation method
suggests training an adapter for each domain on the task of language modeling.
This leads to good in-domain scores but can be impractical for domain- or
resource-restricted settings. A solution is to use a related-domain adapter for
the novel domain at test time. In this paper, we introduce AdapterSoup, an
approach that performs weight-space averaging of adapters trained on different
domains. Our approach is embarrassingly parallel: first, we train a set of
domain-specific adapters; then, for each novel domain, we determine which
adapters should be averaged at test time. We present extensive experiments
showing that AdapterSoup consistently improves performance to new domains
without extra training. We also explore weight averaging of adapters trained on
the same domain with different hyper-parameters, and show that it preserves the
performance of a PLM on new domains while obtaining strong in-domain results.
We explore various approaches for choosing which adapters to combine, such as
text clustering and semantic similarity. We find that using clustering leads to
the most competitive results on novel domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023; camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        BigScience Workshop,  :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Transducer Training: Reduced Memory Consumption with Sample-wise
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Braun, Erik McDermott, Roger Hsiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The neural transducer is an end-to-end model for automatic speech recognition
(ASR). While the model is well-suited for streaming ASR, the training process
remains challenging. During training, the memory requirements may quickly
exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence
lengths. In this work, we analyze the time and space complexity of a typical
transducer training setup. We propose a memory-efficient training method that
computes the transducer loss and gradients sample by sample. We present
optimizations to increase the efficiency and parallelism of the sample-wise
method. In a set of thorough benchmarks, we show that our sample-wise method
significantly reduces memory usage, and performs at competitive speed when
compared to the default batched computation. As a highlight, we manage to
compute the transducer loss and gradients for a batch size of 1024, and audio
length of 40 seconds, using only 6 GB of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accidental Learners: Spoken Language Identification in Multilingual
  <span class="highlight-title">Self-Supervised</span> Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Travis M. Bartley, Fei Jia, Krishna C. Puvvada, Samuel Kriman, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we extend previous self-supervised approaches for language
identification by experimenting with Conformer based architecture in a
multilingual pre-training paradigm. We find that pre-trained speech models
optimally encode language discriminatory information in lower layers. Further,
we demonstrate that the embeddings obtained from these layers are significantly
robust to classify unseen languages and different acoustic environments without
additional training. After fine-tuning a pre-trained Conformer model on the
VoxLingua107 dataset, we achieve results similar to current state-of-the-art
systems for language identification. More, our model accomplishes this with 5x
less parameters. We open-source the model through the NVIDIA NeMo toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EDU-level Extractive Summarization with Varying Summary Lengths <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Wu, Ching-Hsun Tseng, Jiayu Shang, Shengzhong Mao, Goran Nenadic, Xiao-Jun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extractive models usually formulate text summarization as extracting fixed
top-$k$ salient sentences from the document as a summary. Few works exploited
extracting finer-grained Elementary Discourse Unit (EDU) with little analysis
and justification for the extractive unit selection. Further, the selection
strategy of the fixed top-$k$ salient sentences fits the summarization need
poorly, as the number of salient sentences in different documents varies and
therefore a common or best $k$ does not exist in reality. To fill these gaps,
this paper first conducts the comparison analysis of oracle summaries based on
EDUs and sentences, which provides evidence from both theoretical and
experimental perspectives to justify and quantify that EDUs make summaries with
higher automatic evaluation scores than sentences. Then, considering this merit
of EDUs, this paper further proposes an EDU-level extractive model with Varying
summary Lengths and develops the corresponding learning algorithm. EDU-VL
learns to encode and predict probabilities of EDUs in the document, generate
multiple candidate summaries with varying lengths based on various $k$ values,
and encode and score candidate summaries, in an end-to-end training manner.
Finally, EDU-VL is experimented on single and multi-document benchmark datasets
and shows improved performances on ROUGE scores in comparison with
state-of-the-art extractive models, and further human evaluation suggests that
EDU-constituent summaries maintain good grammaticality and readability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Wang, Minghui Qiu, Chen Shi, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun Huang, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Reasonability of the Test Set for Simultaneous Machine
  Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengge Liu, Wen Zhang, Xiang Li, Jian Luan, Bin Wang, Yuhang Guo, Shuoying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation (SimulMT) models start translation before
the end of the source sentence, making the translation monotonically aligned
with the source sentence. However, the general full-sentence translation test
set is acquired by offline translation of the entire source sentence, which is
not designed for SimulMT evaluation, making us rethink whether this will
underestimate the performance of SimulMT models. In this paper, we manually
annotate a monotonic test set based on the MuST-C English-Chinese test set,
denoted as SiMuST-C. Our human evaluation confirms the acceptability of our
annotated test set. Evaluations on three different SimulMT models verify that
the underestimation problem can be alleviated on our test set. Further
experiments show that finetuning on an automatically extracted monotonic
training set improves SimulMT models by up to 3 BLEU points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 48th IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Machine Translation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and adapt to
corrected translations in domain-specific projects. Machine translation (MT)
has achieved significant progress in the area of domain adaptation. However,
real-time adaptation remains challenging. Large-scale language models (LLMs)
have recently shown interesting capabilities of in-context learning, where they
learn to replicate certain input-output text generation patterns, without
further fine-tuning. By feeding an LLM at inference time with a prompt that
consists of a list of translation pairs, it can then simulate the domain and
style characteristics. This work aims to investigate how we can utilize
in-context learning to improve real-time adaptive MT. Our extensive experiments
show promising results at translation time. For example, GPT-3.5 can adapt to a
set of in-domain sentence pairs and/or terminology while translating a new
sentence. We observe that the translation quality with few-shot in-context
learning can surpass that of strong encoder-decoder MT systems, especially for
high-resource languages. Moreover, we investigate whether we can combine MT
from strong encoder-decoder models with fuzzy matches, which can further
improve translation quality, especially for less supported languages. We
conduct our experiments across five diverse language pairs, namely
English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French
(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image
  Captioning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng Zhang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning is a traditional vision-and-language task that aims to
generate the language description of an image. Recent studies focus on scaling
up the model size and the number of training data, which significantly increase
the cost of model training. Different to these heavy-cost models, we introduce
a lightweight image captioning framework (I-Tuning), which contains a small
number of trainable parameters. We design a novel I-Tuning cross-attention
module to connect the non-trainable pre-trained language decoder GPT2 and
vision encoder CLIP-ViT. Since most parameters are not required to be updated
during training, our framework is lightweight and fast. Experimental results
conducted on three image captioning benchmarks reveal that our framework
achieves comparable or better performance than the large-scale baseline
systems. But our models contain up to 10 times fewer trainable parameters and
require much fewer data for training compared with state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DailyTalk: Spoken <span class="highlight-title">Dialogue</span> <span class="highlight-title">Dataset</span> for <span class="highlight-title">Conversation</span>al Text-to-Speech <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01063v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01063v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon Lee, Kyumin Park, Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alternate Intermediate Conditioning with Syllable-level and
  Character-level Targets for Japanese ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Fujita, Tatsuya Komatsu, Yusuke Kida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition directly maps input speech to
characters. However, the mapping can be problematic when several different
pronunciations should be mapped into one character or when one pronunciation is
shared among many different characters. Japanese ASR suffers the most from such
many-to-one and one-to-many mapping problems due to Japanese kanji characters.
To alleviate the problems, we introduce explicit interaction between characters
and syllables using Self-conditioned connectionist temporal classification
(CTC), in which the upper layers are ``self-conditioned'' on the intermediate
predictions from the lower layers. The proposed method utilizes character-level
and syllable-level intermediate predictions as conditioning features to deal
with mutual dependency between characters and syllables. Experimental results
on Corpus of Spontaneous Japanese show that the proposed method outperformed
the conventional multi-task and Self-conditioned CTC methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Attention Networks Can Process Bounded Hierarchical Languages <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive performance in NLP, self-attention networks were
recently proved to be limited for processing formal languages with hierarchical
structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested
parentheses of $k$ types. This suggested that natural language can be
approximated well with models that are too weak for formal languages, or that
the role of hierarchy and recursion in natural language might be limited. We
qualify this implication by proving that self-attention networks can process
$\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by
$D$, which arguably better captures the bounded hierarchical structure of
natural language. Specifically, we construct a hard-attention network with
$D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes
$\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and
$O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show
that self-attention networks trained on $\mathsf{Dyck}_{k, D}$ generalize to
longer inputs with near-perfect accuracy, and also verify the theoretical
memory advantage of self-attention networks over recurrent networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2021. 19 pages with extended appendix. Fixed a small typo in the
  formula at the end of page 5 (thank to Gabriel Faria). Code:
  https://github.com/princeton-nlp/dyck-transformer</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Group Recommendation Based on a Probabilistic Semantic
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Dueñas-Lerín, Raúl Lara-Cabrera, Fernando Ortega, Jesús Bobadilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation to groups of users is a challenging subfield of recommendation
systems. Its key concept is how and where to make the aggregation of each set
of user information into an individual entity, such as a ranked recommendation
list, a virtual user, or a multi-hot input vector encoding. This paper proposes
an innovative strategy where aggregation is made in the multi-hot vector that
feeds the neural network model. The aggregation provides a probabilistic
semantic, and the resulting input vectors feed a model that is able to
conveniently generalize the group recommendation from the individual
predictions. Furthermore, using the proposed architecture, group
recommendations can be obtained by simply feedforwarding the pre-trained model
with individual ratings; that is, without the need to obtain datasets
containing group of user information, and without the need of running two
separate trainings (individual and group). This approach also avoids
maintaining two different models to support both individual and group learning.
Experiments have tested the proposed architecture using three representative
collaborative filtering datasets and a series of baselines; results show
suitable accuracy improvements compared to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Single Items: Exploring User Preferences in Item Sets with the
  <span class="highlight-title">Conversation</span>al Playlist Curation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users in consumption domains, like music, are often able to more efficiently
provide preferences over a set of items (e.g. a playlist or radio) than over
single items (e.g. songs). Unfortunately, this is an underexplored area of
research, with most existing recommendation systems limited to understanding
preferences over single items. Curating an item set exponentiates the search
space that recommender systems must consider (all subsets of items!): this
motivates conversational approaches-where users explicitly state or refine
their preferences and systems elicit preferences in natural language-as an
efficient way to understand user needs. We call this task conversational item
set curation and present a novel data collection methodology that efficiently
collects realistic preferences about item sets in a conversational setting by
observing both item-level and set-level feedback. We apply this methodology to
music recommendation to build the Conversational Playlist Curation Dataset
(CPCD), where we show that it leads raters to express preferences that would
not be otherwise expressed. Finally, we propose a wide range of conversational
retrieval models as baselines for this task and evaluate them on the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Class-Incremental Learning with <span class="highlight-title">Pre-Train</span>ed Models:
  Generalizability and Adaptivity are All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) aims to adapt to emerging new classes
without forgetting old ones. Traditional CIL models are trained from scratch to
continually acquire knowledge as data evolves. Recently, pre-training has
achieved substantial progress, making vast pre-trained models (PTMs) accessible
for CIL. Contrary to traditional methods, PTMs possess generalizable
embeddings, which can be easily transferred. In this work, we revisit CIL with
PTMs and argue that the core factors in CIL are adaptivity for model updating
and generalizability for knowledge transferring. 1) We first reveal that frozen
PTM can already provide generalizable embeddings for CIL. Surprisingly, a
simple baseline (SimpleCIL) which continually sets the classifiers of PTM to
prototype features can beat state-of-the-art even without training on the
downstream task. 2) Due to the distribution gap between pre-trained and
downstream datasets, PTM can be further cultivated with adaptivity via model
adapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of
PTM and adapted models for classifier construction. ADAM is a general framework
that can be orthogonally combined with any parameter-efficient tuning method,
which holds the advantages of PTM's generalizability and adapted model's
adaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the
era of PTM due to data overlapping and propose four new benchmarks for
assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive
experiments validate the effectiveness of ADAM with a unified and concise
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision Cross-entropy and EM Algorithm for Self-labeled Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwen Zhang, Yuri Boykov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose "collision cross-entropy" as a robust alternative to the Shannon's
cross-entropy in the context of self-labeled classification with posterior
models. Assuming unlabeled data, self-labeling works by estimating latent
pseudo-labels, categorical distributions y, that optimize some discriminative
clustering criteria, e.g. "decisiveness" and "fairness". All existing
self-labeled losses incorporate Shannon's cross-entropy term targeting the
model prediction, softmax, at the estimated distribution y. In fact, softmax is
trained to mimic the uncertainty in y exactly. Instead, we propose the negative
log-likelihood of "collision" to maximize the probability of equality between
two random variables represented by distributions softmax and y. We show that
our loss satisfies some properties of a generalized cross-entropy.
Interestingly, it agrees with the Shannon's cross-entropy for one-hot
pseudo-labels y, but the training from softer labels weakens. For example, if y
is a uniform distribution at some data point, it has zero contribution to the
training. Our self-labeling loss combining collision cross entropy with basic
clustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize
over the probability simplex. We derive a practical EM algorithm optimizing
pseudo-labels y significantly faster than generic methods, e.g. the projectile
gradient descent. The collision cross-entropy consistently improves the results
on multiple self-labeled clustering examples using different DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-tuning Via <span class="highlight-title">Prompt</span>s Makes NLP Models Adversarially Robust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations, such
as word-level synonym substitutions. In this work, we demonstrate surprising
gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an
alternative method of adapting to downstream tasks. Rather than modifying the
model (by appending an MLP head), MVP instead modifies the input (by appending
a prompt template). Across three classification datasets, MVP improves
performance against adversarial word-level synonym substitutions by an average
of 8% over standard methods and even outperforms adversarial training-based
state-of-art defenses by 3.5%. By combining MVP with adversarial training, we
achieve further improvements in robust accuracy while maintaining clean
accuracy. Finally, we conduct ablations to investigate the mechanism underlying
these gains. Notably, we find that the main causes of vulnerability of MLP can
be attributed to the misalignment between pre-training and fine-tuning tasks,
and the randomly initialized MLP parameters. Code is available at
https://github.com/acmi-lab/mvp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Reduced-Order Models for Cardiovascular Simulations with Graph
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Pegolotti, Martin R. Pfaller, Natalia L. Rubio, Ke Ding, Rita Brugarolas Brufau, Eric Darve, Alison L. Marsden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reduced-order models based on physics are a popular choice in cardiovascular
modeling due to their efficiency, but they may experience reduced accuracy when
working with anatomies that contain numerous junctions or pathological
conditions. We develop one-dimensional reduced-order models that simulate blood
flow dynamics using a graph neural network trained on three-dimensional
hemodynamic simulation data. Given the initial condition of the system, the
network iteratively predicts the pressure and flow rate at the vessel
centerline nodes. Our numerical results demonstrate the accuracy and
generalizability of our method in physiological geometries comprising a variety
of anatomies and boundary conditions. Our findings demonstrate that our
approach can achieve errors below 2% and 3% for pressure and flow rate,
respectively, provided there is adequate training data. As a result, our method
exhibits superior performance compared to physics-based one-dimensional models,
while maintaining high efficiency at inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSE: Neural SE(3)-Equivariant Embedding for <span class="highlight-title">Consist</span>ent Spatial
  Understanding with Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Fu, Yilun Du, Kurran Singh, Joshua B. Tenenbaum, John J. Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://neuse-slam.github.io/neuse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Models for Acute Brain Dysfunction Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Silva, Miguel Contreras, Tezcan Ozrazgat Baslanti, Yuanfang Ren, Guan Ziyuan, Kia Khezeli, Azra Bihorac, Parisa Rashidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acute brain dysfunctions (ABD), which include coma and delirium, are
prevalent in the ICU, especially among older patients. The current approach in
manual assessment of ABD by care providers may be sporadic and subjective.
Hence, there exists a need for a data-driven robust system automating the
assessment and prediction of ABD. In this work, we develop a machine learning
system for real-time prediction of ADB using Electronic Health Record (HER)
data. Our data processing pipeline enables integration of static and temporal
data, and extraction of features relevant to ABD. We train several
state-of-the-art transformer models and baseline machine learning models
including CatBoost and XGB on the data that was collected from patients
admitted to the ICU at UF Shands Hospital. We demonstrate the efficacy of our
system for tasks related to acute brain dysfunction including binary
classification of brain acuity and multi-class classification (i.e., coma,
delirium, death, or normal), achieving a mean AUROC of 0.953 on our Long-former
implementation. Our system can then be deployed for real-time prediction of ADB
in ICUs to reduce the number of incidents caused by ABD. Moreover, the
real-time system has the potential to reduce costs, duration of patients stays
in the ICU, and mortality among those afflicted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meet in the Middle: A New <span class="highlight-title">Pre-train</span>ing Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Nguyen, Nikos Karampatziakis, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiming Zhang, Haoyu Wei, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In non-asymptotic statistical inferences, variance-type parameters of
sub-Gaussian distributions play a crucial role. However, direct estimation of
these parameters based on the empirical moment generating function (MGF) is
infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment
norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series
of normalized moments. Importantly, the recommended norm can not only recover
the exponential moment bounds for the corresponding MGFs, but also lead to
tighter Hoeffding's sub-Gaussian concentration inequalities. In practice,
{\color{black} we propose an intuitive way of checking sub-Gaussian data with a
finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be
robustly estimated via a simple plug-in approach. Our theoretical results are
applied to non-asymptotic analysis, including the multi-armed bandit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models as Success Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, Serkan Cabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting successful behaviour is crucial for training intelligent agents. As
such, generalisable reward models are a prerequisite for agents that can learn
to generalise their behaviour. In this work we focus on developing robust
success detectors that leverage large, pretrained vision-language models
(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we
treat success detection as a visual question answering (VQA) problem, denoted
SuccessVQA. We study success detection across three vastly different domains:
(i) interactive language-conditioned agents in a simulated household, (ii) real
world robotic manipulation, and (iii) "in-the-wild" human egocentric videos. We
investigate the generalisation properties of a Flamingo-based success detection
model across unseen language and visual changes in the first two domains, and
find that the proposed method is able to outperform bespoke reward models in
out-of-distribution test scenarios with either variation. In the last domain of
"in-the-wild" human videos, we show that success detection on unseen real
videos presents an even more challenging generalisation task warranting future
work. We hope our initial results encourage further work in real world success
detection and reward modelling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Graph <span class="highlight-title">Prompt</span>ing Methods: Techniques, Applications, and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Kaixiong Zhou, Mingchen Sun, Xin Wang, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has achieved great success on various tasks, the
task-specific model training notoriously relies on a large volume of labeled
data. Recently, a new training paradigm of ``pre-train, prompt, predict'' has
been proposed to improve model generalization ability with limited labeled
data. The main idea is that, based on a pre-trained model, the prompting
function uses a template to augment input samples with indicative context and
reformalizes the target task to one of the pre-training tasks. In this survey,
we provide a unique review of prompting methods from the graph perspective.
Graph data has served as structured knowledge repositories in various systems
by explicitly modeling the interaction between entities. Compared with
traditional methods, graph prompting functions could induce task-related
context and apply templates with structured knowledge. The pre-trained model is
then adaptively generalized for future samples. In particular, we introduce the
basic concepts of graph prompt learning, organize the existing work of
designing graph prompting functions, and describe their applications and
challenges to a variety of machine learning problems. This survey attempts to
bridge the gap between structured graphs and prompt design to facilitate future
methodology development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Surface-normal Based Neural Framework for Colonoscopy Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuxian Wang, Yubo Zhang, Sarah K. McGill, Julian G. Rosenman, Jan-Michael Frahm, Soumyadip Sengupta, Stephen M. Pizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D surface from colonoscopy video is challenging due to
illumination and reflectivity variation in the video frame that can cause
defective shape predictions. Aiming to overcome this challenge, we utilize the
characteristics of surface normal vectors and develop a two-step neural
framework that significantly improves the colonoscopy reconstruction quality.
The normal-based depth initialization network trained with self-supervised
normal consistency loss provides depth map initialization to the normal-depth
refinement module, which utilizes the relationship between illumination and
surface normals to refine the frame-wise normal and depth predictions
recursively. Our framework's depth accuracy performance on phantom colonoscopy
data demonstrates the value of exploiting the surface normals in colonoscopy
reconstruction, especially on en face views. Due to its low depth error, the
prediction result from our framework will require limited post-processing to be
clinically applicable for real-time colonoscopy reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IPMI 2023; first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Estimation for Underwater Visible Light Communication: A Sparse
  Learning Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younan Mou, Sicong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The underwater propagation environment for visible light signals is affected
by complex factors such as absorption, shadowing, and reflection, making it
very challengeable to achieve effective underwater visible light communication
(UVLC) channel estimation. It is difficult for the UVLC channel to be sparse
represented in the time and frequency domains, which limits the chance of using
sparse signal processing techniques to achieve better performance of channel
estimation. To this end, a compressed sensing (CS) based framework is
established in this paper by fully exploiting the sparsity of the underwater
visible light channel in the distance domain of the propagation links. In order
to solve the sparse recovery problem and achieve more accurate UVLC channel
estimation, a sparse learning based underwater visible light channel estimation
(SL-UVCE) scheme is proposed. Specifically, a deep-unfolding neural network
mimicking the classical iterative sparse recovery algorithm of approximate
message passing (AMP) is employed, which decomposes the iterations of AMP into
a series of layers with different learnable parameters. Compared with the
existing non-CS-based and CS-based schemes, the proposed scheme shows better
performance of accuracy in channel estimation, especially in severe conditions
such as insufficient measurement pilots and large number of multipath
components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by and is to appear in Proc. 2023 IEEE
  International Conference on Communications (ICC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Convolutional Neural Networks for Chronic Obstructive
  Pulmonary Disease Detection in Clinical Computed Tomography Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Dorosti, Manuel Schultheiss, Felix Hofmann, Luisa Kirchner, Theresa Urban, Franz Pfeiffer, Johannes Thalhammer, Florian Schaff, Tobias Lasser, Daniela Pfeiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death
worldwide, yet early detection and treatment can prevent the progression of the
disease. In contrast to the conventional method of detecting COPD with
spirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a
measure of morphological changes in the lung. It has been shown that automated
detection of COPD can be performed with deep learning models. However, the
potential of incorporating optimal window setting selection, typically carried
out by clinicians during examination of CT scans for COPD, is generally
overlooked in deep learning approaches. We aim to optimize the binary
classification of COPD with densely connected convolutional neural networks
(DenseNets) through implementation of manual and automated Window-Setting
Optimization (WSO) steps. Our dataset consisted of 78 CT scans from the
Klinikum rechts der Isar research hospital. Repeated inference on the test set
showed that without WSO, the plain DenseNet resulted in a mean slice-level AUC
of 0.80$\pm$0.05. With input images manually adjusted to the emphysema window
setting, the plain DenseNet model predicted COPD with a mean AUC of
0.86$\pm$0.04. By automating the WSO through addition of a customized layer to
the DenseNet, an optimal window setting in the proximity of the emphysema
window setting was learned and a mean AUC of 0.82$\pm$0.04 was achieved.
Detection of COPD with DenseNet models was optimized by WSO of CT data to the
emphysema window setting range, demonstrating the importance of implementing
optimal window setting selection in the deep learning pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic Prediction with Transfer Learning: A Mutual Information-based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Huang, Xiaozhuang Song, Yuanshao Zhu, Shiyao Zhang, James J. Q. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern traffic management, one of the most essential yet challenging tasks
is accurately and timely predicting traffic. It has been well investigated and
examined that deep learning-based Spatio-temporal models have an edge when
exploiting Spatio-temporal relationships in traffic data. Typically,
data-driven models require vast volumes of data, but gathering data in small
cities can be difficult owing to constraints such as equipment deployment and
maintenance costs. To resolve this problem, we propose TrafficTL, a cross-city
traffic prediction approach that uses big data from other cities to aid
data-scarce cities in traffic prediction. Utilizing a periodicity-based
transfer paradigm, it identifies data similarity and reduces negative transfer
caused by the disparity between two data distributions from distant cities. In
addition, the suggested method employs graph reconstruction techniques to
rectify defects in data from small data cities. TrafficTL is evaluated by
comprehensive case studies on three real-world datasets and outperforms the
state-of-the-art baseline by around 8 to 25 percent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submited to T-ITS, 16 pages, 13 figures in color</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-
  and Category-Aware <span class="highlight-title">Transformer</span>s <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengliang Liu, Jie Wen, Xiaoling Luo, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we all know, multi-view data is more expressive than single-view data and
multi-label annotation enjoys richer supervision information than single-label,
which makes multi-view multi-label learning widely applicable for various
pattern recognition tasks. In this complex representation learning problem,
three main challenges can be characterized as follows: i) How to learn
consistent representations of samples across all views? ii) How to exploit and
utilize category correlations of multi-label to guide inference? iii) How to
avoid the negative impact resulting from the incompleteness of views or labels?
To cope with these problems, we propose a general multi-view multi-label
learning framework named label-guided masked view- and category-aware
transformers in this paper. First, we design two transformer-style based
modules for cross-view features aggregation and multi-label classification,
respectively. The former aggregates information from different views in the
process of extracting view-specific features, and the latter learns subcategory
embedding to improve classification performance. Second, considering the
imbalance of expressive power among views, an adaptively weighted view fusion
module is proposed to obtain view-consistent embedding features. Third, we
impose a label manifold constraint in sample-level representation learning to
maximize the utilization of supervised information. Last but not least, all the
modules are designed under the premise of incomplete views and labels, which
makes our method adaptable to arbitrary multi-view and multi-label data.
Extensive experiments on five datasets confirm that our method has clear
advantages over other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Visual Number Discrimination in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivana Kajić, Aida Nematzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to discriminate between large and small quantities is a core
aspect of basic numerical competence in both humans and animals. In this work,
we examine the extent to which the state-of-the-art neural networks designed
for vision exhibit this basic ability. Motivated by studies in animal and
infant numerical cognition, we use the numerical bisection procedure to test
number discrimination in different families of neural architectures. Our
results suggest that vision-specific inductive biases are helpful in numerosity
discrimination, as models with such biases have lowest test errors on the task,
and often have psychometric curves that qualitatively resemble those of humans
and animals performing the task. However, even the strongest models, as
measured on standard metrics of performance, fail to discriminate quantities in
transfer experiments with differing training and testing conditions, indicating
that such inductive biases might not be sufficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Tree Search for Automatic Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aran Carmon, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the task of automatic program synthesis, one obtains pairs of matching
inputs and outputs and generates a computer program, in a particular
domain-specific language (DSL), which given each sample input returns the
matching output. A key element is being able to perform an efficient search in
the space of valid programs. Here, we suggest a variant of MCTS that leads to
state of the art results on two vastly different DSLs. The exploration method
we propose includes multiple contributions: a modified visit count, a
preprocessing procedure for the training dataset, and encoding the part of the
program that was already executed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 2nd Exploration in Reinforcement Learning Workshop
  at the 36th International Conference on Machine Learning, 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Cha, Jaewook Lee, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study convergence lower bounds of without-replacement stochastic gradient
descent (SGD) for solving smooth (strongly-)convex finite-sum minimization
problems. Unlike most existing results focusing on final iterate lower bounds
in terms of the number of components $n$ and the number of epochs $K$, we seek
bounds for arbitrary weighted average iterates that are tight in all factors
including the condition number $\kappa$. For SGD with Random Reshuffling, we
present lower bounds that have tighter $\kappa$ dependencies than existing
bounds. Our results are the first to perfectly close the gap between lower and
upper bounds for weighted average iterates in both strongly-convex and convex
cases. We also prove weighted average iterate lower bounds for arbitrary
permutation-based SGD, which apply to all variants that carefully choose the
best permutation. Our bounds improve the existing bounds in factors of $n$ and
$\kappa$ and thereby match the upper bounds shown for a recently proposed
algorithm called GraB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Good Arm Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Da Tsai, Tzu-Hsien Tsai, Shou-De Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper targets a variant of the stochastic multi-armed bandit problem
called good arm identification (GAI). GAI is a pure-exploration bandit problem
with the goal to output as many good arms using as few samples as possible,
where a good arm is defined as an arm whose expected reward is greater than a
given threshold. In this work, we propose DGAI - a differentiable good arm
identification algorithm to improve the sample complexity of the
state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that
the DGAI can further boost the performance of a general multi-arm bandit (MAB)
problem given a threshold as a prior knowledge to the arm set. Extensive
experiments confirm that our algorithm outperform the baseline algorithms
significantly in both synthetic and real world datasets for both GAI and MAB
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SA-CNN: Application to text categorization issues using simulated
  annealing-based convolutional neural network optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Guo, Yueying Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are a representative class of deep
learning algorithms including convolutional computation that perform
translation-invariant classification of input data based on their hierarchical
architecture. However, classical convolutional neural network learning methods
use the steepest descent algorithm for training, and the learning performance
is greatly influenced by the initial weight settings of the convolutional and
fully connected layers, requiring re-tuning to achieve better performance under
different model structures and data. Combining the strengths of the simulated
annealing algorithm in global search, we propose applying it to the
hyperparameter search process in order to increase the effectiveness of
convolutional neural networks (CNNs). In this paper, we introduce SA-CNN neural
networks for text classification tasks based on Text-CNN neural networks and
implement the simulated annealing algorithm for hyperparameter search.
Experiments demonstrate that we can achieve greater classification accuracy
than earlier models with manual tuning, and the improvement in time and space
for exploration relative to human tuning is substantial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM EITCE-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score Attack: A Lower Bound Technique for Optimal Differentially Private
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Tony Cai, Yichen Wang, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving optimal statistical performance while ensuring the privacy of
personal data is a challenging yet crucial objective in modern data analysis.
However, characterizing the optimality, particularly the minimax lower bound,
under privacy constraints is technically difficult.
  To address this issue, we propose a novel approach called the score attack,
which provides a lower bound on the differential-privacy-constrained minimax
risk of parameter estimation. The score attack method is based on the tracing
attack concept in differential privacy and can be applied to any statistical
model with a well-defined score statistic. It can optimally lower bound the
minimax risk of estimating unknown model parameters, up to a logarithmic
factor, while ensuring differential privacy for a range of statistical
problems. We demonstrate the effectiveness and optimality of this general
method in various examples, such as the generalized linear model in both
classical and high-dimensional sparse settings, the Bradley-Terry-Luce model
for pairwise comparisons, and nonparametric regression over the Sobolev class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2011.03900</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for
  Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Tomer Weiss, Dor Noti, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and
reliable technique for the dynamic imaging of internal organs and tissues,
making it a leading diagnostic tool. A major difficulty in using MRI in this
setting is the relatively long acquisition time (and, hence, increased cost)
required for imaging in high spatio-temporal resolution, leading to the
appearance of related motion artifacts and decrease in resolution. Compressed
Sensing (CS) techniques have become a common tool to reduce MRI acquisition
time by subsampling images in the k-space according to some acquisition
trajectory. Several studies have particularly focused on applying deep learning
techniques to learn these acquisition trajectories in order to attain better
image reconstruction, rather than using some predefined set of trajectories. To
the best of our knowledge, learning acquisition trajectories has been only
explored in the context of static MRI. In this study, we consider acquisition
trajectory learning in the dynamic imaging setting. We design an end-to-end
pipeline for the joint optimization of multiple per-frame acquisition
trajectories along with a reconstruction neural network, and demonstrate
improved image reconstruction quality in shorter acquisition times. The code
for reproducing all experiments is accessible at
https://github.com/tamirshor7/MultiPILOT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Microphone Speaker Separation by Spatial Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Wechsler, Srikanth Raj Chetupalli, Wolfgang Mack, Emanuël A. P. Habets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of region-based source separation of reverberant
multi-microphone recordings. We assume pre-defined spatial regions with a
single active source per region. The objective is to estimate the signals from
the individual spatial regions as captured by a reference microphone while
retaining a correspondence between signals and spatial regions. We propose a
data-driven approach using a modified version of a state-of-the-art network,
where different layers model spatial and spectro-temporal information. The
network is trained to enforce a fixed mapping of regions to network outputs.
Using speech from LibriMix, we construct a data set specifically designed to
contain the region information. Additionally, we train the network with
permutation invariant training. We show that both training methods result in a
fixed mapping of regions to network outputs, achieve comparable performance,
and that the networks exploit spatial information. The proposed network
outperforms a baseline network by 1.5 dB in scale-invariant
signal-to-distortion ratio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing statistical and machine learning methods for time series
  forecasting in data-driven logistics -- A simulation study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Schmid, Moritz Roidl, Markus Pauly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many planning and decision activities in logistics and supply chain
management are based on forecasts of multiple time dependent factors.
Therefore, the quality of planning depends on the quality of the forecasts. We
compare various forecasting methods in terms of out of the box forecasting
performance on a broad set of simulated time series. We simulate various linear
and non-linear time series and look at the one step forecast performance of
statistical learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Deep Learning Power System Short-Term Voltage Stability
  Assessment with Physics-Informed Topological Feature Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Feng, Xin Chen, Zijian Lv, Peiyuan Sun, Kai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) algorithms have been widely applied to short-term voltage
stability (STVS) assessment in power systems. However, transferring the
knowledge learned in one power grid to other power grids with topology changes
is still a challenging task. This paper proposed a transferable DL-based model
for STVS assessment by constructing the topology-aware voltage dynamic features
from raw PMU data. Since the reactive power flow and grid topology are
essential to voltage stability, the topology-aware and physics-informed voltage
dynamic features are utilized to effectively represent the topological and
temporal patterns from post-disturbance system dynamic trajectories. The
proposed DL-based STVS assessment model is tested under random operating
conditions on the New England 39-bus system. It has 99.99\% classification
accuracy of the short-term voltage stability status using the topology-aware
and physics-informed voltage dynamic features. In addition to high accuracy,
the experiments show good adaptability to PMU errors. Moreover, The proposed
STVS assessment method has outstanding performance on new grid topologies after
fine-tuning. In particular, the highest accuracy reaches 99.68\% in evaluation,
which demonstrates a good knowledge transfer ability of the proposed model for
power grid topology change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Transactions on Power
  Systems for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolutionary quantum feature selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton S. Albino, Otto M. Pires, Mauro Q. Nooblath, Erick G. S. Nascimento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective feature selection is essential for enhancing the performance of
artificial intelligence models. It involves identifying feature combinations
that optimize a given metric, but this is a challenging task due to the
problem's exponential time complexity. In this study, we present an innovative
heuristic called Evolutionary Quantum Feature Selection (EQFS) that employs the
Quantum Circuit Evolution (QCE) algorithm. Our approach harnesses the unique
capabilities of QCE, which utilizes shallow depth circuits to generate sparse
probability distributions. Our computational experiments demonstrate that EQFS
can identify good feature combinations with quadratic scaling in the number of
features. To evaluate EQFS's performance, we counted the number of times a
given classical model assesses the cost function for a specific metric, as a
function of the number of generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing COVID-19 Severity Analysis through Ensemble Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Thyagachandran, Hema A Murthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed Tomography (CT) scans provide a detailed image of the lungs,
allowing clinicians to observe the extent of damage caused by COVID-19. The CT
severity score (CTSS) of COVID-19 can be categorized based on the extent of
lung involvement observed on a CT scan. This paper proposes a domain
knowledge-based pipeline to extract the infection regions using diverse
image-processing algorithms and a pre-trained UNET model. An ensemble of three
machine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),
and Support Vector Machine (SVM), is employed to classify the CT scans into
different severity classes. The proposed system achieved a macro F1 score of
57.47% on the validation dataset in the AI-Enabled Medical Image Analysis
Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptiveNet: Post-deployment Neural Architecture Adaptation for Diverse
  Edge Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wen, Yuanchun Li, Zunshuai Zhang, Shiqi Jiang, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, Yunxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are increasingly deployed to edge devices for real-time
applications. To ensure stable service quality across diverse edge
environments, it is highly desirable to generate tailored model architectures
for different conditions. However, conventional pre-deployment model generation
approaches are not satisfactory due to the difficulty of handling the diversity
of edge environments and the demand for edge information. In this paper, we
propose to adapt the model architecture after deployment in the target
environment, where the model quality can be precisely measured and private edge
data can be retained. To achieve efficient and effective edge model generation,
we introduce a pretraining-assisted on-cloud model elastification method and an
edge-friendly on-device architecture search method. Model elastification
generates a high-quality search space of model architectures with the guidance
of a developer-specified oracle model. Each subnet in the space is a valid
model with different environment affinity, and each device efficiently finds
and maintains the most suitable subnet based on a series of edge-tailored
optimizations. Extensive experiments on various edge devices demonstrate that
our approach is able to achieve significantly better accuracy-latency tradeoffs
(e.g. 46.74\% higher on average accuracy with a 60\% latency budget) than
strong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes
on the edge server).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving physics-informed neural networks with meta-learned
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Bihlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the error achievable using physics-informed neural networks for
solving systems of differential equations can be substantially reduced when
these networks are trained using meta-learned optimization methods rather than
to using fixed, hand-crafted optimizers as traditionally done. We choose a
learnable optimization method based on a shallow multi-layer perceptron that is
meta-trained for specific classes of differential equations. We illustrate
meta-trained optimizers for several equations of practical relevance in
mathematical physics, including the linear advection equation, Poisson's
equation, the Korteweg--de Vries equation and Burgers' equation. We also
illustrate that meta-learned optimizers exhibit transfer learning abilities, in
that a meta-trained optimizer on one differential equation can also be
successfully deployed on another differential equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't PANIC: Prototypical Additive Neural Network for Interpretable
  Classification of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Nuno Wolf, Sebastian Pölster, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) has a complex and multifactorial etiology, which
requires integrating information about neuroanatomy, genetics, and
cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep
learning approaches combined image and tabular information to improve
diagnostic performance. However, the black-box nature of such neural networks
is still a barrier for clinical applications, in which understanding the
decision of a heterogeneous model is integral. We propose PANIC, a prototypical
additive neural network for interpretable AD classification that integrates 3D
image and tabular data. It is interpretable by design and, thus, avoids the
need for post-hoc explanations that try to approximate the decision of a
network. Our results demonstrate that PANIC achieves state-of-the-art
performance in AD classification, while directly providing local and global
explanations. Finally, we show that PANIC extracts biologically meaningful
signatures of AD, and satisfies a set of desirable desiderata for trustworthy
machine learning. Our implementation is available at
\url{https://github.com/ai-med/PANIC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of Information Processing In Medical
  Imaging 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Agnostic Debiasing for Single Domain Generalization <span class="chip">CVPR-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Yingwei Pan, Guang Chen, Ting Yao, Changjun Jiang, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) usually fail to generalize well to outside of
distribution (OOD) data, especially in the extreme case of single domain
generalization (single-DG) that transfers DNNs from single domain to multiple
unseen domains. Existing single-DG techniques commonly devise various
data-augmentation algorithms, and remould the multi-source domain
generalization methodology to learn domain-generalized (semantic) features.
Nevertheless, these methods are typically modality-specific, thereby being only
applicable to one single modality (e.g., image). In contrast, we target a
versatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that
enables generalization for different modalities. Technically, MAD introduces a
novel two-branch classifier: a biased-branch encourages the classifier to
identify the domain-specific (superficial) features, and a general-branch
captures domain-generalized features based on the knowledge from biased-branch.
Our MAD is appealing in view that it is pluggable to most single-DG models. We
validate the superiority of our MAD in a variety of single-DG scenarios with
different modalities, including recognition on 1D texts, 2D images, 3D point
clouds, and semantic segmentation on 2D images. More remarkably, for
recognition on 3D point clouds and semantic segmentation on 2D images, MAD
improves DSU by 2.82\% and 1.5\% in accuracy and mIOU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upcycling Models under Domain and Category Shift <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Tianpei Zou, Florian Roehrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often perform poorly in the presence of domain
shift and category shift. How to upcycle DNNs and adapt them to the target task
remains an important open problem. Unsupervised Domain Adaptation (UDA),
especially recently proposed Source-free Domain Adaptation (SFDA), has become a
promising technology to address this issue. Nevertheless, existing SFDA methods
require that the source domain and target domain share the same label space,
consequently being only applicable to the vanilla closed-set setting. In this
paper, we take one step further and explore the Source-free Universal Domain
Adaptation (SF-UniDA). The goal is to identify "known" data samples under both
domain and category shift, and reject those "unknown" data samples (not present
in source classes), with only the knowledge from standard pre-trained source
model. To this end, we introduce an innovative global and local clustering
learning technique (GLC). Specifically, we design a novel, adaptive one-vs-all
global clustering algorithm to achieve the distinction across different target
classes and introduce a local k-NN clustering strategy to alleviate negative
transfer. We examine the superiority of our GLC on multiple benchmarks with
different category shift scenarios, including partial-set, open-set, and
open-partial-set DA. Remarkably, in the most challenging open-partial-set DA
scenario, GLC outperforms UMAD by 14.8\% on the VisDA benchmark. The code is
available at https://github.com/ispc-lab/GLC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2023. The code has been made public</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based World Models Are Happy With 100k Interactions <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023. Code is available at
  https://github.com/jrobine/twm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ n-Step Temporal Difference Learning with Optimal n 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmi Mandal, Shalabh Bhatnagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding the optimal value of n in the n-step
temporal difference (TD) algorithm. We find the optimal n by resorting to the
model-free optimization technique of simultaneous perturbation stochastic
approximation (SPSA). We adopt a one-simulation SPSA procedure that is
originally for continuous optimization to the discrete optimization framework
but incorporates a cyclic perturbation sequence. We prove the convergence of
our proposed algorithm, SDPSA, and show that it finds the optimal value of n in
n-step TD. Through experiments, we show that the optimal value of n is achieved
with SDPSA for any arbitrary initial value of the same.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-device Federated Learning for Mobile Health Diagnostics: A First
  Study on COVID-19 Detection <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xia, Jing Han, Abhirup Ghosh, Cecilia Mascolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) aided health diagnostic models can incorporate data
from a large number of personal edge devices (e.g., mobile phones) while
keeping the data local to the originating devices, largely ensuring privacy.
However, such a cross-device FL approach for health diagnostics still imposes
many challenges due to both local data imbalance (as extreme as local data
consists of a single disease class) and global data imbalance (the disease
prevalence is generally low in a population). Since the federated server has no
access to data distribution information, it is not trivial to solve the
imbalance issue towards an unbiased model. In this paper, we propose FedLoss, a
novel cross-device FL framework for health diagnostics. Here the federated
server averages the models trained on edge devices according to the predictive
loss on the local data, rather than using only the number of samples as
weights. As the predictive loss better quantifies the data distribution at a
device, FedLoss alleviates the impact of data imbalance. Through a real-world
dataset on respiratory sound and symptom-based COVID-$19$ detection task, we
validate the superiority of FedLoss. It achieves competitive COVID-$19$
detection performance compared to a centralised model with an AUC-ROC of
$79\%$. It also outperforms the state-of-the-art FL baselines in sensitivity
and convergence speed. Our work not only demonstrates the promise of federated
COVID-$19$ detection but also paves the way to a plethora of mobile health
model development in a privacy-preserving fashion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile Online Learning for Semiconductor Failure Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangjian Zhou, Pan Jieming, Maheswari Sivan, Aaron Voon-Yew Thean, J. Senthilnath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With high device integration density and evolving sophisticated device
structures in semiconductor chips, detecting defects becomes elusive and
complex. Conventionally, machine learning (ML)-guided failure analysis is
performed with offline batch mode training. However, the occurrence of new
types of failures or changes in the data distribution demands retraining the
model. During the manufacturing process, detecting defects in a single-pass
online fashion is more challenging and favoured. This paper focuses on novel
quantile online learning for semiconductor failure analysis. The proposed
method is applied to semiconductor device-level defects: FinFET bridge defect,
GAA-FET bridge defect, GAA-FET dislocation defect, and a public database:
SECOM. From the obtained results, we observed that the proposed method is able
to perform better than the existing methods. Our proposed method achieved an
overall accuracy of 86.66% and compared with the second-best existing method it
improves 15.50% on the GAA-FET dislocation defect dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bandit-supported care planning for older people with complex health and
  care needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gi-Soo Kim, Young Suh Hong, Tae Hoon Lee, Myunghee Cho Paik, Hongsoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term care service for old people is in great demand in most of the aging
societies. The number of nursing homes residents is increasing while the number
of care providers is limited. Due to the care worker shortage, care to
vulnerable older residents cannot be fully tailored to the unique needs and
preference of each individual. This may bring negative impacts on health
outcomes and quality of life among institutionalized older people. To improve
care quality through personalized care planning and delivery with limited care
workforce, we propose a new care planning model assisted by artificial
intelligence. We apply bandit algorithms which optimize the clinical decision
for care planning by adapting to the sequential feedback from the past
decisions. We evaluate the proposed model on empirical data acquired from the
Systems for Person-centered Elder Care (SPEC) study, a ICT-enhanced care
management program.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Variational Autoencoder for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Cai, Shuiqiao Yang, Longxiang Gao, Yong Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational autoencoders (VAE) are powerful generative models that learn the
latent representations of input data as random variables. Recent studies show
that VAE can flexibly learn the complex temporal dynamics of time series and
achieve more promising forecasting results than deterministic models. However,
a major limitation of existing works is that they fail to jointly learn the
local patterns (e.g., seasonality and trend) and temporal dynamics of time
series for forecasting. Accordingly, we propose a novel hybrid variational
autoencoder (HyVAE) to integrate the learning of local patterns and temporal
dynamics by variational inference for time series forecasting. Experimental
results on four real-world datasets show that the proposed HyVAE achieves
better forecasting results than various counterpart methods, as well as two
HyVAE variants that only learn the local patterns or temporal dynamics of time
series, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deploying Offline Reinforcement Learning with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziniu Li, Ke Xu, Liu Liu, Lanqing Li, Deheng Ye, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown promise for decision-making tasks in
real-world applications. One practical framework involves training
parameterized policy models from an offline dataset and subsequently deploying
them in an online environment. However, this approach can be risky since the
offline training may not be perfect, leading to poor performance of the RL
models that may take dangerous actions. To address this issue, we propose an
alternative framework that involves a human supervising the RL models and
providing additional feedback in the online deployment phase. We formalize this
online deployment problem and develop two approaches. The first approach uses
model selection and the upper confidence bound algorithm to adaptively select a
model to deploy from a candidate set of trained offline RL models. The second
approach involves fine-tuning the model in the online deployment phase when a
supervision signal arrives. We demonstrate the effectiveness of these
approaches for robot locomotion control and traffic light control tasks through
empirical validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\nabla$SD: Differentiable Programming for Sparse Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shaikhha, Mathieu Huot, Shideh Hashemian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse tensors are prevalent in many data-intensive applications, yet
existing differentiable programming frameworks are tailored towards dense
tensors. This presents a significant challenge for efficiently computing
gradients through sparse tensor operations, as their irregular sparsity
patterns can result in substantial memory and computational overheads. In this
work, we introduce a novel framework that enables the efficient and automatic
differentiation of sparse tensors, addressing this fundamental issue. Our
experiments demonstrate the effectiveness of the proposed framework in terms of
performance and scalability, outperforming state-of-the-art frameworks across a
range of synthetic and real-world datasets. Our approach offers a promising
direction for enabling efficient and scalable differentiable programming with
sparse tensors, which has significant implications for numerous applications in
machine learning, natural language processing, and scientific computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic Regression for PDEs using Pruned Differentiable Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritam Majumdar, Vishal Jadhav, Anirudh Deodhar, Shirish Karande, Lovekesh Vig, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed Neural Networks (PINNs) have been widely used to obtain
accurate neural surrogates for a system of Partial Differential Equations
(PDE). One of the major limitations of PINNs is that the neural solutions are
challenging to interpret, and are often treated as black-box solvers. While
Symbolic Regression (SR) has been studied extensively, very few works exist
which generate analytical expressions to directly perform SR for a system of
PDEs. In this work, we introduce an end-to-end framework for obtaining
mathematical expressions for solutions of PDEs. We use a trained PINN to
generate a dataset, upon which we perform SR. We use a Differentiable Program
Architecture (DPA) defined using context-free grammar to describe the space of
symbolic expressions. We improve the interpretability by pruning the DPA in a
depth-first manner using the magnitude of weights as our heuristic. On average,
we observe a 95.3% reduction in parameters of DPA while maintaining accuracy at
par with PINNs. Furthermore, on an average, pruning improves the accuracy of
DPA by 7.81% . We demonstrate our framework outperforms the existing
state-of-the-art SR solvers on systems of complex PDEs like Navier-Stokes:
Kovasznay flow and Taylor-Green Vortex flow. Furthermore, we produce analytical
expressions for a complex industrial use-case of an Air-Preheater, without
suffering from performance loss viz-a-viz PINNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publication accepted at International Conference for Learning
  Representations 2023: Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Density of States via Multi-modal <span class="highlight-title">Transformer</span> <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namkyeong Lee, Heewoong Noh, Sungwon Kim, Dongmin Hyun, Gyoung S. Na, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The density of states (DOS) is a spectral property of materials, which
provides fundamental insights on various characteristics of materials. In this
paper, we propose a model to predict the DOS by reflecting the nature of DOS:
DOS determines the general distribution of states as a function of energy.
Specifically, we integrate the heterogeneous information obtained from the
crystal structure and the energies via multi-modal transformer, thereby
modeling the complex relationships between the atoms in the crystal structure,
and various energy levels. Extensive experiments on two types of DOS, i.e.,
Phonon DOS and Electron DOS, with various real-world scenarios demonstrate the
superiority of DOSTransformer. The source code for DOSTransformer is available
at https://github.com/HeewoongNoh/DOSTransformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 Workshop on Machine Learning for Materials (ML4Materials)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Label Errors in Object Detection <span class="highlight-title">Dataset</span>s by <span class="highlight-title">Loss</span> Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Schubert, Tobias Riedlinger, Karsten Kahl, Daniel Kröll, Sebastian Schoenen, Siniša Šegvić, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages' classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector's score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., when considering 200
proposals from our method, we detect label errors with a precision for a) of up
to 71.5% and for b) with 97%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Mutual Information Estimation with Annealed and Energy-Based
  Bounds <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Brekelmans, Sicong Huang, Marzyeh Ghassemi, Greg Ver Steeg, Roger Grosse, Alireza Makhzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mutual information (MI) is a fundamental quantity in information theory and
machine learning. However, direct estimation of MI is intractable, even if the
true joint probability density for the variables of interest is known, as it
involves estimating a potentially high-dimensional log partition function. In
this work, we present a unifying view of existing MI bounds from the
perspective of importance sampling, and propose three novel bounds based on
this approach. Since accurate estimation of MI without density information
requires a sample size exponential in the true MI, we assume either a single
marginal or the full joint density information is known. In settings where the
full joint density is available, we propose Multi-Sample Annealed Importance
Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large
values of MI in our experiments. In settings where only a single marginal
distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds.
Our GIWAE bound unifies variational and contrastive bounds in a single
framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our
MINE-AIS method improves upon existing energy-based methods such as MINE-DV and
MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC
sampling to estimate gradients for training and Multi-Sample AIS for evaluating
the bound. Our methods are particularly suitable for evaluating MI in deep
generative models, since explicit forms of the marginal or joint densities are
often available. We evaluate our bounds on estimating the MI of VAEs and GANs
trained on the MNIST and CIFAR datasets, and showcase significant gains over
existing bounds in these challenging settings with high ground truth MI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version appeared in the International Conference on
  Learning Representations (ICLR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> based general laboratory progress <span class="highlight-title">pretrain</span>ed model for
  cardiovascular event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Chin Chen, Kuo-Hsuan Hung, Yi-Ju Tseng, Hsin-Yao Wang, Tse-Min Lu, Wei-Chieh Huang, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular surveillance is an indispensable aspect of managing cardiovascular
disorders. Patient recruitment for rare or specific diseases is often limited
due to their small patient size and episodic observations, whereas prevalent
cases accumulate longitudinal data easily due to regular follow-ups. These
data, however, are notorious for their irregularity, temporality, sparsity, and
absenteeism. In this study, we leveraged self-supervised learning (SSL) and
transfer learning to overcome the above-mentioned barriers, transferring
patient progress trends in cardiovascular laboratory parameters from prevalent
cases to rare or specific cardiovascular events detection. We pretrained a
general laboratory progress (GLP) pretrain model using hypertension patients
(who were yet to be diabetic), and transferred their laboratory progress trend
to assist in detecting target vessel revascularization (TVR) in percutaneous
coronary intervention patients. GLP adopted a two-stage training process that
utilized interpolated data, enhancing the performance of SSL. After pretraining
GLP, we fine-tuned it for TVR prediction. The proposed two-stage training
process outperformed SSL. Upon processing by GLP, the classification
demonstrated a marked improvement, increasing from 0.63 to 0.90 in averaged
accuracy. All metrics were significantly superior (p < 0.01) to the performance
of prior GLP processing. The representation displayed distinct separability
independent of algorithmic mechanisms, and diverse data distribution trend. Our
approach effectively transferred the progression trends of cardiovascular
laboratory parameters from prevalent cases to small-numbered cases, thereby
demonstrating its efficacy in aiding the risk assessment of cardiovascular
events without limiting to episodic observation. The potential for extending
this approach to other laboratory tests and diseases is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Neural Koopman Operators to Learn Continuous Representations
  of Dynamical Systems from Scarce Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Frion, Lucas Drumetz, Mauro Dalla Mura, Guillaume Tochon, Abdeldjalil Aissa El Bey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, several works have proposed deep learning
architectures to learn dynamical systems from observation data with no or
little knowledge of the underlying physics. A line of work relies on learning
representations where the dynamics of the underlying phenomenon can be
described by a linear operator, based on the Koopman operator theory. However,
despite being able to provide reliable long-term predictions for some dynamical
systems in ideal situations, the methods proposed so far have limitations, such
as requiring to discretize intrinsically continuous dynamical systems, leading
to data loss, especially when handling incomplete or sparsely sampled data.
Here, we propose a new deep Koopman framework that represents dynamics in an
intrinsically continuous way, leading to better performance on limited training
data, as exemplified on several datasets arising from dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-RXN: An Unified Framework that Bridge the Gap between Chemical
  Reaction <span class="highlight-title">Pretrain</span>ing and Conditional Molecule <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Liangren Zhang, Zhenming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical reactions are the fundamental building blocks of drug design and
organic chemistry research. Machine learning for chemistry is a rapidly
advancing field with numerous applications. In recent years, there has been a
growing need for a large-scale deep-learning framework that can efficiently
capture the basic rules of chemical reactions. In this paper, we have proposed
a unified framework that addresses both the reaction representation learning
and molecule generation tasks, which allows for a more holistic approach.
Inspired by the organic chemistry mechanism, we develop a novel pretraining
framework that enables us to incorporate inductive biases into the model. Our
framework achieves state-of-the-art results on challenging downstream tasks. By
possessing chemical knowledge, this framework can be applied to reaction-based
generative models, overcoming the limitations of current molecule generation
models that rely on a small number of reaction templates. In the extensive
experiments, our model generates synthesizable drug-like structures of high
quality. Overall, our work presents a significant step toward a large-scale
deep-learning framework for a variety of reaction-based applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X
  Communications in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Cazzella, Francesco Linsalata, Maurizio Magarini, Matteo Matteucci, Umberto Spagnolini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital Twins (DTs) for physical wireless environments have been recently
proposed as accurate virtual representations of the propagation environment
that can enable multi-layer decisions at the physical communication equipment.
At high frequency bands, DTs can help to overcome the challenges emerging in
the high mobility conditions featuring vehicular environments. In this paper,
we propose a novel data-driven workflow for the creation of the DT of a
Vehicle-to-Everything (V2X) communication scenario and a multi-modal simulation
framework for the generation of realistic sensor data and accurate
mmWave/sub-THz wireless channels. The proposed method leverages an automotive
simulation and testing framework based on the Unreal Engine game engine and an
accurate ray-tracing channel simulator. Simulations over an urban scenario show
the achievable realistic sensor and channel modelling both at the
infrastructure and at an ego-vehicle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Selective Label Smoothing for Calibrating Sequence
  Recognition Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangping Huang, Yu Luo, Zhenzhou Zhuang, Jin-Gang Yu, Mengchao He, Yongpan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of deep neural network (DNN) on sequential data (i.e.,
scene text and speech) recognition, it suffers from the over-confidence problem
mainly due to overfitting in training with the cross-entropy loss, which may
make the decision-making less reliable. Confidence calibration has been
recently proposed as one effective solution to this problem. Nevertheless, the
majority of existing confidence calibration methods aims at non-sequential
data, which is limited if directly applied to sequential data since the
intrinsic contextual dependency in sequences or the class-specific statistical
prior is seldom exploited. To the end, we propose a Context-Aware Selective
Label Smoothing (CASLS) method for calibrating sequential data. The proposed
CASLS fully leverages the contextual dependency in sequences to construct
confusion matrices of contextual prediction statistics over different classes.
Class-specific error rates are then used to adjust the weights of smoothing
strength in order to achieve adaptive calibration. Experimental results on
sequence recognition tasks, including scene text recognition and speech
recognition, demonstrate that our method can achieve the state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for
  Protein-Protein Interaction Site Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Guo, Xuening Zhu, Zixin Hu, Xiaoxi Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein-protein interactions are essential in biochemical processes. Accurate
prediction of the protein-protein interaction sites (PPIs) deepens our
understanding of biological mechanism and is crucial for new drug design.
However, conventional experimental methods for PPIs prediction are costly and
time-consuming so that many computational approaches, especially ML-based
methods, have been developed recently. Although these approaches have achieved
gratifying results, there are still two limitations: (1) Most models have
excavated some useful input features, but failed to take coevolutionary
features into account, which could provide clues for inter-residue
relationships; (2) The attention-based models only allocate attention weights
for neighboring residues, instead of doing it globally, neglecting that some
residues being far away from the target residues might also matter.
  We propose a coevolution-enhanced global attention neural network, a
sequence-based deep learning model for PPIs prediction, called CoGANPPIS. It
utilizes three layers in parallel for feature extraction: (1) Local-level
representation aggregation layer, which aggregates the neighboring residues'
features; (2) Global-level representation learning layer, which employs a novel
coevolution-enhanced global attention mechanism to allocate attention weights
to all the residues on the same protein sequences; (3) Coevolutionary
information learning layer, which applies CNN & pooling to coevolutionary
information to obtain the coevolutionary profile representation. Then, the
three outputs are concatenated and passed into several fully connected layers
for the final prediction. Application on two benchmark datasets demonstrated a
state-of-the-art performance of our model. The source code is publicly
available at https://github.com/Slam1423/CoGANPPIS_source_code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Catastrophic Forgetting in Federated Class-Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Chen Chen, Weiming Zhuang, Lingjuan Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on an under-explored yet important problem: Federated
Class-Continual Learning (FCCL), where new classes are dynamically added in
federated learning. Existing FCCL works suffer from various limitations, such
as requiring additional datasets or storing the private data from previous
tasks. In response, we first demonstrate that non-IID data exacerbates
catastrophic forgetting issue in FL. Then we propose a novel method called
TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G}
via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates
catastrophic forgetting in FCCL while preserving client data privacy. Our
proposed method leverages the previously trained global model to transfer
knowledge of old tasks to the current task at the model level. Moreover, a
generator is trained to produce synthetic data to simulate the global
distribution of data on each client at the data level. Compared to previous
FCCL methods, TARGET does not require any additional datasets or storing real
data from previous tasks, which makes it ideal for data-sensitive scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepVigor: Vulnerability Value Ranges and Factors for DNNs' Reliability
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hasan Ahmadilivani, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, Maksim Jenihhin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) and their accelerators are being deployed ever
more frequently in safety-critical applications leading to increasing
reliability concerns. A traditional and accurate method for assessing DNNs'
reliability has been resorting to fault injection, which, however, suffers from
prohibitive time complexity. While analytical and hybrid fault
injection-/analytical-based methods have been proposed, they are either
inaccurate or specific to particular accelerator architectures. In this work,
we propose a novel accurate, fine-grain, metric-oriented, and
accelerator-agnostic method called DeepVigor that provides vulnerability value
ranges for DNN neurons' outputs. An outcome of DeepVigor is an analytical model
representing vulnerable and non-vulnerable ranges for each neuron that can be
exploited to develop different techniques for improving DNNs' reliability.
Moreover, DeepVigor provides reliability assessment metrics based on
vulnerability factors for bits, neurons, and layers using the vulnerability
ranges. The proposed method is not only faster than fault injection but also
provides extensive and accurate information about the reliability of DNNs,
independent from the accelerator. The experimental evaluations in the paper
indicate that the proposed vulnerability ranges are 99.9% to 100% accurate even
when evaluated on previously unseen test data. Also, it is shown that the
obtained vulnerability factors represent the criticality of bits, neurons, and
layers proficiently. DeepVigor is implemented in the PyTorch framework and
validated on complex DNN benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 2 tables, accepted at ETS 2023
  (cas.polito.it/ETS23/#/program-conference#tab-accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Property Prediction by Semantic-invariant Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Zhang, Ailin Xie, Jihong Guan, Shuigeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning have been widely used as pretext tasks for
self-supervised pre-trained molecular representation learning models in
AI-aided drug design and discovery. However, exiting methods that generate
molecular views by noise-adding operations for contrastive learning may face
the semantic inconsistency problem, which leads to false positive pairs and
consequently poor prediction performance. To address this problem, in this
paper we first propose a semantic-invariant view generation method by properly
breaking molecular graphs into fragment pairs. Then, we develop a
Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) model based on
this view generation method for molecular property prediction. The FraSICL
model consists of two branches to generate representations of views for
contrastive learning, meanwhile a multi-view fusion and an auxiliary similarity
loss are introduced to make better use of the information contained in
different fragment-pair views. Extensive experiments on various benchmark
datasets show that with the least number of pre-training samples, FraSICL can
achieve state-of-the-art performance, compared with major existing counterpart
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spacecraft Anomaly Detection with Attention Temporal Convolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Liu, Ling Tian, Zhao Kang, Tianqi Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spacecraft faces various situations when carrying out exploration missions in
complex space, thus monitoring the anomaly status of spacecraft is crucial to
the development of \textcolor{blue}{the} aerospace industry. The time series
telemetry data generated by on-orbit spacecraft \textcolor{blue}{contains}
important information about the status of spacecraft. However, traditional
domain knowledge-based spacecraft anomaly detection methods are not effective
due to high dimensionality and complex correlation among variables. In this
work, we propose an anomaly detection framework for spacecraft multivariate
time-series data based on temporal convolution networks (TCNs). First, we
employ dynamic graph attention to model the complex correlation among variables
and time series. Second, temporal convolution networks with parallel processing
ability are used to extract multidimensional \textcolor{blue}{features} for
\textcolor{blue}{the} downstream prediction task. Finally, many potential
anomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL
spacecraft datasets show the superiority of our proposed model with respect to
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-driven machine learning models coupling PyTorch and Firedrake <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nacime Bouziani, David A. Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial differential equations (PDEs) are central to describing and modelling
complex physical systems that arise in many disciplines across science and
engineering. However, in many realistic applications PDE modelling provides an
incomplete description of the physics of interest. PDE-based machine learning
techniques are designed to address this limitation. In this approach, the PDE
is used as an inductive bias enabling the coupled model to rely on fundamental
physical laws while requiring less training data. The deployment of
high-performance simulations coupling PDEs and machine learning to complex
problems necessitates the composition of capabilities provided by machine
learning and PDE-based frameworks. We present a simple yet effective coupling
between the machine learning framework PyTorch and the PDE system Firedrake
that provides researchers, engineers and domain specialists with a high
productive way of specifying coupled models while only requiring trivial
changes to existing code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ICLR 2023 Workshop on Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three Guidelines You Should Know for Universally Slimmable
  <span class="highlight-title">Self-Supervised</span> Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Hao Cao, Peiqin Sun, Shuchang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose universally slimmable self-supervised learning (dubbed as US3L) to
achieve better accuracy-efficiency trade-offs for deploying self-supervised
models across different devices. We observe that direct adaptation of
self-supervised learning (SSL) to universally slimmable networks misbehaves as
the training process frequently collapses. We then discover that temporal
consistent guidance is the key to the success of SSL for universally slimmable
networks, and we propose three guidelines for the loss design to ensure this
temporal consistency from a unified gradient perspective. Moreover, we propose
dynamic sampling and group regularization strategies to simultaneously improve
training efficiency and accuracy. Our US3L method has been empirically
validated on both convolutional neural networks and vision transformers. With
only once training and one copy of weights, our method outperforms various
state-of-the-art methods (individually trained or not) on benchmarks including
recognition, object detection and instance segmentation. Our code is available
at https://github.com/megvii-research/US3L-CVPR2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-throughput Generative Inference of Large Language Models with a
  Single GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational and memory requirements of large language model (LLM)
inference traditionally make it feasible only with multiple high-end
accelerators. Motivated by the emerging demand for latency-insensitive tasks
with batched processing, this paper initiates the study of high-throughput LLM
inference using limited resources, such as a single commodity GPU. We present
FlexGen, a high-throughput generation engine for running LLMs with limited GPU
memory. FlexGen can be flexibly configured under various hardware resource
constraints by aggregating memory and computation from the GPU, CPU, and disk.
Through a linear programming optimizer, it searches for efficient patterns to
store and access tensors. FlexGen further compresses these weights and the
attention cache to 4 bits with negligible accuracy loss. These techniques
enable FlexGen to have a larger space of batch size choices and thus
significantly increase maximum throughput. As a result, when running OPT-175B
on a single 16GB GPU, FlexGen achieves significantly higher throughput compared
to state-of-the-art offloading systems, reaching a generation throughput of 1
token/s for the first time with an effective batch size of 144. On the HELM
benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7
representative sub-scenarios in 21 hours. The code is available at
https://github.com/FMInference/FlexGen
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Neural Network for Multi-Task Learning Searching across Diverse
  Network Topologies <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonhyeok Choi, Sunghoon Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new MTL framework that searches for structures
optimized for multiple tasks with diverse graph topologies and shares features
among tasks. We design a restricted DAG-based central network with
read-in/read-out layers to build topologically diverse task-adaptive structures
while limiting search space and time. We search for a single optimized network
that serves as multiple task adaptive sub-networks using our three-stage
training process. To make the network compact and discretized, we propose a
flow-based reduction algorithm and a squeeze loss used in the training process.
We evaluate our optimized network on various public MTL datasets and show ours
achieves state-of-the-art performance. An extensive ablation study
experimentally validates the effectiveness of the sub-module and schemes in our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing against Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of adversarial attacks, including targeted and
backdoor data poisoning attacks. Despite this vulnerability, robust contrastive
vision-language pretraining against adversarial attacks has remained
unaddressed. In this work, we propose RoCLIP, the first effective method for
robust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP
effectively breaks the association between poisoned image-caption pairs by
considering a pool of random examples, and (1) matching every image with the
text that is most similar to its caption in the pool, and (2) matching every
caption with the image that is most similar to its image in the pool. Our
extensive experiments show that our method renders state-of-the-art targeted
data poisoning and backdoor attacks ineffective during pre-training or
fine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor
attack success rates down to 0\% during pre-training and 1\%-4\% during
fine-tuning, and effectively improves the model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Regret of Online Edge Service Hosting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R Sri Prakash, Nikhil Karamchandani, Sharayu Moharir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of service hosting where a service provider can
dynamically rent edge resources via short term contracts to ensure better
quality of service to its customers. The service can also be partially hosted
at the edge, in which case, customers' requests can be partially served at the
edge. The total cost incurred by the system is modeled as a combination of the
rent cost, the service cost incurred due to latency in serving customers, and
the fetch cost incurred as a result of the bandwidth used to fetch the
code/databases of the service from the cloud servers to host the service at the
edge. In this paper, we compare multiple hosting policies with regret as a
metric, defined as the difference in the cost incurred by the policy and the
optimal policy over some time horizon $T$. In particular we consider the Retro
Renting (RR) and Follow The Perturbed Leader (FTPL) policies proposed in the
literature and provide performance guarantees on the regret of these policies.
We show that under i.i.d stochastic arrivals, RR policy has linear regret while
FTPL policy has constant regret. Next, we propose a variant of FTPL, namely
Wait then FTPL (W-FTPL), which also has constant regret while demonstrating
much better dependence on the fetch cost. We also show that under adversarial
arrivals, RR policy has linear regret while both FTPL and W-FTPL have regret
$\mathrm{O}(\sqrt{T})$ which is order-optimal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Distribution Learning from Logical Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Jia, Jiawei Tang, Jiahao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label distribution learning (LDL) is an effective method to predict the label
description degree (a.k.a. label distribution) of a sample. However, annotating
label distribution (LD) for training samples is extremely costly. So recent
studies often first use label enhancement (LE) to generate the estimated label
distribution from the logical label and then apply external LDL algorithms on
the recovered label distribution to predict the label distribution for unseen
samples. But this step-wise manner overlooks the possible connections between
LE and LDL. Moreover, the existing LE approaches may assign some description
degrees to invalid labels. To solve the above problems, we propose a novel
method to learn an LDL model directly from the logical label, which unifies LE
and LDL into a joint model, and avoids the drawbacks of the previous LE
methods. Extensive experiments on various datasets prove that the proposed
approach can construct a reliable LDL model directly from the logical label,
and produce more accurate label distribution than the state-of-the-art LE
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Encoder with Multiscale Deep Learning for Pain
  Classification Using Physiological Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Lu, Burcu Ozek, Sagar Kamarthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pain is a serious worldwide health problem that affects a vast proportion of
the population. For efficient pain management and treatment, accurate
classification and evaluation of pain severity are necessary. However, this can
be challenging as pain is a subjective sensation-driven experience. Traditional
techniques for measuring pain intensity, e.g. self-report scales, are
susceptible to bias and unreliable in some instances. Consequently, there is a
need for more objective and automatic pain intensity assessment strategies. In
this research, we develop PainAttnNet (PAN), a novel transfomer-encoder
deep-learning framework for classifying pain intensities with physiological
signals as input. The proposed approach is comprised of three feature
extraction architectures: multiscale convolutional networks (MSCN), a
squeeze-and-excitation residual network (SEResNet), and a transformer encoder
block. On the basis of pain stimuli, MSCN extracts short- and long-window
information as well as sequential features. SEResNet highlights relevant
extracted features by mapping the interdependencies among features. The third
architecture employs a transformer encoder consisting of three temporal
convolutional networks (TCN) with three multi-head attention (MHA) layers to
extract temporal dependencies from the features. Using the publicly available
BioVid pain dataset, we test the proposed PainAttnNet model and demonstrate
that our outcomes outperform state-of-the-art models. These results confirm
that our approach can be utilized for automated classification of pain
intensity using physiological signals to improve pain management and treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Information Bottleneck for Label Enhancement <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghai Zheng, Jihua Zhu, Haoyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we focus on the challenging problem of Label Enhancement (LE),
which aims to exactly recover label distributions from logical labels, and
present a novel Label Information Bottleneck (LIB) method for LE. For the
recovery process of label distributions, the label irrelevant information
contained in the dataset may lead to unsatisfactory recovery performance. To
address this limitation, we make efforts to excavate the essential label
relevant information to improve the recovery performance. Our method formulates
the LE problem as the following two joint processes: 1) learning the
representation with the essential label relevant information, 2) recovering
label distributions based on the learned representation. The label relevant
information can be excavated based on the "bottleneck" formed by the learned
representation. Significantly, both the label relevant information about the
label assignments and the label relevant information about the label gaps can
be explored in our method. Evaluation experiments conducted on several
benchmark label distribution learning datasets verify the effectiveness and
competitiveness of LIB. Our source codes are available
"https://github.com/qinghai-zheng/LIBLE"
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023, our source codes are available at
  "https://github.com/qinghai-zheng/LIBLE"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based Planning for Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic regression (SR) is a challenging task in machine learning that
involves finding a mathematical expression for a function based on its values.
Recent advancements in SR have demonstrated the efficacy of pretrained
transformer-based models for generating equations as sequences, which benefit
from large-scale pretraining on synthetic datasets and offer considerable
advantages over GP-based methods in terms of inference time. However, these
models focus on supervised pretraining goals borrowed from text generation and
ignore equation-specific objectives like accuracy and complexity. To address
this, we propose TPSR, a Transformer-based Planning strategy for Symbolic
Regression that incorporates Monte Carlo Tree Search into the transformer
decoding process. TPSR, as opposed to conventional decoding strategies, allows
for the integration of non-differentiable feedback, such as fitting accuracy
and complexity, as external sources of knowledge into the equation generation
process. Extensive experiments on various datasets show that our approach
outperforms state-of-the-art methods, enhancing the model's fitting-complexity
trade-off, extrapolation abilities, and robustness to noise. We also
demonstrate that the utilization of various caching mechanisms can further
enhance the efficiency of TPSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Parshin Shojaee and Kazem Meidani contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODIN: On-demand Data Formulation to Mitigate <span class="highlight-title">Dataset</span> Lock-in 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Spchoi, Jihoon Lee, HyeongSeok Ahn, Sanghee Jung, Bumsoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ODIN is an innovative approach that addresses the problem of dataset
constraints by integrating generative AI models. Traditional zero-shot learning
methods are constrained by the training dataset. To fundamentally overcome this
limitation, ODIN attempts to mitigate the dataset constraints by generating
on-demand datasets based on user requirements. ODIN consists of three main
modules: a prompt generator, a text-to-image generator, and an image
post-processor. To generate high-quality prompts and images, we adopted a large
language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g.,
Stable Diffusion), respectively. We evaluated ODIN on various datasets in terms
of model accuracy and data diversity to demonstrate its potential, and
conducted post-experiments for further investigation. Overall, ODIN is a
feasible approach that enables Al to learn unseen knowledge beyond the training
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Density Bayesian Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Mandyam, Didong Li, Diana Cai, Andrew Jones, Barbara E. Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning~(IRL) is a powerful framework to infer an
agent's reward function by observing its behavior, but IRL algorithms that
learn point estimates of the reward function can be misleading because there
may be several functions that describe an agent's behavior equally well. A
Bayesian approach to IRL models a distribution over candidate reward functions,
alleviating the shortcomings of learning a point estimate. However, several
Bayesian IRL algorithms use a $Q$-value function in place of the likelihood
function. The resulting posterior is computationally intensive to calculate,
has few theoretical guarantees, and the $Q$-value function is often a poor
approximation for the likelihood. We introduce kernel density Bayesian IRL
(KD-BIRL), which uses conditional kernel density estimation to directly
approximate the likelihood, providing an efficient framework that, with a
modified reward function parameterization, is applicable to environments with
complex and infinite state spaces. We demonstrate KD-BIRL's benefits through a
series of experiments in Gridworld environments and a simulated sepsis
treatment task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best-of-three-worlds Analysis for Linear Bandits with
  Follow-the-regularized-leader Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Kong, Canzhe Zhao, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The linear bandit problem has been studied for many years in both stochastic
and adversarial settings. Designing an algorithm that can optimize the
environment without knowing the loss type attracts lots of interest.
\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and
then switches between different algorithms specially designed for different
settings. However, such an approach requires meticulous designs to perform well
in all settings. Follow-the-regularized-leader (FTRL) is another popular
algorithm type that can adapt to different environments. This algorithm is of
simple design and the regret bounds are shown to be optimal in traditional
multi-armed bandit problems compared with the detect-switch type algorithms.
Designing an FTRL-type algorithm for linear bandits is an important question
that has been open for a long time. In this paper, we prove that the FTRL-type
algorithm with a negative entropy regularizer can achieve the
best-of-three-world results for the linear bandit problem with the tacit
cooperation between the choice of the learning rate and the specially designed
self-bounding inequality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoor Defense via Deconfounded Representation Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor
attacks, where attackers embed hidden backdoors in the DNN model by injecting a
few poisoned examples into the training dataset. While extensive efforts have
been made to detect and remove backdoors from backdoored DNNs, it is still not
clear whether a backdoor-free clean model can be directly obtained from
poisoned datasets. In this paper, we first construct a causal graph to model
the generation process of poisoned data and find that the backdoor attack acts
as the confounder, which brings spurious associations between the input images
and target labels, making the model predictions less reliable. Inspired by the
causal understanding, we propose the Causality-inspired Backdoor Defense (CBD),
to learn deconfounded representations for reliable classification.
Specifically, a backdoored model is intentionally trained to capture the
confounding effects. The other clean model dedicates to capturing the desired
causal effects by minimizing the mutual information with the confounding
representations from the backdoored model and employing a sample-wise
re-weighting scheme. Extensive experiments on multiple benchmark datasets
against 6 state-of-the-art attacks verify that our proposed defense method is
effective in reducing backdoor threats while maintaining high accuracy in
predicting benign samples. Further analysis shows that CBD can also resist
potential adaptive attacks. The code is available at
\url{https://github.com/zaixizhang/CBD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Convergence of Tensor Decomposition-Based Neural Network
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Li, Bo Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced tensor decomposition, such as tensor train (TT), has been widely
studied for tensor decomposition-based neural network (NN) training, which is
one of the most common model compression methods. However, training NN with
tensor decomposition always suffers significant accuracy loss and convergence
issues. In this paper, a holistic framework is proposed for tensor
decomposition-based NN training by formulating TT decomposition-based NN
training as a nonconvex optimization problem. This problem can be solved by the
proposed tensor block coordinate descent (tenBCD) method, which is a
gradient-free algorithm. The global convergence of tenBCD to a critical point
at a rate of O(1/k) is established with the Kurdyka {\L}ojasiewicz (K{\L})
property, where k is the number of iterations. The theoretical results can be
extended to the popular residual neural networks (ResNets). The effectiveness
and efficiency of our proposed framework are verified through an image
classification dataset, where our proposed method can converge efficiently in
training and prevent overfitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2107.12422 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Process on the Product of Directional Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Cao, Kailai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a principled study on establishing Gaussian processes over
variables on the product of directional manifolds. As a basic functional
component, a manifold-adaptive kernel is presented based on the von Mises
distribution for Gaussian process regression on unit circles. Afterward, a
novel hypertoroidal von Mises kernel is introduced to enable topology-aware
Gaussian processes on hypertori with consideration of correlational circular
components. Based thereon, we enable multi-output regression for learning
vector-valued functions on hypertori using intrinsic coregionalization model
and provide analytical derivatives in hyperparameter optimization. The proposed
multi-output hypertoroidal Gaussian process is further embedded to a
data-driven recursive estimation scheme for learning unknown range sensing
models of angle-of-arrival inputs. Evaluations on range-based localization show
that the proposed scheme enables superior tracking accuracy over parametric
modeling and common Gaussian processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization with access to auxiliary information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Mahdi Chayti, Sai Praneeth Karimireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the fundamental optimization question of minimizing a target
function $f(x)$ whose gradients are expensive to compute or have limited
availability, given access to some auxiliary side function $h(x)$ whose
gradients are cheap or more available. This formulation captures many settings
of practical relevance such as i) re-using batches in SGD, ii) transfer
learning, iii) federated learning, iv) training with compressed models/dropout,
etc. We propose two generic new algorithms which are applicable in all these
settings and prove using only an assumption on the Hessian similarity between
the target and side information that we can benefit from this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We corrected a mistake that we had in Lemma 9 in the previous version
  of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Laplacian Features for Learning with Hyperbolic Space <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06854v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06854v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yu, Christopher De Sa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its geometric properties, hyperbolic space can support high-fidelity
embeddings of tree- and graph-structured data, upon which various hyperbolic
networks have been developed. Existing hyperbolic networks encode geometric
priors not only for the input, but also at every layer of the network. This
approach involves repeatedly mapping to and from hyperbolic space, which makes
these networks complicated to implement, computationally expensive to scale,
and numerically unstable to train. In this paper, we propose a simpler
approach: learn a hyperbolic embedding of the input, then map once from it to
Euclidean space using a mapping that encodes geometric priors by respecting the
isometries of hyperbolic space, and finish with a standard Euclidean network.
The key insight is to use a random feature mapping via the eigenfunctions of
the Laplace operator, which we show can approximate any isometry-invariant
kernel on hyperbolic space. Our method can be used together with any graph
neural networks: using even a linear graph model yields significant
improvements in both efficiency and performance over other hyperbolic baselines
in both transductive and inductive tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PØDA: <span class="highlight-title">Prompt</span>-driven Zero-shot Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation has been vastly investigated in computer vision but still
requires access to target images at train time, which might be intractable in
some uncommon conditions. In this paper, we propose the task of `Prompt-driven
Zero-shot Domain Adaptation', where we adapt a model trained on a source domain
using only a single general textual description of the target domain, i.e., a
prompt. First, we leverage a pretrained contrastive vision-language model
(CLIP) to optimize affine transformations of source features, steering them
towards target text embeddings, while preserving their content and semantics.
Second, we show that augmented features can be used to perform zero-shot domain
adaptation for semantic segmentation. Experiments demonstrate that our method
significantly outperforms CLIP-based style transfer baselines on several
datasets for the downstream task at hand. Our prompt-driven approach even
outperforms one-shot unsupervised domain adaptation on some datasets, and gives
comparable results on others. Our code is available at
https://github.com/astra-vision/PODA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://astra-vision.github.io/PODA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Vision, Text, and Layout for Universal Document Processing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and
  Loopy Belief Propagation in JAX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04110v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04110v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zhou, Antoine Dedieu, Nishanth Kumar, Miguel Lázaro-Gredilla, Shrinu Kushagra, Dileep George
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PGMax is an open-source Python package for (a) easily specifying discrete
Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically
running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax
supports general factor graphs with tractable factors, and leverages modern
accelerators like GPUs for inference. Compared with existing alternatives,
PGMax obtains higher-quality inference results with up to three
orders-of-magnitude inference time speedups. PGMax additionally interacts
seamlessly with the rapidly growing JAX ecosystem, opening up new research
possibilities. Our source code, examples and documentation are available at
https://github.com/deepmind/PGMax.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update authors list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual-Neural-Physical Sound Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Han, Vincent Lostanlen, Mathieu Lagrange
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound matching algorithms seek to approximate a target waveform by parametric
audio synthesis. Deep neural networks have achieved promising results in
matching sustained harmonic tones. However, the task is more challenging when
targets are nonstationary and inharmonic, e.g., percussion. We attribute this
problem to the inadequacy of loss function. On one hand, mean square error in
the parametric domain, known as "P-loss", is simple and fast but fails to
accommodate the differing perceptual significance of each parameter. On the
other hand, mean square error in the spectrotemporal domain, known as "spectral
loss", is perceptually motivated and serves in differentiable digital signal
processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals
and its gradient may be computationally expensive; hence a slow convergence.
Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP
is the optimal quadratic approximation of spectral loss while being as fast as
P-loss during training. We instantiate PNP with physical modeling synthesis as
decoder and joint time-frequency scattering transform (JTFS) as spectral
representation. We demonstrate its potential on matching synthetic drum sounds
in comparison with other loss functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Frequency Joint Community Detection and Phase Synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingda Wang, Zhizhen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the joint community detection and phase synchronization
problem on the stochastic block model with relative phase, where each node is
associated with an unknown phase angle. This problem, with a variety of
real-world applications, aims to recover the cluster structure and associated
phase angles simultaneously. We show this problem exhibits a
``multi-frequency'' structure by closely examining its maximum likelihood
estimation (MLE) formulation, whereas existing methods are not originated from
this perspective. To this end, two simple yet efficient algorithms that
leverage the MLE formulation and benefit from the information across multiple
frequencies are proposed. The former is a spectral method based on the novel
multi-frequency column-pivoted QR factorization. The factorization applied to
the top eigenvectors of the observation matrix provides key information about
the cluster structure and associated phase angles. The second approach is an
iterative multi-frequency generalized power method, where each iteration
updates the estimation in a matrix-multiplication-then-projection manner.
Numerical experiments show that our proposed algorithms significantly improve
the ability of exactly recovering the cluster structure and the accuracy of the
estimated phase angles, compared to state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Signal and Information Processing
  over Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mira, Buye Xu, Jacob Donley, Anurag Kumar, Stavros Petridis, Vamsi Krishna Ithapu, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech enhancement aims to extract clean speech from a noisy
environment by leveraging not only the audio itself but also the target
speaker's lip movements. This approach has been shown to yield improvements
over audio-only speech enhancement, particularly for the removal of interfering
speech. Despite recent advances in speech synthesis, most audio-visual
approaches continue to use spectral mapping/masking to reproduce the clean
audio, often resulting in visual backbones added to existing speech enhancement
architectures. In this work, we propose LA-VocE, a new two-stage approach that
predicts mel-spectrograms from noisy audio-visual speech via a
transformer-based architecture, and then converts them into waveform audio
using a neural vocoder (HiFi-GAN). We train and evaluate our framework on
thousands of speakers and 11+ different languages, and study our model's
ability to adapt to different levels of background noise and speech
interference. Our experiments show that LA-VocE outperforms existing methods
according to multiple metrics, particularly under very noisy scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ State-Conditioned Adversarial Subgoal <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09635v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09635v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivienne Huiling Wang, Joni Pajarinen, Tinghuai Wang, Joni-Kristian Kämäräinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks
by performing decision-making and control at successively higher levels of
temporal abstraction. However, off-policy HRL often suffers from the problem of
a non-stationary high-level policy since the low-level policy is constantly
changing. In this paper, we propose a novel HRL approach for mitigating the
non-stationarity by adversarially enforcing the high-level policy to generate
subgoals compatible with the current instantiation of the low-level policy. In
practice, the adversarial learning is implemented by training a simple
state-conditioned discriminator network concurrently with the high-level policy
which determines the compatibility level of subgoals. Comparison to
state-of-the-art algorithms shows that our approach improves both learning
efficiency and performance in challenging continuous control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Audio Features with Metadata and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Moummad, Nicolas Farrugia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods based on supervised learning using annotations in an end-to-end
fashion have been the state-of-the-art for classification problems. However,
they may be limited in their generalization capability, especially in the low
data regime. In this study, we address this issue using supervised contrastive
learning combined with available metadata to solve multiple pretext tasks that
learn a good representation of data. We apply our approach on ICBHI, a
respiratory sound classification dataset suited for this setting. We show that
learning representations using only metadata, without class labels, obtains
similar performance as using cross entropy with those labels only. In addition,
we obtain state-of-the-art score when combining class labels with metadata
using multiple supervised contrastive learning. This work suggests the
potential of using multiple metadata sources in supervised contrastive
settings, in particular in settings with class imbalance and few data. Our code
is released at https://github.com/ilyassmoummad/scl_icbhi2017
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinical <span class="highlight-title">BERT</span>Score: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial random forests for density estimation and generative
  modeling <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09435v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09435v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David S. Watson, Kristin Blesch, Jan Kapar, Marvin N. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose methods for density estimation and data synthesis using a novel
form of unsupervised random forests. Inspired by generative adversarial
networks, we implement a recursive procedure in which trees gradually learn
structural properties of the data through alternating rounds of generation and
discrimination. The method is provably consistent under minimal assumptions.
Unlike classic tree-based alternatives, our approach provides smooth
(un)conditional densities and allows for fully synthetic data generation. We
achieve comparable or superior performance to state-of-the-art probabilistic
circuits and deep learning models on various tabular data benchmarks while
executing about two orders of magnitude faster on average. An accompanying
$\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version (AISTATS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks on SPD Manifolds for Motor Imagery Classification:
  A Perspective from the Time-Frequency Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Ju, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of motor imagery (MI) is a highly sought-after research
topic in the field of Electroencephalography (EEG)-based brain-computer
interfaces (BCIs), with immense commercial value. Over the past two decades,
there has been a fundamental shift in the trend of MI-EEG classifiers,
resulting in a gradual increase in their performance. The emergence of
Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI
research, is attributed to the imperative of characterizing the non-Euclidean
nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based
classifier that capitalizes on the second-order statistics of EEGs. In contrast
to the conventional approach of utilizing first-order statistics for EEG
signals, the utilization of these second-order statistics represents the
classical treatment. These statistics provide adequate discriminative
information, rendering them suitable for MI-EEG classification. In this study,
we introduce another GDL classifier, called Graph-CSPNet, for MI-EEG
classification. Graph-CSPNet utilizes graph-based techniques to characterize
EEG signals in both the time and frequency domains, realizing the fundamental
perspective of time-frequency analysis. The architecture of Graph-CSPNet is
further simplified, offering greater flexibility to cope with variable
time-frequency resolution for signal segmentation and capturing localized
fluctuations. In contrast to Tensor-CSPNet, this approach enables Graph-CSPNet
to achieve better results in MI-EEG classification. To evaluate the efficacy of
Graph-CSPNet, we utilize five commonly-used publicly available MI-EEG datasets,
and it produces near-optimal classification accuracies, winning nine out of
eleven subject-specific scenarios. The Python implementation of Graph-CSPNet is
available on a GitHub repository
https://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, 11 Tables; This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Optimization-based Combinatorial Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14698v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14698v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Weissteiner, Jakob Heiss, Julien Siems, Sven Seuken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the combinatorial assignment domain, which includes combinatorial
auctions and course allocation. The main challenge in this domain is that the
bundle space grows exponentially in the number of items. To address this,
several papers have recently proposed machine learning-based preference
elicitation algorithms that aim to elicit only the most important information
from agents. However, the main shortcoming of this prior work is that it does
not model a mechanism's uncertainty over values for not yet elicited bundles.
In this paper, we address this shortcoming by presenting a Bayesian
optimization-based combinatorial assignment (BOCA) mechanism. Our key technical
contribution is to integrate a method for capturing model uncertainty into an
iterative combinatorial auction mechanism. Concretely, we design a new method
for estimating an upper uncertainty bound that can be used to define an
acquisition function to determine the next query to the agents. This enables
the mechanism to properly explore (and not just exploit) the bundle space
during its preference elicitation phase. We run computational experiments in
several spectrum auction domains to evaluate BOCA's performance. Our results
show that BOCA achieves higher allocative efficiency than state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Short-Term Density Forecasting of Low-Voltage Load using
  Bernstein-Polynomial Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Arpogaus, Marcus Voss, Beate Sick, Mark Nigge-Uricher, Oliver Dürr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transition to a fully renewable energy grid requires better forecasting
of demand at the low-voltage level to increase efficiency and ensure reliable
control. However, high fluctuations and increasing electrification cause huge
forecast variability, not reflected in traditional point estimates.
Probabilistic load forecasts take future uncertainties into account and thus
allow more informed decision-making for the planning and operation of
low-carbon energy systems. We propose an approach for flexible conditional
density forecasting of short-term load based on Bernstein polynomial
normalizing flows, where a neural network controls the parameters of the flow.
In an empirical study with 363 smart meter customers, our density predictions
compare favorably against Gaussian and Gaussian mixture densities. Also, they
outperform a non-parametric approach based on the pinball loss for 24h-ahead
load forecasting for two different neural network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Goel, Nathan Michael, Wennie Tabib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter presents a continuous probabilistic modeling methodology for
spatial point cloud data using finite Gaussian Mixture Models (GMMs) where the
number of components are adapted based on the scene complexity. Few
hierarchical and adaptive methods have been proposed to address the challenge
of balancing model fidelity with size. Instead, state-of-the-art mapping
approaches require tuning parameters for specific use cases, but do not
generalize across diverse environments. To address this gap, we utilize a
self-organizing principle from information-theoretic learning to automatically
adapt the complexity of the GMM model based on the relevant information in the
sensor data. The approach is evaluated against existing point cloud modeling
techniques on real-world data with varying degrees of scene complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, to appear in IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Tester-Learner for Halfspaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give the first efficient algorithm for learning halfspaces in the testable
learning model recently defined by Rubinfeld and Vasilyan (2023). In this
model, a learner certifies that the accuracy of its output hypothesis is near
optimal whenever the training set passes an associated test, and training sets
drawn from some target distribution -- e.g., the Gaussian -- must pass the
test. This model is more challenging than distribution-specific agnostic or
Massart noise models where the learner is allowed to fail arbitrarily if the
distributional assumption does not hold.
  We consider the setting where the target distribution is Gaussian (or more
generally any strongly log-concave distribution) in $d$ dimensions and the
noise model is either Massart or adversarial (agnostic). For Massart noise, our
tester-learner runs in polynomial time and outputs a hypothesis with
(information-theoretically optimal) error $\mathsf{opt} + \epsilon$ for any
strongly log-concave target distribution. For adversarial noise, our
tester-learner obtains error $O(\mathsf{opt}) + \epsilon$ in polynomial time
when the target distribution is Gaussian; for strongly log-concave
distributions, we obtain $\tilde{O}(\mathsf{opt}) + \epsilon$ in
quasipolynomial time.
  Prior work on testable learning ignores the labels in the training set and
checks that the empirical moments of the covariates are close to the moments of
the base distribution. Here we develop new tests of independent interest that
make critical use of the labels and combine them with the moment-matching
approach of Gollakota et al. (2023). This enables us to simulate a variant of
the algorithm of Diakonikolas et al. (2020) for learning noisy halfspaces using
nonconvex SGD but in the testable learning setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 3 figures, Version v2: strengthened the agnostic guarantee</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feihu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the paper, we study a class of nonconvex nonconcave minimax optimization
problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in
$x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition
in $y$. Moreover, we propose a class of enhanced momentum-based gradient
descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic
Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use
various adaptive learning rates in updating the variables $x$ and $y$ without
relying on any global and coordinate-wise adaptive learning rates.
Theoretically, we present an effective convergence analysis framework for our
methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the
best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring
one sample at each loop in finding an $\epsilon$-stationary solution (i.e.,
$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This
manuscript commemorates the mathematician Boris Polyak (1935-2023).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynLight: Realize dynamic phase duration with multi-level traffic signal
  control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Zhang, Shubin Xie, Jianming Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We would like to withdraw this article for the following reasons: 1 this
article is not satisfactory for limited language and theoretical description; 2
we have enriched and revised this article with the help of other authors; 3 we
must update the author contribution information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We would like to withdraw this article for the following reasons: 1
  this article is not satisfactory for limited language and theoretical
  description; 2 we have enriched and revised this article with the help of
  other authors; 3 we must update the author contribution information. PLease
  see: arXiv:2211.01025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry Defense Against CNN Adversarial Perturbation Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blerta Lindqvist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network classifiers (CNNs) are susceptible to
adversarial attacks that perturb original samples to fool classifiers such as
an autonomous vehicle's road sign image classifier. CNNs also lack invariance
in the classification of symmetric samples because CNNs can classify symmetric
samples differently. Considered together, the CNN lack of adversarial
robustness and the CNN lack of invariance mean that the classification of
symmetric adversarial samples can differ from their incorrect classification.
Could symmetric adversarial samples revert to their correct classification?
This paper answers this question by designing a symmetry defense that inverts
or horizontally flips adversarial samples before classification against
adversaries unaware of the defense. Against adversaries aware of the defense,
the defense devises a Klein four symmetry subgroup that includes the horizontal
flip and pixel inversion symmetries. The symmetry defense uses the subgroup
symmetries in accuracy evaluation and the subgroup closure property to confine
the transformations that an adaptive adversary can apply before or after
generating the adversarial sample. Without changing the preprocessing,
parameters, or model, the proposed symmetry defense counters the Projected
Gradient Descent (PGD) and AutoAttack attacks with near-default accuracies for
ImageNet. Without using attack knowledge or adversarial samples, the proposed
defense exceeds the current best defense, which trains on adversarial samples.
The defense maintains and even improves the classification accuracy of
non-adversarial samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PreFallKD: Pre-Impact Fall Detection via CNN-ViT <span class="highlight-title">Knowledge</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin-Han Chi, Kai-Chun Liu, Chia-Yeh Hsieh, Yu Tsao, Chia-Tai Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fall accidents are critical issues in an aging and aged society. Recently,
many researchers developed pre-impact fall detection systems using deep
learning to support wearable-based fall protection systems for preventing
severe injuries. However, most works only employed simple neural network models
instead of complex models considering the usability in resource-constrained
mobile devices and strict latency requirements. In this work, we propose a
novel pre-impact fall detection via CNN-ViT knowledge distillation, namely
PreFallKD, to strike a balance between detection performance and computational
complexity. The proposed PreFallKD transfers the detection knowledge from the
pre-trained teacher model (vision transformer) to the student model
(lightweight convolutional neural networks). Additionally, we apply data
augmentation techniques to tackle issues of data imbalance. We conduct the
experiment on the KFall public dataset and compare PreFallKD with other
state-of-the-art models. The experiment results show that PreFallKD could boost
the student model during the testing phase and achieves reliable F1-score
(92.66%) and lead time (551.3 ms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PDEBENCH: An Extensive Benchmark for Scientific Machine Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07182v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07182v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pflüger, Mathias Niepert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning-based modeling of physical systems has experienced increased
interest in recent years. Despite some impressive progress, there is still a
lack of benchmarks for Scientific ML that are easy to use but still challenging
and representative of a wide range of problems. We introduce PDEBench, a
benchmark suite of time-dependent simulation tasks based on Partial
Differential Equations (PDEs). PDEBench comprises both code and data to
benchmark the performance of novel machine learning models against both
classical numerical simulations and machine learning baselines. Our proposed
set of benchmark problems contribute the following unique features: (1) A much
wider range of PDEs compared to existing benchmarks, ranging from relatively
common examples to more realistic and difficult problems; (2) much larger
ready-to-use datasets compared to prior work, comprising multiple simulation
runs across a larger number of initial and boundary conditions and PDE
parameters; (3) more extensible source codes with user-friendly APIs for data
generation and baseline results with popular machine learning models (FNO,
U-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to
extend the benchmark freely for their own purposes using a standardized API and
to compare the performance of new models to existing baseline methods. We also
propose new evaluation metrics with the aim to provide a more holistic
understanding of learning methods in the context of Scientific ML. With those
metrics we identify tasks which are challenging for recent ML methods and
propose these tasks as future challenges for the community. The code is
available at https://github.com/pdebench/PDEBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (main body) + 34 pages (supplemental material), accepted for
  publication in NeurIPS 2022 Track Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-Sleep's resilience to AASM guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Fiorillo, Giuliana Monachino, Julia van der Meer, Marco Pesce, Jan D. Warncke, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Paolo Favaro, Francesca D. Faraci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AASM guidelines are the result of decades of efforts aiming at standardizing
sleep scoring procedure, with the final goal of sharing a worldwide common
methodology. The guidelines cover several aspects from the technical/digital
specifications,e.g., recommended EEG derivations, to detailed sleep scoring
rules accordingly to age. Automated sleep scoring systems have always largely
exploited the standards as fundamental guidelines. In this context, deep
learning has demonstrated better performance compared to classical machine
learning. Our present work shows that a deep learning based sleep scoring
algorithm may not need to fully exploit the clinical knowledge or to strictly
adhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a
state-of-the-art sleep scoring algorithm, can be strong enough to solve the
scoring task even using clinically non-recommended or non-conventional
derivations, and with no need to exploit information about the chronological
age of the subjects. We finally strengthen a well-known finding that using data
from multiple data centers always results in a better performing model compared
with training on a single cohort. Indeed, we show that this latter statement is
still valid even by increasing the size and the heterogeneity of the single
data cohort. In all our experiments we used 28528 polysomnography studies from
13 different clinical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A attention way in Explainable methods for infant brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying reliable deep learning techniques in interdisciplinary applications
needs learned models to output accurate and ({even more importantly})
explainable predictions. Existing approaches typically explicate network
outputs in a post-hoc fashion, under an implicit assumption that faithful
explanations come from accurate predictions/classifications. We have an
opposite claim that explanations boost (or even determine) classification. That
is, end-to-end learning of explanation factors to augment discriminative
representation extraction could be a more intuitive strategy to inversely
assure fine-grained explainability, e.g., in those neuroimaging and
neuroscience studies with high-dimensional data containing noisy, redundant,
and task-irrelevant information. In this paper, we propose such an explainable
geometric deep network dubbed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some parts of the thesis are still being revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomaly Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charanjit K. Khosa, Veronica Sanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new algorithm for anomaly detection called Anomaly Awareness.
The algorithm learns about normal events while being made aware of the
anomalies through a modification of the cost function. We show how this method
works in different Particle Physics situations and in standard Computer Vision
tasks. For example, we apply the method to images from a Fat Jet topology
generated by Standard Model Top and QCD events, and test it against an array of
new physics scenarios, including Higgs production with EFT effects and
resonances decaying into two, three or four subjets. We find that the algorithm
is effective identifying anomalies not seen before, and becomes robust as we
make it aware of a varied-enough set of anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Federated Learning via Contrastive Representation Ensemble <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, Jingjing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing amount of multimedia data on modern mobile systems and
IoT infrastructures, harnessing these rich multimodal data without breaching
user privacy becomes a critical issue. Federated learning (FL) serves as a
privacy-conscious alternative to centralized machine learning. However,
existing FL methods extended to multimodal data all rely on model aggregation
on single modality level, which restrains the server and clients to have
identical model architecture for each modality. This limits the global model in
terms of both model complexity and data capacity, not to mention task
diversity. In this work, we propose Contrastive Representation Ensemble and
Aggregation for Multimodal FL (CreamFL), a multimodal federated learning
framework that enables training larger server models from clients with
heterogeneous model architectures and data modalities, while only communicating
knowledge on public dataset. To achieve better multimodal representation
fusion, we design a global-local cross-modal ensemble strategy to aggregate
client representations. To mitigate local model drift caused by two
unprecedented heterogeneous factors stemming from multimodal discrepancy
(modality gap and task gap), we further propose two inter-modal and intra-modal
contrasts to regularize local training, which complements information of the
absent modality for uni-modal clients and regularizes local clients to head
towards global consensus. Thorough evaluations and ablation studies on
image-text retrieval and visual question answering tasks showcase the
superiority of CreamFL over state-of-the-art FL methods and its practical
value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Code is available at https://github.com/FLAIR-THU/CreamFL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Boosting Performs Gaussian Process Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05608v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05608v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksei Ustimenko, Artem Beliakov, Liudmila Prokhorenkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows that gradient boosting based on symmetric decision trees can
be equivalently reformulated as a kernel method that converges to the solution
of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence
to a Gaussian Process' posterior mean, which, in turn, allows us to easily
transform gradient boosting into a sampler from the posterior to provide better
knowledge uncertainty estimates through Monte-Carlo estimation of the posterior
variance. We show that the proposed sampler allows for better knowledge
uncertainty estimates leading to improved out-of-domain detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient ECG-based Atrial Fibrillation Detection via Parameterised
  Hypercomplex Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Basso, Zhao Ren, Wolfgang Nejdl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated
with a high risk for serious conditions like stroke. The use of wearable
devices embedded with automatic and timely AF assessment from
electrocardiograms (ECGs) has shown to be promising in preventing
life-threatening situations. Although deep neural networks have demonstrated
superiority in model performance, their use on wearable devices is limited by
the trade-off between model performance and complexity. In this work, we
propose to use lightweight convolutional neural networks (CNNs) with
parameterised hypercomplex (PH) layers for AF detection based on ECGs. The
proposed approach trains small-scale CNNs, thus overcoming the limited
computing resources on wearable devices. We show comparable performance to
corresponding real-valued CNNs on two publicly available ECG datasets using
significantly fewer model parameters. PH models are more flexible than other
hypercomplex neural networks and can operate on any number of input ECG leads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised paper organisation. Further experiments to emphasise flexible
  model compression and comparison with other baselines</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and
  Envelope-based Features for Machinery Fault Detection <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Wißbrock, Yvonne Richter, David Pelkmann, Zhao Ren, Gregory Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic-based fault detection has a high potential to monitor the health
condition of mechanical parts. However, the background noise of an industrial
environment may negatively influence the performance of fault detection.
Limited attention has been paid to improving the robustness of fault detection
against industrial environmental noise. Therefore, we present the Lenze
production background-noise (LPBN) real-world dataset and an automated and
noise-robust auditory inspection (ARAI) system for the end-of-line inspection
of geared motors. An acoustic array is used to acquire data from motors with a
minor fault, major fault, or which are healthy. A benchmark is provided to
compare the psychoacoustic features with different types of envelope features
based on expert knowledge of the gearbox. To the best of our knowledge, we are
the first to apply time-varying psychoacoustic features for fault detection. We
train a state-of-the-art one-class-classifier, on samples from healthy motors
and separate the faulty ones for fault detection using a threshold. The
best-performing approaches achieve an area under curve of 0.87 (logarithm
envelope), 0.86 (time-varying psychoacoustics), and 0.91 (combination of both).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the final published version at ICASSP 2023 include small additional
  content as well as some minor revisions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-Based Interpolation and Geodesics in the Latent Spaces of
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1904.03445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1904.03445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Struski, Michał Sadowski, Tomasz Danel, Jacek Tabor, Igor T. Podolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpolating between points is a problem connected simultaneously with
finding geodesics and study of generative models. In the case of geodesics, we
search for the curves with the shortest length, while in the case of generative
models we typically apply linear interpolation in the latent space. However,
this interpolation uses implicitly the fact that Gaussian is unimodal. Thus the
problem of interpolating in the case when the latent density is non-Gaussian is
an open problem.
  In this paper, we present a general and unified approach to interpolation,
which simultaneously allows us to search for geodesics and interpolating curves
in latent space in the case of arbitrary density. Our results have a strong
theoretical background based on the introduced quality measure of an
interpolating curve. In particular, we show that maximising the quality measure
of the curve can be equivalently understood as a search of geodesic for a
certain redefinition of the Riemannian metric on the space.
  We provide examples in three important cases. First, we show that our
approach can be easily applied to finding geodesics on manifolds. Next, we
focus our attention in finding interpolations in pre-trained generative models.
We show that our model effectively works in the case of arbitrary density.
Moreover, we can interpolate in the subset of the space consisting of data
possessing a given feature. The last case is focused on finding interpolation
in the space of chemical compounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Sparse Graphon Mean Field Games <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03880v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03880v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Fabian, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the field of multi-agent reinforcement learning (MARL) has made
considerable progress in the last years, solving systems with a large number of
agents remains a hard challenge. Graphon mean field games (GMFGs) enable the
scalable analysis of MARL problems that are otherwise intractable. By the
mathematical structure of graphons, this approach is limited to dense graphs
which are insufficient to describe many real-world networks such as power law
graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs,
which leverages the graph theoretical concept of $L^p$ graphons and provides a
machine learning tool to efficiently and accurately approximate solutions for
sparse network problems. This especially includes power law networks which are
empirically observed in various application areas and cannot be captured by
standard graphons. We derive theoretical existence and convergence guarantees
and give empirical examples that demonstrate the accuracy of our learning
approach for systems with many agents. Furthermore, we extend the Online Mirror
Descent (OMD) learning algorithm to our setup to accelerate learning speed,
empirically show its capabilities, and conduct a theoretical analysis using the
novel concept of smoothed step graphons. In general, we provide a scalable,
mathematically well-founded machine learning approach to a large class of
otherwise intractable problems of great relevance in numerous research fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at the International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2023; code available at:
  https://github.com/ChrFabian/Learning_sparse_GMFGs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning by Detecting Incorrect Location Embeddings <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Sameni, Simon Jenni, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel self-supervised learning (SSL) loss for
image representation learning. There is a growing belief that generalization in
deep neural networks is linked to their ability to discriminate object shapes.
Since object shape is related to the location of its parts, we propose to
detect those that have been artificially misplaced. We represent object parts
with image tokens and train a ViT to detect which token has been combined with
an incorrect positional embedding. We then introduce sparsity in the inputs to
make the model more robust to occlusions and to speed up the training. We call
our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings
with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an
improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under
the same training time and with a linear probing transfer on ImageNet-1K. We
also show full fine-tuning improvements of MAE combined with our method on
ImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.
Moreover, we show that when downstream tasks are strongly reliant on shape
(such as in the YOGA-82 pose dataset), our pre-trained features yield a
significant gain over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI2023, https://github.com/Separius/DILEMMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Non-Linear Quantum Operations through Variational Quantum
  Splines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Antonio Inajetovic, Filippo Orazi, Antonio Macaluso, Stefano Lodi, Claudio Sartori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The postulates of quantum mechanics impose only unitary transformations on
quantum states, which is a severe limitation for quantum machine learning
algorithms. Quantum Splines (QSplines) have recently been proposed to
approximate quantum activation functions to introduce non-linearity in quantum
algorithms. However, QSplines make use of the HHL as a subroutine and require a
fault-tolerant quantum computer to be correctly implemented. This work proposes
the Generalised QSplines (GQSplines), a novel method for approximating
non-linear quantum activation functions using hybrid quantum-classical
computation. The GQSplines overcome the highly demanding requirements of the
original QSplines in terms of quantum hardware and can be implemented using
near-term quantum computers. Furthermore, the proposed method relies on a
flexible problem representation for non-linear approximation and it is suitable
to be embedded in existing quantum neural network architectures. In addition,
we provide a practical implementation of GQSplines using Pennylane and show
that our model outperforms the original QSplines in terms of quality of
fitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarially Regularized Graph Attention Networks for Inductive
  Learning on Partially Labeled Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaren Xiao, Quanyu Dai, Xiaochen Xie, James Lam, Ka-Wai Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high cost of data labeling often results in node label shortage in real
applications. To improve node classification accuracy, graph-based
semi-supervised learning leverages the ample unlabeled nodes to train together
with the scarce available labeled nodes. However, most existing methods require
the information of all nodes, including those to be predicted, during model
training, which is not practical for dynamic graphs with newly added nodes. To
address this issue, an adversarially regularized graph attention model is
proposed to classify newly added nodes in a partially labeled graph. An
attention-based aggregator is designed to generate the representation of a node
by aggregating information from its neighboring nodes, thus naturally
generalizing to previously unseen nodes. In addition, adversarial training is
employed to improve the model's robustness and generalization ability by
enforcing node representations to match a prior distribution. Experiments on
real-world datasets demonstrate the effectiveness of the proposed method in
comparison with the state-of-the-art methods. The code is available at
https://github.com/JiarenX/AGAIN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PSVRF: Learning to restore Pitch-Shifted Voice without reference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangfu Li, Xiaodan Lin, Jiaxin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pitch scaling algorithms have a significant impact on the security of
Automatic Speaker Verification (ASV) systems. Although numerous anti-spoofing
algorithms have been proposed to identify the pitch-shifted voice and even
restore it to the original version, they either have poor performance or
require the original voice as a reference, limiting the prospects of
applications. In this paper, we propose a no-reference approach termed
PSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on
AISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised
by various pitch-scaling techniques, which obviously enhances the robustness of
ASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF
even surpasses that of the state-of-the-art reference-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Have some errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a model is paramount for sample efficiency in reinforcement
  learning control of PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Werner, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this paper is to make a strong point for the usage of dynamical
models when using reinforcement learning (RL) for feedback control of dynamical
systems governed by partial differential equations (PDEs). To breach the gap
between the immense promises we see in RL and the applicability in complex
engineering systems, the main challenges are the massive requirements in terms
of the training data, as well as the lack of performance guarantees. We present
a solution for the first issue using a data-driven surrogate model in the form
of a convolutional LSTM with actuation. We demonstrate that learning an
actuated model in parallel to training the RL agent significantly reduces the
total amount of required data sampled from the real system. Furthermore, we
show that iteratively updating the model is of major importance to avoid biases
in the RL training. Detailed ablation studies reveal the most important
ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky
equation do demonstarte our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnbiasedNets: A <span class="highlight-title">Dataset</span> Diversification Framework for Robustness Bias
  Alleviation in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahum Naseer, Bharath Srinivas Prabakaran, Osman Hasan, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of trained neural network (NN) models, in terms of testing
accuracy, has improved remarkably over the past several years, especially with
the advent of deep learning. However, even the most accurate NNs can be biased
toward a specific output classification due to the inherent bias in the
available training datasets, which may propagate to the real-world
implementations. This paper deals with the robustness bias, i.e., the bias
exhibited by the trained NN by having a significantly large robustness to noise
for a certain output class, as compared to the remaining output classes. The
bias is shown to result from imbalanced datasets, i.e., the datasets where all
output classes are not equally represented. Towards this, we propose the
UnbiasedNets framework, which leverages K-means clustering and the NN's noise
tolerance to diversify the given training dataset, even from relatively smaller
datasets. This generates balanced datasets and reduces the bias within the
datasets themselves. To the best of our knowledge, this is the first framework
catering to the robustness bias problem in NNs. We use real-world datasets to
demonstrate the efficacy of the UnbiasedNets for data diversification, in case
of both binary and multi-label classifiers. The results are compared to
well-known tools aimed at generating balanced datasets, and illustrate how
existing works have limited success while addressing the robustness bias. In
contrast, UnbiasedNets provides a notable improvement over existing works,
while even reducing the robustness bias significantly in some cases, as
observed by comparing the NNs trained on the diversified and original datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Springer Machine Learning 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamical System View of Langevin-Based Non-Convex Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Karimi, Ya-Ping Hsieh, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-convex sampling is a key challenge in machine learning, central to
non-convex optimization in deep learning as well as to approximate
probabilistic inference. Despite its significance, theoretically there remain
many important challenges: Existing guarantees (1) typically only hold for the
averaged iterates rather than the more desirable last iterates, (2) lack
convergence metrics that capture the scales of the variables such as
Wasserstein distances, and (3) mainly apply to elementary schemes such as
stochastic gradient Langevin dynamics. In this paper, we develop a new
framework that lifts the above issues by harnessing several tools from the
theory of dynamical systems. Our key result is that, for a large class of
state-of-the-art sampling schemes, their last-iterate convergence in
Wasserstein distances can be reduced to the study of their continuous-time
counterparts, which is much better understood. Coupled with standard
assumptions of MCMC sampling, our theory immediately yields the last-iterate
Wasserstein convergence of many advanced sampling schemes such as proximal,
randomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our
framework also motivates more efficient schemes that enjoy the same rigorous
guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>typos corrected, references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximum Mean Discrepancy on Exponential Windows for Online Change
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kalinke, Marco Heyden, Edouard Fouché, Klemens Böhm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting changes is of fundamental importance when analyzing data streams
and has many applications, e.g., predictive maintenance, fraud detection, or
medicine. A principled approach to detect changes is to compare the
distributions of observations within the stream to each other via hypothesis
testing. Maximum mean discrepancy (MMD; also called energy distance) is a
well-known (semi-)metric on the space of probability distributions. MMD gives
rise to powerful non-parametric two-sample tests on kernel-enriched domains
under mild conditions, which makes its deployment for change detection
desirable. However, the classic MMD estimators suffer quadratic complexity,
which prohibits their application in the online change detection setting. We
propose a general-purpose change detection algorithm, Maximum Mean Discrepancy
on Exponential Windows (MMDEW), which leverages the MMD two-sample test,
facilitates its efficient online computation on any kernel-enriched domain, and
is able to detect any disparity between distributions. Our experiments and
analysis show that (1) MMDEW achieves better detection quality than
state-of-the-art competitors and that (2) the algorithm has polylogarithmic
runtime and logarithmic memory requirements, which allow its deployment to the
streaming setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Training Machine Learning Models over Multiple Sources with Privacy
  Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.03386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.03386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lushan Song, Guopeng Lin, Jiaxuan Wang, Haoqi Wu, Wenqiang Ruan, Weili Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, gathering high-quality training data from multiple data sources
with privacy preservation is a crucial challenge to training high-performance
machine learning models. The potential solutions could break the barriers among
isolated data corpus, and consequently enlarge the range of data available for
processing. To this end, both academic researchers and industrial vendors are
recently strongly motivated to propose two main-stream folders of solutions
mainly based on software constructions: 1) Secure Multi-party Learning (MPL for
short); and 2) Federated Learning (FL for short). The above two technical
folders have their advantages and limitations when we evaluate them according
to the following five criteria: security, efficiency, data distribution, the
accuracy of trained models, and application scenarios.
  Motivated to demonstrate the research progress and discuss the insights on
the future directions, we thoroughly investigate these protocols and frameworks
of both MPL and FL. At first, we define the problem of Training machine
learning Models over Multiple data sources with Privacy Preservation (TMMPP for
short). Then, we compare the recent studies of TMMPP from the aspects of the
technical routes, the number of parties supported, data partitioning, threat
model, and machine learning models supported, to show their advantages and
limitations. Next, we investigate and evaluate five popular FL platforms.
Finally, we discuss the potential directions to resolve the problem of TMMPP in
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting data-driven robust statistical arbitrage strategies with deep
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03179v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03179v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Neufeld, Julian Sester, Daiying Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach, based on deep neural networks, that allows
identifying robust statistical arbitrage strategies in financial markets.
Robust statistical arbitrage strategies refer to trading strategies that enable
profitable trading under model ambiguity. The presented novel methodology
allows to consider a large amount of underlying securities simultaneously and
does not depend on the identification of cointegrated pairs of assets, hence it
is applicable on high-dimensional financial markets or in markets where
classical pairs trading approaches fail. Moreover, we provide a method to build
an ambiguity set of admissible probability measures that can be derived from
observed market data. Thus, the approach can be considered as being model-free
and entirely data-driven. We showcase the applicability of our method by
providing empirical investigations with highly profitable trading performances
even in 50 dimensions, during financial crises, and when the cointegration
relationship between asset pairs stops to persist.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Max-Margin Works while Large Margin Fails: Generalization without
  Uniform Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margalit Glasgow, Colin Wei, Mary Wootters, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in modern machine learning is theoretically understanding
the generalization properties of overparameterized models. Many existing tools
rely on uniform convergence (UC), a property that, when it holds, guarantees
that the test loss will be close to the training loss, uniformly over a class
of candidate models. Nagarajan and Kolter (2019) show that in certain simple
linear and neural-network settings, any uniform convergence bound will be
vacuous, leaving open the question of how to prove generalization in settings
where UC fails. Our main contribution is proving novel generalization bounds in
two such settings, one linear, and one non-linear. We study the linear
classification setting of Nagarajan and Kolter, and a quadratic ground truth
function learned via a two-layer neural network in the non-linear regime. We
prove a new type of margin bound showing that above a certain signal-to-noise
threshold, any near-max-margin classifier will achieve almost no test loss in
these two settings. Our results show that near-max-margin is important: while
any model that achieves at least a $(1 - \epsilon)$-fraction of the max-margin
generalizes well, a classifier achieving half of the max-margin may fail
terribly. Building on the impossibility results of Nagarajan and Kolter, under
slightly stronger assumptions, we show that one-sided UC bounds and classical
margin bounds will fail on near-max-margin classifiers. Our analysis provides
insight on why memorization can coexist with generalization: we show that in
this challenging regime where generalization occurs but UC fails,
near-max-margin classifiers simultaneously contain some generalizable
components and some overfitting components that memorize the data. The presence
of the overfitting components is enough to preclude UC, but the near-extremal
margin guarantees that sufficient generalizable components are present.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time scheduling of renewable power systems through planning-based
  reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohuai Liu, Jinbo Liu, Weirui Ye, Nan Yang, Guanglun Zhang, Haiwang Zhong, Chongqing Kang, Qirong Jiang, Xuri Song, Fangchun Di, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing renewable energy sources have posed significant challenges to
traditional power scheduling. It is difficult for operators to obtain accurate
day-ahead forecasts of renewable generation, thereby requiring the future
scheduling system to make real-time scheduling decisions aligning with
ultra-short-term forecasts. Restricted by the computation speed, traditional
optimization-based methods can not solve this problem. Recent developments in
reinforcement learning (RL) have demonstrated the potential to solve this
challenge. However, the existing RL methods are inadequate in terms of
constraint complexity, algorithm performance, and environment fidelity. We are
the first to propose a systematic solution based on the state-of-the-art
reinforcement learning algorithm and the real power grid environment. The
proposed approach enables planning and finer time resolution adjustments of
power generators, including unit commitment and economic dispatch, thus
increasing the grid's ability to admit more renewable energy. The well-trained
scheduling agent significantly reduces renewable curtailment and load shedding,
which are issues arising from traditional scheduling's reliance on inaccurate
day-ahead forecasts. High-frequency control decisions exploit the existing
units' flexibility, reducing the power grid's dependence on hardware
transformations and saving investment and operating costs, as demonstrated in
experimental results. This research exhibits the potential of reinforcement
learning in promoting low-carbon and intelligent power systems and represents a
solid step toward sustainable electricity generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separate and conquer heuristic allows robust mining of contrast sets in
  classification, regression, and survival data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Gudyś, Marek Sikora, Łukasz Wróbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying differences between groups is one of the most important knowledge
discovery problems. The procedure, also known as contrast sets mining, is
applied in a wide range of areas like medicine, industry, or economics.
  In the paper we present RuleKit-CS, an algorithm for contrast set mining
based on separate and conquer - a well established heuristic for decision rule
induction. Multiple passes accompanied with an attribute penalization scheme
provide contrast sets describing same examples with different attributes,
distinguishing presented approach from the standard separate and conquer. The
algorithm was also generalized for regression and survival data allowing
identification of contrast sets whose label attribute/survival prognosis is
consistent with the label/prognosis for the predefined contrast groups. This
feature, not provided by the existing approaches, further extends the usability
of RuleKit-CS.
  Experiments on over 130 data sets from various areas and detailed analysis of
selected cases confirmed RuleKit-CS to be a useful tool for discovering
differences between defined groups. The algorithm was implemented as a part of
the RuleKit suite available at GitHub under GNU AGPL 3 licence
(https://github.com/adaa-polsl/RuleKit).
  Keywords: contrast sets, separate and conquer, regression, survival
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 6 figures, 3 tables, 3 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Estimation by Fisher Information-based Evidential Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danruo Deng, Guangyong Chen, Yang Yu, Furui Liu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty estimation is a key factor that makes deep learning reliable in
practical applications. Recently proposed evidential neural networks explicitly
account for different uncertainties by treating the network's outputs as
evidence to parameterize the Dirichlet distribution, and achieve impressive
performance in uncertainty estimation. However, for high data uncertainty
samples but annotated with the one-hot label, the evidence-learning process for
those mislabeled classes is over-penalized and remains hindered. To address
this problem, we propose a novel method, Fisher Information-based Evidential
Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher
Information Matrix (FIM) to measure the informativeness of evidence carried by
each sample, according to which we can dynamically reweight the objective loss
terms to make the network more focused on the representation learning of
uncertain classes. The generalization ability of our network is further
improved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our
proposed method consistently outperforms traditional EDL-related algorithms in
multiple uncertainty estimation tasks, especially in the more challenging
few-shot classification settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Client Selection for Federated Bayesian Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarong Yang, Yuan Liu, Rahif Kassab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric
distributed learning framework for federated Bayesian learning, where multiple
clients jointly train a machine learning model by communicating a number of
non-random and interacting particles with the server. Since communication
resources are limited, selecting the clients with most informative local
learning updates can improve the model convergence and communication
efficiency. In this paper, we propose two selection schemes for DSVGD based on
Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive
the upper bound on the decrease of the global free energy per iteration for
both schemes, which is then minimized to speed up the model convergence. We
evaluate and compare our schemes with conventional schemes in terms of model
accuracy, convergence speed, and stability using various learning tasks and
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Journal on Selected Areas in Communications Special
  Issue on Communication-Efficient Distributed Learning over Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Kant Gupta, Shivani Nandgaonkar, Nikhil Cherian Kurian, Swapnil Rane, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard diagnostic procedures for targeted therapies in lung cancer
treatment involve histological subtyping and subsequent detection of key driver
mutations, such as EGFR. Even though molecular profiling can uncover the driver
mutation, the process is often expensive and time-consuming. Deep
learning-oriented image analysis offers a more economical alternative for
discovering driver mutations directly from whole slide images (WSIs). In this
work, we used customized deep learning pipelines with weak supervision to
identify the morphological correlates of EGFR mutation from hematoxylin and
eosin-stained WSIs, in addition to detecting tumor and histologically subtyping
it. We demonstrate the effectiveness of our pipeline by conducting rigorous
experiments and ablation studies on two lung cancer datasets - TCGA and a
private dataset from India. With our pipeline, we achieved an average area
under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological
subtyping between adenocarcinoma and squamous cell carcinoma on the TCGA
dataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA
dataset and 0.783 on the dataset from India. Our key learning points include
the following. Firstly, there is no particular advantage of using a feature
extractor layers trained on histology, if one is going to fine-tune the feature
extractor on the target dataset. Secondly, selecting patches with high
cellularity, presumably capturing tumor regions, is not always helpful, as the
sign of a disease class may be present in the tumor-adjacent stroma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We need to improve</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is the Performance of My Deep Network Too Good to Be True? A Direct
  Approach to Estimating the Bayes Error in Binary Classification <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Ishida, Ikko Yamane, Nontawat Charoenphakdee, Gang Niu, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a fundamental limitation in the prediction performance that a
machine learning model can achieve due to the inevitable uncertainty of the
prediction target. In classification problems, this can be characterized by the
Bayes error, which is the best achievable error with any classifier. The Bayes
error can be used as a criterion to evaluate classifiers with state-of-the-art
performance and can be used to detect test set overfitting. We propose a simple
and direct Bayes error estimator, where we just take the mean of the labels
that show \emph{uncertainty} of the class assignments. Our flexible approach
enables us to perform Bayes error estimation even for weakly supervised data.
In contrast to others, our method is model-free and even instance-free.
Moreover, it has no hyperparameters and gives a more accurate estimate of the
Bayes error than several baselines empirically. Experiments using our method
suggest that recently proposed deep networks such as the Vision Transformer may
have reached, or is about to reach, the Bayes error for benchmark datasets.
Finally, we discuss how we can study the inherent difficulty of the
acceptance/rejection decision for scientific articles, by estimating the Bayes
error of the ICLR papers from 2017 to 2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 (notable-top-5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Parsing and Visual Grounding of Natural Language
  Instructions for Object Placement <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00215v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00215v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Zhao, Wee Sun Lee, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method, PARsing And visual GrOuNding (ParaGon), for
grounding natural language in object placement tasks. Natural language
generally describes objects and spatial relations with compositionality and
ambiguity, two major obstacles to effective language grounding. For
compositionality, ParaGon parses a language instruction into an object-centric
graph representation to ground objects individually. For ambiguity, ParaGon
uses a novel particle-based graph neural network to reason about object
placements with uncertainty. Essentially, ParaGon integrates a parsing
algorithm into a probabilistic, data-driven learning framework. It is fully
differentiable and trained end-to-end from data for robustness against complex,
ambiguous language input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProxyBO: Accelerating Neural Architecture Search via Bayesian
  Optimization with Zero-cost Proxies <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10423v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10423v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shen, Yang Li, Jian Zheng, Wentao Zhang, Peng Yao, Jixiang Li, Sen Yang, Ji Liu, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing neural architectures requires immense manual efforts. This has
promoted the development of neural architecture search (NAS) to automate the
design. While previous NAS methods achieve promising results but run slowly,
zero-cost proxies run extremely fast but are less promising. Therefore, it is
of great potential to accelerate NAS via those zero-cost proxies. The existing
method has two limitations, which are unforeseeable reliability and one-shot
usage. To address the limitations, we present ProxyBO, an efficient Bayesian
optimization (BO) framework that utilizes the zero-cost proxies to accelerate
neural architecture search. We apply the generalization ability measurement to
estimate the fitness of proxies on the task during each iteration and design a
novel acquisition function to combine BO with zero-cost proxies based on their
dynamic influence. Extensive empirical studies show that ProxyBO consistently
outperforms competitive baselines on five tasks from three public benchmarks.
Concretely, ProxyBO achieves up to 5.41x and 3.86x speedups over the
state-of-the-art approaches REA and BRP-NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Location Leakage in Federated Signal Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evita Bakopoulou, Jiang Zhang, Mengwei Yang, Konstantinos Psounis, Athina Markopoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of predicting cellular network performance (signal
maps) from measurements collected by several mobile devices. We formulate the
problem within the online federated learning framework: (i) federated learning
(FL) enables users to collaboratively train a model, while keeping their
training data on their devices; (ii) measurements are collected as users move
around over time and are used for local training in an online fashion. We
consider an honest-but-curious server, who observes the updates from target
users participating in FL and infers their location using a deep leakage from
gradients (DLG) type of attack, originally developed to reconstruct training
data of DNN image classifiers. We make the key observation that a DLG attack,
applied to our setting, infers the average location of a batch of local data,
and can thus be used to reconstruct the target users' trajectory at a coarse
granularity. We build on this observation to protect location privacy, in our
setting, by revisiting and designing mechanisms within the federated learning
framework including: tuning the FL parameters for averaging, curating local
batches so as to mislead the DLG attacker, and aggregating across multiple
users with different trajectories. We evaluate the performance of our
algorithms through both analysis and simulation based on real-world mobile
datasets, and we show that they achieve a good privacy-utility tradeoff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Stationary Bandit Learning via Predictive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01970v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01970v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyang Liu, Benjamin Van Roy, Kuang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thompson sampling has proven effective across a wide range of stationary
bandit environments. However, as we demonstrate in this paper, it can perform
poorly when applied to non-stationary environments. We show that such failures
are attributed to the fact that, when exploring, the algorithm does not
differentiate actions based on how quickly the information acquired loses its
usefulness due to non-stationarity. Building upon this insight, we propose
predictive sampling, an algorithm that deprioritizes acquiring information that
quickly loses usefulness. Theoretical guarantee on the performance of
predictive sampling is established through a Bayesian regret bound. We provide
versions of predictive sampling for which computations tractably scale to
complex bandit environments of practical interest. Through numerical
simulations, we demonstrate that predictive sampling outperforms Thompson
sampling in all non-stationary environments examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRMDN: Flow-based Recurrent Mixture Density Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.02144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.02144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedeh Fatemeh Razavi, Reshad Hosseini, Tina Behzad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The class of recurrent mixture density networks is an important class of
probabilistic models used extensively in sequence modeling and
sequence-to-sequence mapping applications. In this class of models, the density
of a target sequence in each time-step is modeled by a Gaussian mixture model
with the parameters given by a recurrent neural network. In this paper, we
generalize recurrent mixture density networks by defining a Gaussian mixture
model on a non-linearly transformed target sequence in each time-step. The
non-linearly transformed space is created by normalizing flow. We observed that
this model significantly improves the fit to image sequences measured by the
log-likelihood. We also applied the proposed model on some speech and image
data, and observed that the model has significant modeling power outperforming
other state-of-the-art methods in terms of the log-likelihood.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimation of Correlation Matrices from Limited time series Data using
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01198v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01198v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Easaw, Woo Seok Lee, Prashant Singh Lohiya, Sarika Jalan, Priodyuti Pradhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correlation matrices contain a wide variety of spatio-temporal information
about a dynamical system. Predicting correlation matrices from partial time
series information of a few nodes characterizes the spatio-temporal dynamics of
the entire underlying system. This information can help to predict the
underlying network structure, e.g., inferring neuronal connections from spiking
data, deducing causal dependencies between genes from expression data, and
discovering long spatial range influences in climate variations. Traditional
methods of predicting correlation matrices utilize time series data of all the
nodes of the underlying networks. Here, we use a supervised machine learning
technique to predict the correlation matrix of entire systems from finite time
series information of a few randomly selected nodes. The accuracy of the
prediction validates that only a limited time series of a subset of the entire
system is enough to make good correlation matrix predictions. Furthermore,
using an unsupervised learning algorithm, we furnish insights into the success
of the predictions from our model. Finally, we employ the machine learning
model developed here to real-world data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01217v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01217v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yugang Jiang, Yaowei Wang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in developing unlearnable examples (UEs) against
visual privacy leaks on the Internet. UEs are training samples added with
invisible but unlearnable noise, which have been found can prevent unauthorized
training of machine learning models. UEs typically are generated via a bilevel
optimization framework with a surrogate model to remove (minimize) errors from
the original samples, and then applied to protect the data against unknown
target models. However, existing UE generation methods all rely on an ideal
assumption called label-consistency, where the hackers and protectors are
assumed to hold the same label for a given sample. In this work, we propose and
promote a more practical label-agnostic setting, where the hackers may exploit
the protected data quite differently from the protectors. E.g., a m-class
unlearnable dataset held by the protector may be exploited by the hacker as a
n-class dataset. Existing UE generation methods are rendered ineffective in
this challenging setting. To tackle this challenge, we present a novel
technique called Unlearnable Clusters (UCs) to generate label-agnostic
unlearnable examples with cluster-wise perturbations. Furthermore, we propose
to leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the
surrogate model to improve the transferability of the crafted UCs to diverse
domains. We empirically verify the effectiveness of our proposed approach under
a variety of settings with different datasets, target models, and even
commercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available
at \url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Landslide Susceptibility Modeling by Interpretable Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06837v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06837v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Youssef, Kevin Shao, Seulgi Moon, Louis-Serge Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Landslides are notoriously difficult to predict because numerous spatially
and temporally varying factors contribute to slope stability. Artificial neural
networks (ANN) have been shown to improve prediction accuracy but are largely
uninterpretable. Here we introduce an additive ANN optimization framework to
assess landslide susceptibility, as well as dataset division and outcome
interpretation techniques. We refer to our approach, which features full
interpretability, high accuracy, high generalizability and low model
complexity, as superposable neural network (SNN) optimization. We validate our
approach by training models on landslide inventory from three different
easternmost Himalaya regions. Our SNN outperformed physically-based and
statistical models and achieved similar performance to state-of-the-art deep
neural networks. The SNN models found the product of slope and precipitation
and hillslope aspect to be important primary contributors to high landslide
susceptibility, which highlights the importance of strong slope-climate
couplings, along with microclimates, on landslide occurrences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>79 pages (including SI section); 8 main figures; 12 supplementary
  figures; 9 supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DM$^2$: Decentralized Multi-Agent Reinforcement Learning for
  Distribution Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Wang, Ishan Durugkar, Elad Liebman, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to multi-agent cooperation rely heavily on centralized
mechanisms or explicit communication protocols to ensure convergence. This
paper studies the problem of distributed multi-agent learning without resorting
to centralized components or explicit communication. It examines the use of
distribution matching to facilitate the coordination of independent agents. In
the proposed scheme, each agent independently minimizes the distribution
mismatch to the corresponding component of a target visitation distribution.
The theoretical analysis shows that under certain conditions, each agent
minimizing its individual distribution mismatch allows the convergence to the
joint policy that generated the target distribution. Further, if the target
distribution is from a joint policy that optimizes a cooperative task, the
optimal policy for a combination of this task reward and the distribution
matching reward is the same joint policy. This insight is used to formulate a
practical algorithm (DM$^2$), in which each individual agent matches a target
distribution derived from concurrently sampled trajectories from a joint expert
policy. Experimental validation on the StarCraft domain shows that combining
(1) a task reward, and (2) a distribution matching reward for expert
demonstrations for the same task, allows agents to outperform a naive
distributed baseline. Additional experiments probe the conditions under which
expert demonstrations need to be sampled to obtain the learning benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recipro-CAM: Fast gradient-free visual explanations for convolutional
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok-Yong Byun, Wonju Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Convolutional Neural Network (CNN) is a widely used deep learning
architecture for computer vision. However, its black box nature makes it
difficult to interpret the behavior of the model. To mitigate this issue, AI
practitioners have explored explainable AI methods like Class Activation Map
(CAM) and Grad-CAM. Although these methods have shown promise, they are limited
by architectural constraints or the burden of gradient computing. To overcome
this issue, Score-CAM and Ablation-CAM have been proposed as gradient-free
methods, but they have longer execution times compared to CAM or Grad-CAM based
methods, making them unsuitable for real-world solution though they resolved
gradient related issues and enabled inference mode XAI. To address this
challenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.
Our approach involves spatially masking the extracted feature maps to exploit
the correlation between activation maps and network predictions for target
classes. Our proposed method has yielded promising results, outperforming
current state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)
metric by $1.78 \%$ to $3.72 \%$, excluding VGG-16 backbone. Moreover,
Recipro-CAM generates saliency maps at a similar rate to Grad-CAM and is
approximately $148$ times faster than Score-CAM. The source code for
Recipro-CAM is available in our data analysis framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14013v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14013v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwook Kim, Juseong Kim, Giltae Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in semi- and self-supervised learning has caused a rift in
the long-held belief about the need for an enormous amount of labeled data for
machine learning and the irrelevancy of unlabeled data. Although it has been
successful in various data, there is no dominant semi- and self-supervised
learning method that can be generalized for tabular data (i.e. most of the
existing methods require appropriate tabular datasets and architectures). In
this paper, we revisit self-training which can be applied to any kind of
algorithm including the most widely used architecture, gradient boosting
decision tree, and introduce curriculum pseudo-labeling (a state-of-the-art
pseudo-labeling technique in image) for a tabular domain. Furthermore, existing
pseudo-labeling techniques do not assure the cluster assumption when computing
confidence scores of pseudo-labels generated from unlabeled data. To overcome
this issue, we propose a novel pseudo-labeling approach that regularizes the
confidence scores based on the likelihoods of the pseudo-labels so that more
reliable pseudo-labels which lie in high density regions can be obtained. We
exhaustively validate the superiority of our approaches using various models
and tabular datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages for the main part and 8 extra pages for the appendix. 2
  figures and 3 tables for the main part</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Stability Analysis of Open Federated Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youbang Sun, Heshan Fernando, Tianyi Chen, Shahin Shahrampour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the open federated learning (FL) systems, where clients may join
and/or leave the system during the FL process. Given the variability of the
number of present clients, convergence to a fixed model cannot be guaranteed in
open systems. Instead, we resort to a new performance metric that we term the
stability of open FL systems, which quantifies the magnitude of the learned
model in open systems. Under the assumption that local clients' functions are
strongly convex and smooth, we theoretically quantify the radius of stability
for two FL algorithms, namely local SGD and local Adam. We observe that this
radius relies on several key parameters, including the function condition
number as well as the variance of the stochastic gradient. Our theoretical
results are further verified by numerical simulations on both synthetic and
real-world benchmark data-sets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriDet: Temporal Action Detection with Relative Boundary Modeling <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a one-stage framework TriDet for temporal action
detection. Existing methods often suffer from imprecise boundary predictions
due to the ambiguous action boundaries in videos. To alleviate this problem, we
propose a novel Trident-head to model the action boundary via an estimated
relative probability distribution around the boundary. In the feature pyramid
of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer
to mitigate the rank loss problem of self-attention that takes place in the
video features and aggregate information across different temporal
granularities. Benefiting from the Trident-head and the SGP-based feature
pyramid, TriDet achieves state-of-the-art performance on three challenging
benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational
costs, compared to previous methods. For example, TriDet hits an average mAP of
$69.3\%$ on THUMOS14, outperforming the previous best by $2.5\%$, but with only
$74.6\%$ of its latency. The code is released to
https://github.com/sssste/TriDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023; Temporal Action Detection; Temporal Action Localization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Distortion Invariant Representation for Image Restoration from
  A Causality Perspective <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Bingchen Li, Xin Jin, Cuiling Lan, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have witnessed the great advancement of Deep neural
networks (DNNs) in image restoration. However, a critical limitation is that
they cannot generalize well to real-world degradations with different degrees
or types. In this paper, we are the first to propose a novel training strategy
for image restoration from the causality perspective, to improve the
generalization ability of DNNs for unknown degradations. Our method, termed
Distortion Invariant representation Learning (DIL), treats each distortion type
and degree as one specific confounder, and learns the distortion-invariant
representation by eliminating the harmful confounding effect of each
degradation. We derive our DIL with the back-door criterion in causality by
modeling the interventions of different distortions from the optimization
perspective. Particularly, we introduce counterfactual distortion augmentation
to simulate the virtual distortion types and degrees as the confounders. Then,
we instantiate the intervention of each distortion with a virtual model
updating based on corresponding distorted images, and eliminate them from the
meta-learning perspective. Extensive experiments demonstrate the effectiveness
of our DIL on the generalization capability for unseen distortion types and
degrees. Our code will be available at
https://github.com/lixinustc/Casual-IRDIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Making a Trojan-horse Attack on Text-to-Image Retrieval <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03861v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03861v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Aozhu Chen, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning based image retrieval is reported to be vulnerable to
adversarial attacks, existing works are mainly on image-to-image retrieval with
their attacks performed at the front end via query modification. By contrast,
we present in this paper the first study about a threat that occurs at the back
end of a text-to-image retrieval (T2IR) system. Our study is motivated by the
fact that the image collection indexed by the system will be regularly updated
due to the arrival of new images from various sources such as web crawlers and
advertisers. With malicious images indexed, it is possible for an attacker to
indirectly interfere with the retrieval process, letting users see certain
images that are completely irrelevant w.r.t. their queries. We put this thought
into practice by proposing a novel Trojan-horse attack (THA). In particular, we
construct a set of Trojan-horse images by first embedding word-specific
adversarial information into a QR code and then putting the code on benign
advertising images. A proof-of-concept evaluation, conducted on two popular
T2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed
THA in a white-box mode.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETMA: Efficient <span class="highlight-title">Transformer</span> Based Multilevel Attention framework for
  Multimodal Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashima Yadav, Shivani Gaba, Haneef Khan, Ishan Budhiraja, Akansha Singh, Krishan Kant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this new digital era, social media has created a severe impact on the
lives of people. In recent times, fake news content on social media has become
one of the major challenging problems for society. The dissemination of
fabricated and false news articles includes multimodal data in the form of text
and images. The previous methods have mainly focused on unimodal analysis.
Moreover, for multimodal analysis, researchers fail to keep the unique
characteristics corresponding to each modality. This paper aims to overcome
these limitations by proposing an Efficient Transformer based Multilevel
Attention (ETMA) framework for multimodal fake news detection, which comprises
the following components: visual attention-based encoder, textual
attention-based encoder, and joint attention-based learning. Each component
utilizes the different forms of attention mechanism and uniquely deals with
multimodal data to detect fraudulent content. The efficacy of the proposed
network is validated by conducting several experiments on four real-world fake
news datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,
and Risdal Fake News Dataset using multiple evaluation metrics. The results
show that the proposed method outperforms the baseline methods on all four
datasets. Further, the computation time of the model is also lower than the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Computational Social
  Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-12T00:00:00Z">2023-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUKE-Graph: A <span class="highlight-title">Transformer</span>-based Approach with Gated Relational Graph
  Attention for Cloze-style Reading Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Foolad, Kourosh Kiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating prior knowledge can improve existing pre-training models in
cloze-style machine reading and has become a new trend in recent studies.
Notably, most of the existing models have integrated external knowledge graphs
(KG) and transformer-based models, such as BERT into a unified data structure.
However, selecting the most relevant ambiguous entities in KG and extracting
the best subgraph remains a challenge. In this paper, we propose the
LUKE-Graph, a model that builds a heterogeneous graph based on the intuitive
relationships between entities in a document without using any external KG. We
then use a Relational Graph Attention (RGAT) network to fuse the graph's
reasoning information and the contextual representation encoded by the
pre-trained LUKE model. In this way, we can take advantage of LUKE, to derive
an entity-aware representation; and a graph model - to exploit relation-aware
representation. Moreover, we propose Gated-RGAT by augmenting RGAT with a
gating mechanism that regulates the question information for the graph
convolution operation. This is very similar to human reasoning processing
because they always choose the best entity candidate based on the question
information. Experimental results demonstrate that the LUKE-Graph achieves
state-of-the-art performance on the ReCoRD dataset with commonsense reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for neurocomputing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive
  Machine Translation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengrui Ma, Chenze Shao, Shangtong Gui, Min Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive translation (NAT) reduces the decoding latency but suffers
from performance degradation due to the multi-modality problem. Recently, the
structure of directed acyclic graph has achieved great success in NAT, which
tackles the multi-modality problem by introducing dependency between vertices.
However, training it with negative log-likelihood loss implicitly requires a
strict alignment between reference tokens and vertices, weakening its ability
to handle multiple translation modalities. In this paper, we hold the view that
all paths in the graph are fuzzily aligned with the reference sentence. We do
not require the exact alignment but train the model to maximize a fuzzy
alignment score between the graph and reference, which takes captured
translations in all modalities into account. Extensive experiments on major WMT
benchmarks show that our method substantially improves translation performance
and increases prediction confidence, setting a new state of the art for NAT on
the raw training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MWE as WSD: Solving Multiword Expression Identification with Word Sense
  Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Tanner, Jacob Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in word sense disambiguation (WSD) utilizes encodings of the
sense gloss (definition text), in addition to the input words and context, to
improve performance. In this work we demonstrate that this approach can be
adapted for use in multiword expression (MWE) identification by training a
Bi-encoder model which uses gloss and context information to filter MWE
candidates produced from a simple rule-based extraction pipeline. We achieve
state-of-the-art results in MWE identification on the DiMSUM dataset, and
competitive results on the PARSEME 1.1 English dataset using this method. Our
model also retains most of its ability to perform WSD, demonstrating that a
single model can successfully be applied to both of these tasks. Additionally,
we experiment with applying Poly-encoder models to MWE identification and WSD,
introducing a modified Poly-encoder architecture which outperforms the standard
Poly-encoder on these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Intent Classification accuracy in Noisy Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Nabih Ali, Alessio Brutti, Daniele Falavigna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent classification is a fundamental task in the spoken language
understanding field that has recently gained the attention of the scientific
community, mainly because of the feasibility of approaching it with end-to-end
neural models. In this way, avoiding using intermediate steps, i.e. automatic
speech recognition, is possible, thus the propagation of errors due to
background noise, spontaneous speech, speaking styles of users, etc. Towards
the development of solutions applicable in real scenarios, it is interesting to
investigate how environmental noise and related noise reduction techniques to
address the intent classification task with end-to-end neural models. In this
paper, we experiment with a noisy version of the fluent speech command data
set, combining the intent classifier with a time-domain speech enhancement
solution based on Wave-U-Net and considering different training strategies.
Experimental results reveal that, for this task, the use of speech enhancement
greatly improves the classification accuracy in noisy conditions, in particular
when the classification model is trained on enhanced signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards General Purpose Medical AI: Continual Learning Medical
  Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, Kang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inevitable domain and task discrepancies in real-world scenarios can impair
the generalization performance of the pre-trained deep models for medical data.
Therefore, we audaciously propose that we should build a general-purpose
medical AI system that can be seamlessly adapted to downstream domains/tasks.
Since the domain/task adaption procedures usually involve additional labeling
work for the target data, designing a data-efficient adaption algorithm is
desired to save the cost of transferring the learned knowledge. Our recent work
found that vision-language models (VLMs) are efficient learners with
extraordinary cross-domain ability. Therefore, in this work, we further explore
the possibility of leveraging pre-trained VLMs as medical foundation models for
building general-purpose medical AI, where we thoroughly investigate three
machine-learning paradigms, i.e., domain/task-specialized learning, joint
learning, and continual learning, for training the VLMs and evaluate their
generalization performance on cross-domain and cross-task test sets. To
alleviate the catastrophic forgetting during sequential training, we employ
rehearsal learning and receive a sharp boost in terms of generalization
capability. In a nutshell, our empirical evidence suggests that continual
learning may be a practical and efficient learning paradigm for the medical
foundation model. And we hope researchers can use our empirical evidence as
basement to further explore the path toward medical foundation model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models for Non-autoregressive Text <span class="highlight-title">Generation</span>: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing improved generation quality. In this survey, we
review the recent progress in diffusion models for NAR text generation. As the
background, we first present the general definition of diffusion models and the
text diffusion models, and then discuss their merits for NAR generation. As the
core content, we further introduce two mainstream diffusion models in existing
text diffusion works, and review the key designs of the diffusion process.
Moreover, we discuss the utilization of pre-trained language models (PLMs) for
text diffusion models and introduce optimization techniques for text data.
Finally, we discuss several promising directions and conclude this paper. Our
survey aims to provide researchers with a systematic reference of related
research on text diffusion models for NAR generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Heterogeneous Graph for Abstractive Multi-Document
  Summarization <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jianzhong Qi, Jey Han Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Articulation GAN: Unsupervised modeling of articulatory learning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gašper Beguš, Alan Zhou, Peter Wu, Gopala K Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative deep neural networks are widely used for speech synthesis, but
most existing models directly generate waveforms or spectral outputs. Humans,
however, produce speech by controlling articulators, which results in the
production of speech sounds through physical properties of sound propagation.
We introduce the Articulatory Generator to the Generative Adversarial Network
paradigm, a new unsupervised generative model of speech production/synthesis.
The Articulatory Generator more closely mimics human speech production by
learning to generate articulatory representations (electromagnetic
articulography or EMA) in a fully unsupervised manner. A separate pre-trained
physical model (ema2wav) then transforms the generated EMA representations to
speech waveforms, which get sent to the Discriminator for evaluation.
Articulatory analysis suggests that the network learns to control articulators
in a similar manner to humans during speech production. Acoustic analysis of
the outputs suggests that the network learns to generate words that are both
present and absent in the training distribution. We additionally discuss
implications of articulatory representations for cognitive models of human
language and speech technology in general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Language Models in Biomedical Domain: A Systematic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) have been the de facto paradigm for most
natural language processing (NLP) tasks. This also benefits biomedical domain:
researchers from informatics, medicine, and computer science (CS) communities
propose various PLMs trained on biomedical datasets, e.g., biomedical text,
electronic health records, protein, and DNA sequences for various biomedical
tasks. However, the cross-discipline characteristics of biomedical PLMs hinder
their spreading among communities; some existing works are isolated from each
other without comprehensive comparison and discussions. It expects a survey
that not only systematically reviews recent advances of biomedical PLMs and
their applications but also standardizes terminology and benchmarks. In this
paper, we summarize the recent progress of pre-trained language models in the
biomedical domain and their applications in biomedical downstream tasks.
Particularly, we discuss the motivations and propose a taxonomy of existing
biomedical PLMs. Their applications in biomedical downstream tasks are
exhaustively discussed. At last, we illustrate various limitations and future
trends, which we hope can provide inspiration for the future research of the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An improved version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Correct answers" from the psychology of artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter S. Park, Philipp Schoenegger, Chongyang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have vastly grown in capabilities. One proposed
application of such AI systems is to support data collection in the social and
cognitive sciences, where perfect experimental control is currently unfeasible
and the collection of large, representative datasets is generally expensive. In
this paper, we re-replicate 14 studies from the Many Labs 2 replication project
with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We
collected responses from the default setting of GPT3.5 by inputting each
study's survey as text. Among the eight studies we could analyse, our GPT
sample replicated 37.5% of the original results as well as 37.5% of the Many
Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as
we had planned in our pre-registration. This was because for each of these six
studies, GPT3.5 answered at least one of the survey questions (either a
dependent variable or a condition variable) in an extremely predetermined way:
an unexpected phenomenon we call the "correct answer" effect. Different runs of
GPT3.5 answered nuanced questions probing political orientation, economic
preference, judgement, and moral philosophy with zero or near-zero variation in
responses: with the supposedly "correct answer." For example, our survey
questions found the default setting of GPT3.5 to almost always self-identify as
a maximally strong conservative (99.6%, N=1,030), and to always be morally
deontological in opposing the hypothetical pushing of a large man in front of
an incoming trolley to save the lives of five people (100%, N=1,030). Since AI
models of the future may be trained on much of the same data as GPT3.5,
training data from which GPT3.5 may have learned its supposedly "correct
answers," our results raise concerns that a hypothetical AI-led future may in
certain ways be subject to a diminished diversity of thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages (31-page main text, 21-page SI); nine visualizations (three
  tables and two figures in the main text, four figures in the SI); added
  corrections regarding the previously erroneous survey for Study 4's
  replication of Graham et al. (2009); preregistered OSF database is available
  at https://osf.io/dzp8t/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with
  Variational Information Bottleneck and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingshan Chang, Min Yang, Qingshan Jiang, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have dominated the literature on aspect-based
sentiment analysis (ABSA), yielding state-of-the-art results. However, these
deep models generally suffer from spurious correlation problems between input
features and output labels, which creates significant barriers to robustness
and generalization capability. In this paper, we propose a novel Contrastive
Variational Information Bottleneck framework (called CVIB) to reduce spurious
correlations for ABSA. The proposed CVIB framework is composed of an original
network and a self-pruned network, and these two networks are optimized
simultaneously via contrastive learning. Concretely, we employ the Variational
Information Bottleneck (VIB) principle to learn an informative and compressed
network (self-pruned network) from the original network, which discards the
superfluous patterns or spurious correlations between input features and
prediction labels. Then, self-pruning contrastive learning is devised to pull
together semantically similar positive pairs and push away dissimilar pairs,
where the representations of the anchor learned by the original and self-pruned
networks respectively are regarded as a positive pair while the representations
of two different sentences within a mini-batch are treated as a negative pair.
To verify the effectiveness of our CVIB method, we conduct extensive
experiments on five benchmark ABSA datasets and the experimental results show
that our approach achieves better performance than the strong competitors in
terms of overall prediction performance, robustness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning ASR pathways: A sparse multilingual ASR model <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Yang, Andros Tjandra, Chunxi Liu, David Zhang, Duc Le, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning compresses automatic speech recognition (ASR) models
effectively. However, in multilingual ASR, language-agnostic pruning may lead
to severe performance drops on some languages because language-agnostic pruning
masks may not fit all languages and discard important language-specific
parameters. In this work, we present ASR pathways, a sparse multilingual ASR
model that activates language-specific sub-networks ("pathways"), such that the
parameters for each language are learned explicitly. With the overlapping
sub-networks, the shared parameters can also enable knowledge transfer for
lower-resource languages via joint multilingual training. We propose a novel
algorithm to learn ASR pathways, and evaluate the proposed method on 4
languages with a streaming RNN-T model. Our proposed ASR pathways outperform
both dense models and a language-agnostically pruned model, and provide better
performance on low-resource languages compared to the monolingual sparse
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Emotion Embeddings to Transfer <span class="highlight-title">Knowledge</span> Between Emotions,
  Languages, and Annotation Formats <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Gireesh Mahajan, Sabyasachee Baruah, Keith Burghardt, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for emotional inference from text continues to diversify as more and
more disciplines integrate emotions into their theories and applications. These
needs include inferring different emotion types, handling multiple languages,
and different annotation formats. A shared model between different
configurations would enable the sharing of knowledge and a decrease in training
costs, and would simplify the process of deploying emotion recognition models
in novel environments. In this work, we study how we can build a single model
that can transition between these different configurations by leveraging
multilingual models and Demux, a transformer-based model whose input includes
the emotions of interest, enabling us to dynamically change the emotions
predicted by the model. Demux also produces emotion embeddings, and performing
operations on them allows us to transition to clusters of emotions by pooling
the embeddings of each cluster. We show that Demux can simultaneously transfer
knowledge in a zero-shot manner to a new language, to a novel annotation format
and to unseen emotions. Code is available at
https://github.com/gchochla/Demux-MEmo .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP'23, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Label Correlations in a Multi-label Setting: A Case Study in
  Emotion <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Gireesh Mahajan, Sabyasachee Baruah, Keith Burghardt, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting emotions expressed in text has become critical to a range of
fields. In this work, we investigate ways to exploit label correlations in
multi-label emotion recognition models to improve emotion detection. First, we
develop two modeling approaches to the problem in order to capture word
associations of the emotion words themselves, by either including the emotions
in the input, or by leveraging Masked Language Modeling (MLM). Second, we
integrate pairwise constraints of emotion representations as regularization
terms alongside the classification loss of the models. We split these terms
into two categories, local and global. The former dynamically change based on
the gold labels, while the latter remain static during training. We demonstrate
state-of-the-art performance across Spanish, English, and Arabic in SemEval
2018 Task 1 E-c using monolingual BERT-based models. On top of better
performance, we also demonstrate improved robustness. Code is available at
https://github.com/gchochla/Demux-MEmo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP'23, 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-MMF: Provider Max-min Fairness Re-ranking in Recommender System <span class="chip">WWW23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, Zhenghua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the issue of recommending fairly from the aspect of
providers, which has become increasingly essential in multistakeholder
recommender systems. Existing studies on provider fairness usually focused on
designing proportion fairness (PF) metrics that first consider systematic
fairness. However, sociological researches show that to make the market more
stable, max-min fairness (MMF) is a better metric. The main reason is that MMF
aims to improve the utility of the worst ones preferentially, guiding the
system to support the providers in weak market positions. When applying MMF to
recommender systems, how to balance user preferences and provider fairness in
an online recommendation scenario is still a challenging problem. In this
paper, we proposed an online re-ranking model named Provider Max-min Fairness
Re-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates
provider fair recommendation as a resource allocation problem, where the
exposure slots are considered the resources to be allocated to providers and
the max-min fairness is used as the regularizer during the process. We show
that the problem can be further represented as a regularized online optimizing
problem and solved efficiently in its dual space. During the online re-ranking
phase, a momentum gradient descent method is designed to conduct the dynamic
re-ranking. Theoretical analysis showed that the regret of P-MMF can be
bounded. Experimental results on four public recommender datasets demonstrated
that P-MMF can outperformed the state-of-the-art baselines. Experimental
results also show that P-MMF can retain small computationally costs on a corpus
with the large number of items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WWW23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoDenoise: Automatic Data Instance Denoising for Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, Wanyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historical user-item interaction datasets are essential in training modern
recommender systems for predicting user preferences. However, the arbitrary
user behaviors in most recommendation scenarios lead to a large volume of noisy
data instances being recorded, which cannot fully represent their true
interests. While a large number of denoising studies are emerging in the
recommender system community, all of them suffer from highly dynamic data
distributions. In this paper, we propose a Deep Reinforcement Learning (DRL)
based framework, AutoDenoise, with an Instance Denoising Policy Network, for
denoising data instances with an instance selection manner in deep recommender
systems. To be specific, AutoDenoise serves as an agent in DRL to adaptively
select noise-free and predictive data instances, which can then be utilized
directly in training representative recommendation models. In addition, we
design an alternate two-phase optimization strategy to train and validate the
AutoDenoise properly. In the searching phase, we aim to train the policy
network with the capacity of instance denoising; in the validation phase, we
find out and evaluate the denoised subset of data instances selected by the
trained policy network, so as to validate its denoising ability. We conduct
extensive experiments to validate the effectiveness of AutoDenoise combined
with multiple representative recommender system models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 5 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileRec: A Large-Scale <span class="highlight-title">Dataset</span> for Mobile Apps Recommendation <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. H. Maqbool, Umar Farooq, Adib Mosharrof, A. B. Siddique, Hassan Foroosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in our digital lives, from
recommending products on e-commerce websites to suggesting movies and music on
streaming platforms. Existing recommendation datasets, such as Amazon Product
Reviews and MovieLens, greatly facilitated the research and development of
recommender systems in their respective domains. While the number of mobile
users and applications (aka apps) has increased exponentially over the past
decade, research in mobile app recommender systems has been significantly
constrained, primarily due to the lack of high-quality benchmark datasets, as
opposed to recommendations for products, movies, and news. To facilitate
research for app recommendation systems, we introduce a large-scale dataset,
called MobileRec. We constructed MobileRec from users' activity on the Google
play store. MobileRec contains 19.3 million user interactions (i.e., user
reviews on apps) with over 10K unique apps across 48 categories. MobileRec
records the sequential activity of a total of 0.7 million distinct users. Each
of these users has interacted with no fewer than five distinct apps, which
stands in contrast to previous datasets on mobile apps that recorded only a
single interaction per user. Furthermore, MobileRec presents users' ratings as
well as sentiments on installed apps, and each app contains rich metadata such
as app name, category, description, and overall rating, among others. We
demonstrate that MobileRec can serve as an excellent testbed for app
recommendation through a comparative study of several state-of-the-art
recommendation approaches. The quantitative results can act as a baseline for
other researchers to compare their results against. The MobileRec dataset is
available at https://huggingface.co/datasets/recmeapp/mobilerec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 tables, 4 figures, Under submission at SIGIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Prioritization of App Issues via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moghis Fereidouni, Adib Mosharrof, Umar Farooq, AB Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile app stores produce a tremendous amount of data in the form of user
reviews, which is a huge source of user requirements and sentiments; such
reviews allow app developers to proactively address issues in their apps.
However, only a small number of reviews capture common issues and sentiments
which creates a need for automatically identifying prominent reviews.
Unfortunately, most existing work in text ranking and popularity prediction
focuses on social contexts where other signals are available, which renders
such works ineffective in the context of app reviews. In this work, we propose
a new framework, PPrior, that enables proactive prioritization of app issues
through identifying prominent reviews (ones predicted to receive a large number
of votes in a given time window). Predicting highly-voted reviews is
challenging given that, unlike social posts, social network features of users
are not available. Moreover, there is an issue of class imbalance, since a
large number of user reviews receive little to no votes. PPrior employs a
pre-trained T5 model and works in three phases. Phase one adapts the
pre-trained T5 model to the user reviews data in a self-supervised fashion. In
phase two, we leverage contrastive training to learn a generic and
task-independent representation of user reviews. Phase three uses radius
neighbors classifier t o m ake t he final predictions. This phase also uses
FAISS index for scalability and efficient search. To conduct extensive
experiments, we acquired a large dataset of over 2.1 million user reviews from
Google Play. Our experimental results demonstrate the effectiveness of the
proposed framework when compared against several state-of-the-art approaches.
Moreover, the accuracy of PPrior in predicting prominent reviews is comparable
to that of experienced app developers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2022 IEEE International Conference on Big Data (Big Data)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Know Your Contextual Search Intent: A <span class="highlight-title">Prompt</span>ing
  Framework for <span class="highlight-title">Conversation</span>al Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelong Mao, Zhicheng Dou, Haonan Chen, Fengran Mo, Hongjin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a prompting framework called LLMCS that leverages
large language models, such as code-davinci-002 of GPT-3, to perform few-shot
conversational query rewriting for conversational search. We explore three
prompting methods to generate multiple query rewrites and hypothetical
responses, and propose aggregating them into an integrated representation that
can robustly represent the user's real contextual search intent. Experimental
results on two conversational search datasets, including CAst-19 and CAsT-20,
show that our approach achieves significant improvements in search
effectiveness over existing baselines and manual rewrites. Notably, LLMCS can
significantly outperform the state-of-the-art baselines by up to +5.9\% and
+32.9\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential
of large language models for conversational search. Our code will be released
at https://github.com/kyriemao/LLMCS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention over Self-attention:Intention-aware Re-ranking with Dynamic
  <span class="highlight-title">Transformer</span> Encoders for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05333v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05333v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Sheng Zang, Rundong Wang, Zhu Sun, J. Senthilnath, Chi Xu, Chee-Keong Kwoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Re-ranking models refine item recommendation lists generated by the prior
global ranking model, which have demonstrated their effectiveness in improving
the recommendation quality. However, most existing re-ranking solutions only
learn from implicit feedback with a shared prediction model, which regrettably
ignore inter-item relationships under diverse user intentions. In this paper,
we propose a novel Intention-aware Re-ranking Model with Dynamic Transformer
Encoder (RAISE), aiming to perform user-specific prediction for each individual
user based on her intentions. Specifically, we first propose to mine latent
user intentions from text reviews with an intention discovering module (IDM).
By differentiating the importance of review information with a co-attention
network, the latent user intention can be explicitly modeled for each user-item
pair. We then introduce a dynamic transformer encoder (DTE) to capture
user-specific inter-item relationships among item candidates by seamlessly
accommodating the learned latent user intentions via IDM. As such, one can not
only achieve more personalized recommendations but also obtain corresponding
explanations by constructing RAISE upon existing recommendation engines.
Empirical study on four public datasets shows the superiority of our proposed
RAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by
Precision@5, MAP@5, and NDCG@5 respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: Convolutional Dimension Interaction for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14129v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14129v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Lei Feng, Xingzhi Guo, Yu Zhang, Rui Yin, Chee Keong Kwoh, Chi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning-based recommendation models play a dominant role
among recommendation techniques. However, most of the existing methods assume
both historical interactions and embedding dimensions are independent of each
other, and thus regrettably ignore the high-order interaction information among
historical interactions and embedding dimensions. In this paper, we propose a
novel representation learning-based model called COMET (COnvolutional diMEnsion
inTeraction), which simultaneously models the high-order interaction patterns
among historical interactions and embedding dimensions. To be specific, COMET
stacks the embeddings of historical interactions horizontally at first, which
results in two "embedding maps". In this way, internal interactions and
dimensional interactions can be exploited by convolutional neural networks
(CNN) with kernels of different sizes simultaneously. A fully-connected
multi-layer perceptron (MLP) is then applied to obtain two interaction vectors.
Lastly, the representations of users and items are enriched by the learnt
interaction vectors, which can further be used to produce the final prediction.
Extensive experiments and ablation studies on various public implicit feedback
datasets clearly demonstrate the effectiveness and rationality of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TIST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Dataset</span> for Learning Graph Representations to Predict Customer Returns
  in Fashion Retail <span class="chip">RecSys 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie McGowan, Elizabeth Guest, Ziyang Yan, Cong Zheng, Neha Patel, Mason Cusack, Charlie Donaldson, Sofie de Cnudde, Gabriel Facini, Fabon Dzogang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel dataset collected by ASOS (a major online fashion
retailer) to address the challenge of predicting customer returns in a fashion
retail ecosystem. With the release of this substantial dataset we hope to
motivate further collaboration between research communities and the fashion
industry. We first explore the structure of this dataset with a focus on the
application of Graph Representation Learning in order to exploit the natural
data structure and provide statistical insights into particular features within
the data. In addition to this, we show examples of a return prediction
classification task with a selection of baseline models (i.e. with no
intermediate representation learning step) and a graph representation based
model. We show that in a downstream return prediction classification task, an
F1-score of 0.792 can be found using a Graph Neural Network (GNN), improving
upon other models discussed in this work. Alongside this increased F1-score, we
also present a lower cross-entropy loss by recasting the data into a graph
structure, indicating more robust predictions from a GNN based solution. These
results provide evidence that GNNs could provide more impactful and usable
classifications than other baseline models on the presented dataset and with
this motivation, we hope to encourage further research into graph-based
approaches using the ASOS GraphReturns dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The ASOS GraphReturns dataset can be found at https://osf.io/c793h/.
  Accepted at FashionXRecSys 2022 workshop. Published Version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Decentralized Federated Lifelong Learning for Landmark
  Localization in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a recent development in the machine learning area that
allows a system of devices to train on one or more tasks without sharing their
data to a single location or device. However, this framework still requires a
centralized global model to consolidate individual models into one, and the
devices train synchronously, which both can be potential bottlenecks for using
federated learning. In this paper, we propose a novel method of asynchronous
decentralized federated lifelong learning (ADFLL) method that inherits the
merits of federated learning and can train on multiple tasks simultaneously
without the need for a central node or synchronous training. Thus, overcoming
the potential drawbacks of conventional federated learning. We demonstrate
excellent performance on the brain tumor segmentation (BRATS) dataset for
localizing the left ventricle on multiple image sequences and image
orientation. Our framework allows agents to achieve the best performance with a
mean distance error of 7.81, better than the conventional all-knowing agent's
mean distance error of 11.78, and significantly (p=0.01) better than a
conventional lifelong learning agent with a distance error of 15.17 after eight
rounds of training. In addition, all ADFLL agents have comparable or better
performance than a conventional LL agent. In conclusion, we developed an ADFLL
framework with excellent performance and speed-up compared to conventional RL
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces <span class="chip">ICSE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SM Hasan Mansur, Sabiha Salma, Damilola Awofisayo, Kevin Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past studies have illustrated the prevalence of UI dark patterns, or user
interfaces that can lead end-users toward (unknowingly) taking actions that
they may not have intended. Such deceptive UI designs can result in adverse
effects on end users, such as oversharing personal information or financial
loss. While significant research progress has been made toward the development
of dark pattern taxonomies, developers and users currently lack guidance to
help recognize, avoid, and navigate these often subtle design motifs. However,
automated recognition of dark patterns is a challenging task, as the
instantiation of a single type of pattern can take many forms, leading to
significant variability.
  In this paper, we take the first step toward understanding the extent to
which common UI dark patterns can be automatically recognized in modern
software applications. To do this, we introduce AidUI, a novel automated
approach that uses computer vision and natural language processing techniques
to recognize a set of visual and textual cues in application screenshots that
signify the presence of ten unique UI dark patterns, allowing for their
detection, classification, and localization. To evaluate our approach, we have
constructed ContextDP, the current largest dataset of fully-localized UI dark
patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark
pattern instances. The results of our evaluation illustrate that \AidUI
achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in
detecting dark pattern instances, reports few false positives, and is able to
localize detected patterns with an IoU score of ~0.84. Furthermore, a
significant subset of our studied dark patterns can be detected quite reliably
(F1 score of over 0.82), and future research directions may allow for improved
detection of additional patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Accepted at The 45th IEEE/ACM International Conference on
  Software Engineering (ICSE 2023), Melbourne, Australia, May 14th-20th, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Module-Wise Network Quantization for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saqib Javed, Andrew Price, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many edge applications, such as collaborative robotics and spacecraft
rendezvous, can benefit from 6D object pose estimation, but must do so on
embedded platforms. Unfortunately, existing 6D pose estimation networks are
typically too large for deployment in such situations and must therefore be
compressed, while maintaining reliable performance. In this work, we present an
approach to doing so by quantizing such networks. More precisely, we introduce
a module-wise quantization strategy that, in contrast to uniform and
mixed-precision quantization, accounts for the modular structure of typical 6D
pose estimation frameworks. We demonstrate that uniquely compressing these
modules outperforms uniform and mixed-precision quantization techniques.
Moreover, our experiments evidence that module-wise quantization can lead to a
significant accuracy boost. We showcase the generality of our approach using
different datasets, quantization methodologies, and network architectures,
including the recent ZebraPose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Strategies for Faster Inference using Speech <span class="highlight-title">Self-Supervised</span>
  Models: A Comparative Study <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salah Zaiem, Robin Algayres, Titouan Parcollet, Slim Essid, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has allowed substantial progress in Automatic
Speech Recognition (ASR) performance in low-resource settings. In this context,
it has been demonstrated that larger self-supervised feature extractors are
crucial for achieving lower downstream ASR error rates. Thus, better
performance might be sanctioned with longer inferences. This article explores
different approaches that may be deployed during the fine-tuning to reduce the
computations needed in the SSL encoder, leading to faster inferences. We adapt
a number of existing techniques to common ASR settings and benchmark them,
displaying performance drops and gains in inference times. Interestingly, we
found that given enough downstream data, a simple downsampling of the input
sequences outperforms the other methods with both low performance drops and
high computational savings, reducing computations by 61.3% with an WER increase
of only 0.81. Finally, we analyze the robustness of the comparison to changes
in dataset conditions, revealing sensitivity to dataset size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP "Self-supervision in Audio, Speech and Beyond"
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Optimality of Elman-type RNN in the Mean-Field Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Agazzi, Jianfeng Lu, Sayan Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze Elman-type Recurrent Reural Networks (RNNs) and their training in
the mean-field regime. Specifically, we show convergence of gradient descent
training dynamics of the RNN to the corresponding mean-field formulation in the
large width limit. We also show that the fixed points of the limiting
infinite-width dynamics are globally optimal, under some assumptions on the
initialization of the weights. Our results establish optimality for
feature-learning with wide RNNs in the mean-field regime
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Knowledge</span>-integrated AutoEncoder Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Liron Simon-Keren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data encoding is a common and central operation in most data analysis tasks.
The performance of other models, downstream in the computational process,
highly depends on the quality of data encoding. One of the most powerful ways
to encode data is using the neural network AutoEncoder (AE) architecture.
However, the developers of AE are not able to easily influence the produced
embedding space, as it is usually treated as a \textit{black box} technique,
which makes it uncontrollable and not necessarily has desired properties for
downstream tasks. In this paper, we introduce a novel approach for developing
AE models that can integrate external knowledge sources into the learning
process, possibly leading to more accurate results. The proposed
\methodNamefull{} (\methodName{}) model is able to leverage domain-specific
information to make sure the desired distance and neighborhood properties
between samples are preservative in the embedding space. The proposed model is
evaluated on three large-scale datasets from three different scientific fields
and is compared to nine existing encoding models. The results demonstrate that
the \methodName{} model effectively captures the underlying structures and
relationships between the input data and external knowledge, meaning it
generates a more useful representation. This leads to outperforming the rest of
the models in terms of reconstruction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision Making for Human-in-the-loop Robotic Agents via
  Uncertainty-Aware Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singi, Zhanpeng He, Alvin Pan, Sandip Patel, Gunnar A. Sigurdsson, Robinson Piramuthu, Shuran Song, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly
autonomously in solving a task, but can request help from an external expert
when needed. However, knowing when to request such assistance is critical: too
few requests can lead to the robot making mistakes, but too many requests can
overload the expert. In this paper, we present a Reinforcement Learning based
approach to this problem, where a semi-autonomous agent asks for external
assistance when it has low confidence in the eventual success of the task. The
confidence level is computed by estimating the variance of the return from the
current state. We show that this estimate can be iteratively improved during
training using a Bellman-like recursion. On discrete navigation problems with
both fully- and partially-observable state information, we show that our method
makes effective use of a limited budget of expert calls at run-time, despite
having no access to the expert at training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch & Learn with Post-hoc Correction for Predict+Optimize with
  Unknown Parameters in Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Hu, Jasper C. H. Lee, Jimmy H. M. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining machine learning and constrained optimization, Predict+Optimize
tackles optimization problems containing parameters that are unknown at the
time of solving. Prior works focus on cases with unknowns only in the
objectives. A new framework was recently proposed to cater for unknowns also in
constraints by introducing a loss function, called Post-hoc Regret, that takes
into account the cost of correcting an unsatisfiable prediction. Since Post-hoc
Regret is non-differentiable, the previous work computes only its
approximation. While the notion of Post-hoc Regret is general, its specific
implementation is applicable to only packing and covering linear programming
problems. In this paper, we first show how to compute Post-hoc Regret exactly
for any optimization problem solvable by a recursive algorithm satisfying
simple conditions. Experimentation demonstrates substantial improvement in the
quality of solutions as compared to the earlier approximation approach.
Furthermore, we show experimentally the empirical behavior of different
combinations of correction and penalty functions used in the Post-hoc Regret of
the same benchmarks. Results provide insights for defining the appropriate
Post-hoc Regret in different application scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holistic Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15829v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15829v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Bertsimas, Kimberly Villalobos Carballo, Léonard Boussioux, Michael Lingzhi Li, Alex Paskov, Ivan Paskov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel holistic deep learning framework that
simultaneously addresses the challenges of vulnerability to input
perturbations, overparametrization, and performance instability from different
train-validation splits. The proposed framework holistically improves accuracy,
robustness, sparsity, and stability over standard deep learning models, as
demonstrated by extensive experiments on both tabular and image data sets. The
results are further validated by ablation experiments and SHAP value analysis,
which reveal the interactions and trade-offs between the different evaluation
metrics. To support practitioners applying our framework, we provide a
prescriptive approach that offers recommendations for selecting an appropriate
training loss function based on their specific objectives. All the code to
reproduce the results can be found at https://github.com/kimvc7/HDL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An FPGA-Based On-Device Reinforcement Learning Approach using Online
  Sequential Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.04646v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.04646v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirohisa Watanabe, Mineto Tsukada, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement
learning using deep neural networks. DQNs require a large buffer and batch
processing for an experience replay and rely on a backpropagation based
iterative optimization, making them difficult to be implemented on
resource-limited edge devices. In this paper, we propose a lightweight
on-device reinforcement learning approach for low-cost FPGA devices. It
exploits a recently proposed neural-network based on-device learning approach
that does not rely on the backpropagation method but uses OS-ELM (Online
Sequential Extreme Learning Machine) based training algorithm. In addition, we
propose a combination of L2 regularization and spectral normalization for the
on-device reinforcement learning so that output values of the neural network
can be fit into a certain range and the reinforcement learning becomes stable.
The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a
low-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate
that the proposed algorithm and its FPGA implementation complete a CartPole-v0
task 29.77x and 89.40x faster than a conventional DQN-based approach when the
number of hidden-layer nodes is 64.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RAW'21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Uncalibrated Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidharth Gupta, Konik Kothari, Valentin Debarnot, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a differentiable imaging framework to address uncertainty in
measurement coordinates such as sensor locations and projection angles. We
formulate the problem as measurement interpolation at unknown nodes supervised
through the forward operator. To solve it we apply implicit neural networks,
also known as neural fields, which are naturally differentiable with respect to
the input coordinates. We also develop differentiable spline interpolators
which perform as well as neural networks, require less time to optimize and
have well-understood properties. Differentiability is key as it allows us to
jointly fit a measurement representation, optimize over the uncertain
measurement coordinates, and perform image reconstruction which in turn ensures
consistent calibration. We apply our approach to 2D and 3D computed tomography
and show that it produces improved reconstructions compared to baselines that
do not account for the lack of calibration. The flexibility of the proposed
framework makes it easy to apply to almost arbitrary imaging problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOCUS: A Novel Decomposition Method for Brain Network Connectivity
  Matrices using Low-rank Structure with Uniform Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.08915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.08915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Wang, Ying Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network-oriented research has been increasingly popular in many scientific
areas. In neuroscience research, imaging-based network connectivity measures
have become the key for understanding brain organizations, potentially serving
as individual neural fingerprints. There are major challenges in analyzing
connectivity matrices including the high dimensionality of brain networks,
unknown latent sources underlying the observed connectivity, and the large
number of brain connections leading to spurious findings. In this paper, we
propose a novel blind source separation method with low-rank structure and
uniform sparsity (LOCUS) as a fully data-driven decomposition method for
network measures. Compared with the existing method that vectorizes
connectivity matrices ignoring brain network topology, LOCUS achieves more
efficient and accurate source separation for connectivity matrices using
low-rank structure. We propose a novel angle-based uniform sparsity
regularization that demonstrates better performance than the existing sparsity
controls for low-rank tensor methods. We propose a highly efficient iterative
Node-Rotation algorithm that exploits the block multi-convexity of the
objective function to solve the non-convex optimization problem for learning
LOCUS. We illustrate the advantage of LOCUS through extensive simulation
studies. Application of LOCUS to Philadelphia Neurodevelopmental Cohort
neuroimaging study reveals biologically insightful connectivity traits which
are not found using the existing method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.10019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.10019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Lazar Reich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Critical decisions like loan approvals, medical interventions, and college
admissions are guided by predictions made in the presence of uncertainty. In
this paper, we prove that uncertainty has a disparate impact. While it imparts
errors across all demographic groups, the types of errors vary systematically:
Groups with higher average outcomes are typically assigned higher false
positive rates, while those with lower average outcomes are assigned higher
false negative rates. We show that additional data acquisition can eliminate
the disparity and broaden access to opportunity. The strategy, which we call
Affirmative Information, could stand as an alternative to Affirmative Action.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingang Qu, Thibault Faney, Ze Wang, Patrick Gallinari, Soleiman Yousef, Jean-Charles de Hemptinne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to domain shift, machine learning systems typically fail to generalize
well to domains different from those of training data, which is what domain
generalization (DG) aims to address. Although various DG methods have been
developed, most of them lack interpretability and require domain labels that
are not available in many real-world scenarios. This paper presents a novel DG
method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does
not rely on domain labels and is more interpretable. MoE proves effective in
identifying heterogeneous patterns in data. For the DG problem, heterogeneity
arises exactly from domain shift. HMOE uses hypernetworks taking vectors as
input to generate experts' weights, which allows experts to share useful
meta-knowledge and enables exploring experts' similarities in a low-dimensional
vector space. We compare HMOE with other DG algorithms under a fair and unified
benchmark-DomainBed. Our extensive experiments show that HMOE can divide
mixed-domain data into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Compared to other DG methods,
HMOE shows competitive performance and achieves SOTA results in some cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Continual Test-Time Adaptation for Contextual and Semantic
  Domain Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommie Kerssies, Mert Kılıçkaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, our goal is to adapt a pre-trained convolutional neural
network to domain shifts at test time. We do so continually with the incoming
stream of test batches, without labels. The existing literature mostly operates
on artificial shifts obtained via adversarial perturbations of a test image.
Motivated by this, we evaluate the state of the art on two realistic and
challenging sources of domain shifts, namely contextual and semantic shifts.
Contextual shifts correspond to the environment types, for example, a model
pre-trained on indoor context has to adapt to the outdoor context on CORe-50.
Semantic shifts correspond to the capture types, for example a model
pre-trained on natural images has to adapt to cliparts, sketches, and paintings
on DomainNet. We include in our analysis recent techniques such as
Prediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and
Continual Test-Time Adaptation (CoTTA). Our findings are three-fold: i)
Test-time adaptation methods perform better and forget less on contextual
shifts compared to semantic shifts, ii) TENT outperforms other methods on
short-term adaptation, whereas CoTTA outpeforms other methods on long-term
adaptation, iii) BN is most reliable and robust. Our code is available at
https://github.com/tommiekerssies/Evaluating-Continual-Test-Time-Adaptation-for-Contextual-and-Semantic-Domain-Shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How I Learned to Stop Worrying and Love Retraining <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.00843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.00843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Zimmer, Christoph Spiegel, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Neural Network Pruning approaches consist of several iterative training
and pruning steps, seemingly losing a significant amount of their performance
after pruning and then recovering it in the subsequent retraining phase. Recent
works of Renda et al. (2020) and Le & Hua (2021) demonstrate the significance
of the learning rate schedule during the retraining phase and propose specific
heuristics for choosing such a schedule for IMP (Han et al., 2015). We place
these findings in the context of the results of Li et al. (2020) regarding the
training of models within a fixed training budget and demonstrate that,
consequently, the retraining phase can be massively shortened using a simple
linear learning rate schedule. Improving on existing retraining approaches, we
additionally propose a method to adaptively select the initial value of the
linear schedule. Going a step further, we propose similarly imposing a budget
on the initial dense training phase and show that the resulting simple and
efficient method is capable of outperforming significantly more complex or
heavily parameterized state-of-the-art approaches that attempt to sparsify the
network during training. These findings not only advance our understanding of
the retraining phase, but more broadly question the belief that one should aim
to avoid the need for retraining and reduce the negative effects of 'hard'
pruning by incorporating the sparsification process into the standard training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2023 camera-ready version, 9 pages main text, 34 pages appendix,
  2 tables, 3 figures in main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A large-scale and PCR-referenced vocal audio <span class="highlight-title">dataset</span> for COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jobie Budd, Kieran Baker, Emma Karoune, Harry Coppock, Selina Patel, Ana Tendero Cañadas, Alexander Titcomb, Richard Payne, David Hurley, Sabrina Egglestone, Lorraine Butler, Jonathon Mellor, George Nicholson, Ivan Kiskin, Vasiliki Koutra, Radka Jersakova, Rachel A. McKendry, Peter Diggle, Sylvia Richardson, Björn W. Schuller, Steven Gilmour, Davide Pigoli, Stephen Roberts, Josef Packham, Tracey Thornley, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UK COVID-19 Vocal Audio Dataset is designed for the training and
evaluation of machine learning models that classify SARS-CoV-2 infection status
or associated respiratory symptoms using vocal audio. The UK Health Security
Agency recruited voluntary participants through the national Test and Trace
programme and the REACT-1 survey in England from March 2021 to March 2022,
during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and
some Omicron variant sublineages. Audio recordings of volitional coughs,
exhalations, and speech were collected in the 'Speak up to help beat
coronavirus' digital survey alongside demographic, self-reported symptom and
respiratory condition data, and linked to SARS-CoV-2 test results. The UK
COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2
PCR-referenced audio recordings to date. PCR results were linked to 70,794 of
72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms
were reported by 45.62% of participants. This dataset has additional potential
uses for bioacoustics research, with 11.30% participants reporting asthma, and
27.20% with linked influenza PCR test results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: Convolutional Dimension Interaction for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14129v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14129v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Lei Feng, Xingzhi Guo, Yu Zhang, Rui Yin, Chee Keong Kwoh, Chi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning-based recommendation models play a dominant role
among recommendation techniques. However, most of the existing methods assume
both historical interactions and embedding dimensions are independent of each
other, and thus regrettably ignore the high-order interaction information among
historical interactions and embedding dimensions. In this paper, we propose a
novel representation learning-based model called COMET (COnvolutional diMEnsion
inTeraction), which simultaneously models the high-order interaction patterns
among historical interactions and embedding dimensions. To be specific, COMET
stacks the embeddings of historical interactions horizontally at first, which
results in two "embedding maps". In this way, internal interactions and
dimensional interactions can be exploited by convolutional neural networks
(CNN) with kernels of different sizes simultaneously. A fully-connected
multi-layer perceptron (MLP) is then applied to obtain two interaction vectors.
Lastly, the representations of users and items are enriched by the learnt
interaction vectors, which can further be used to produce the final prediction.
Extensive experiments and ablation studies on various public implicit feedback
datasets clearly demonstrate the effectiveness and rationality of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TIST</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Graph Learning for Acoustic Event Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shirian, Mona Ahmadian, Krishna Somandepalli, Tanaya Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous graphs provide a compact, efficient, and scalable way to model
data involving multiple disparate modalities. This makes modeling audiovisual
data using heterogeneous graphs an attractive option. However, graph structure
does not appear naturally in audiovisual data. Graphs for audiovisual data are
constructed manually which is both difficult and sub-optimal. In this work, we
address this problem by (i) proposing a parametric graph construction strategy
for the intra-modal edges, and (ii) learning the crossmodal edges. To this end,
we develop a new model, heterogeneous graph crossmodal network (HGCN) that
learns the crossmodal edges. Our proposed model can adapt to various spatial
and temporal scales owing to its parametric construction, while the learnable
crossmodal edges effectively connect the relevant nodes across modalities.
Experiments on a large benchmark dataset (AudioSet) show that our model is
state-of-the-art (0.53 mean average precision), outperforming transformer-based
models and other graph-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2207.07935</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision, Deduction and Alignment: An Empirical Study on Multi-modal
  <span class="highlight-title">Knowledge</span> Graph Alignment <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-11T00:00:00Z">2023-03-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcription free filler word detection with Neural semi-CRFs <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Yujia Yan, Juan-Pablo Caceres, Zhiyao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-linguistic filler words, such as "uh" or "um", are prevalent in
spontaneous speech and serve as indicators for expressing hesitation or
uncertainty. Previous works for detecting certain non-linguistic filler words
are highly dependent on transcriptions from a well-established commercial
automatic speech recognition (ASR) system. However, certain ASR systems are not
universally accessible from many aspects, e.g., budget, target languages, and
computational power. In this work, we investigate filler word detection system
that does not depend on ASR systems. We show that, by using the structured
state space sequence model (S4) and neural semi-Markov conditional random
fields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment
level) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a
qualitative analysis on the detected results to analyze the limitations of our
proposed system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We will release the codes and models at
  https://github.com/yangbang18/ZeroNLG soon. Without any labeled downstream
  pairs for training, the ZeroNLG can deal with multiple NLG tasks, including
  image-to-text, video-to-text, and text-to-text, across English, Chinese,
  German, and French within a unified framework</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parachute: Evaluating Interactive Human-LM Co-writing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Shen, Tongshuang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A surge of advances in language models (LMs) has led to significant interest
in using LMs to build co-writing systems, in which humans and LMs interactively
contribute to a shared writing artifact. However, there is a lack of studies
assessing co-writing systems in interactive settings. We propose a
human-centered evaluation framework, Parachute, for interactive co-writing
systems. Parachute showcases an integrative view of interaction evaluation,
where each evaluation aspect consists of categorized practical metrics.
Furthermore, we present Parachute with a use case to demonstrate how to
evaluate and compare co-writing systems using Parachute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CHI'23 In2Writing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stabilizing <span class="highlight-title">Transformer</span> Training by Preventing Attention Entropy
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, Josh Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training stability is of great importance to Transformers. In this work, we
investigate the training dynamics of Transformers by examining the evolution of
the attention layers. In particular, we track the attention entropy for each
attention head during the course of training, which is a proxy for model
sharpness. We identify a common pattern across different architectures and
tasks, where low attention entropy is accompanied by high training instability,
which can take the form of oscillating loss or divergence. We denote the
pathologically low attention entropy, corresponding to highly concentrated
attention scores, as $\textit{entropy collapse}$. As a remedy, we propose
$\sigma$Reparam, a simple and efficient solution where we reparametrize all
linear layers with spectral normalization and an additional learned scalar. We
demonstrate that the proposed reparameterization successfully prevents entropy
collapse in the attention layers, promoting more stable training. Additionally,
we prove a tight lower bound of the attention entropy, which decreases
exponentially fast with the spectral norm of the attention logits, providing
additional motivation for our approach. We conduct experiments with
$\sigma$Reparam on image classification, image self-supervised learning,
machine translation, automatic speech recognition, and language modeling tasks,
across Transformer architectures. We show that $\sigma$Reparam provides
stability and robustness with respect to the choice of hyperparameters, going
so far as enabling training (a) a Vision Transformer to competitive performance
without warmup, weight decay, layer normalization or adaptive optimizers; (b)
deep architectures in machine translation and (c) speech recognition to
competitive performance without warmup and adaptive optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Consist</span>ency Analysis of <span class="highlight-title">ChatGPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongjun Jang, Thomas Lukasiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, a question-and-answer dialogue system based on a large language
model, has gained huge popularity since its introduction. Its positive aspects
have been reported through many media platforms, and some analyses even showed
that ChatGPT achieved a decent grade in professional exams, including the law,
medical, and finance domains, adding extra support to the claim that AI now can
assist and, even, replace humans in industrial fields. Others, however, doubt
its reliability and trustworthiness. In this paper, we investigate ChatGPT's
trustworthiness regarding logically consistent behaviours. Our findings suggest
that, although ChatGPT seems to achieve an improved language understanding
ability, it still fails to generate logically correct predictions frequently.
Hence, while it is true that ChatGPT is an impressive and promising new
technique, we conclude that its usage in real-world applications without
thorough human inspection requires further consideration, especially for
risk-sensitive areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interactive UI to Support Sensemaking over Collections of Parallel
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joyce Zhou, Elena Glassman, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientists and science journalists, among others, often need to make sense of
a large number of papers and how they compare with each other in scope, focus,
findings, or any other important factors. However, with a large corpus of
papers, it's cognitively demanding to pairwise compare and contrast them all
with each other. Fully automating this review process would be infeasible,
because it often requires domain-specific knowledge, as well as understanding
what the context and motivations for the review are. While there are existing
tools to help with the process of organizing and annotating papers for
literature reviews, at the core they still rely on people to serially read
through papers and manually make sense of relevant information.
  We present AVTALER, which combines peoples' unique skills, contextual
awareness, and knowledge, together with the strength of automation. Given a set
of comparable text excerpts from a paper corpus, it supports users in
sensemaking and contrasting paper attributes by interactively aligning text
excerpts in a table so that comparable details are presented in a shared
column. AVTALER is based on a core alignment algorithm that makes use of modern
NLP tools. Furthermore, AVTALER is a mixed-initiative system: users can
interactively give the system constraints which are integrated into the
alignment construction process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatically Extracting Information in Medical <span class="highlight-title">Dialogue</span>: Expert System
  And Attention for Labelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshi Wang, Daniel Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical dialogue information extraction is becoming an increasingly
significant problem in modern medical care. It is difficult to extract key
information from electronic medical records (EMRs) due to their large numbers.
Previously, researchers proposed attention-based models for retrieving features
from EMRs, but their limitations were reflected in their inability to recognize
different categories in medical dialogues. In this paper, we propose a novel
model, Expert System and Attention for Labelling (ESAL). We use mixture of
experts and pre-trained BERT to retrieve the semantics of different categories,
enabling the model to fuse the differences between them. In our experiment,
ESAL was applied to a public dataset and the experimental results indicated
that ESAL significantly improved the performance of Medical Information
Classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the impact of contextual information in hate speech detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Pérez, Franco Luque, Demian Zayat, Martín Kondratzky, Agustín Moro, Pablo Serrati, Joaquín Zajac, Paula Miguel, Natalia Debandi, Agustín Gravano, Viviana Cotik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-view Graph Neural Networks for <span class="highlight-title">Knowledge</span> Graph Completion <span class="chip">ESWC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09231v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09231v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinh Tong, Dai Quoc Nguyen, Dinh Phung, Dat Quoc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an effective graph neural network (GNN)-based knowledge graph
embedding model, which we name WGE, to capture entity- and relation-focused
graph structures. Given a knowledge graph, WGE builds a single undirected
entity-focused graph that views entities as nodes. WGE also constructs another
single undirected graph from relation-focused constraints, which views entities
and relations as nodes. WGE then proposes a GNN-based architecture to better
learn vector representations of entities and relations from these two single
entity- and relation-focused graphs. WGE feeds the learned entity and relation
representations into a weighted score function to return the triple scores for
knowledge graph completion. Experimental results show that WGE outperforms
strong baselines on seven benchmark datasets for knowledge graph completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of ESWC 2023; 17 pages; 4 tables; 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Token-level Contrastive Framework for Sign Language Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Fu, Peigen Ye, Liang Zhang, Pei Yu, Cong Hu, Yidong Chen, Xiaodong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign Language Translation (SLT) is a promising technology to bridge the
communication gap between the deaf and the hearing people. Recently,
researchers have adopted Neural Machine Translation (NMT) methods, which
usually require large-scale corpus for training, to achieve SLT. However, the
publicly available SLT corpus is very limited, which causes the collapse of the
token representations and the inaccuracy of the generated tokens. To alleviate
this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive
learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation ,
which learns effective token representations by incorporating token-level
contrastive learning into the SLT decoding process. Concretely, ConSLT treats
each token and its counterpart generated by different dropout masks as positive
pairs during decoding, and then randomly samples $K$ tokens in the vocabulary
that are not in the current sentence to construct negative examples. We conduct
comprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both
end-to-end and cascaded settings. The experimental results demonstrate that
ConSLT can achieve better translation quality than the strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Wihan van der Heever, Ranjan Satapathy, Gianmarco Mengaldo, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach for explainability in financial analysis
by utilizing the Pearson correlation coefficient to establish a relationship
between aspect-based sentiment analysis and stock prices. The proposed
methodology involves constructing an aspect list from financial news articles
and analyzing sentiment intensity scores for each aspect. These scores are then
compared to the stock prices for the relevant companies using the Pearson
coefficient to determine any significant correlations. The results indicate
that the proposed approach provides a more detailed and accurate understanding
of the relationship between sentiment analysis and stock prices, which can be
useful for investors and financial analysts in making informed decisions.
Additionally, this methodology offers a transparent and interpretable way to
explain the sentiment analysis results and their impact on stock prices.
Overall, the findings of this paper demonstrate the importance of
explainability in financial analysis and highlight the potential benefits of
utilizing the Pearson coefficient for analyzing aspect-based sentiment analysis
and stock prices. The proposed approach offers a valuable tool for
understanding the complex relationships between financial news sentiment and
stock prices, providing a new perspective on the financial market and aiding in
making informed investment decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Named Entity Detection and Injection for Direct Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Gaido, Yun Tang, Ilia Kulikov, Rongqing Huang, Hongyu Gong, Hirofumi Inaguma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a sentence, certain words are critical for its semantic. Among them, named
entities (NEs) are notoriously challenging for neural models. Despite their
importance, their accurate handling has been neglected in speech-to-text (S2T)
translation research, and recent work has shown that S2T models perform poorly
for locations and notably person names, whose spelling is challenging unless
known in advance. In this work, we explore how to leverage dictionaries of NEs
known to likely appear in a given context to improve S2T model outputs. Our
experiments show that we can reliably detect NEs likely present in an utterance
starting from S2T encoder outputs. Indeed, we demonstrate that the current
detection quality is sufficient to improve NE accuracy in the translation with
a 31% reduction in person name errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmentation with Projection: Towards an Effective and Efficient Data
  Augmentation Paradigm for Distillation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Yuexin Wu, Frederick Liu, Daogao Liu, Le Hou, Hongkun Yu, Jing Li, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is one of the primary methods of transferring
knowledge from large to small models. However, it requires massive
task-specific data, which may not be plausible in many real-world applications.
Data augmentation methods such as representation interpolation, token
replacement, or augmentation with models are applied to tackle this problem.
However, these data augmentation methods either potentially cause shifts in
decision boundaries (representation interpolation), are not expressive enough
(token replacement), or introduce too much computational overhead (augmentation
with models). To this end, we propose AugPro (Augmentation with Projection), an
effective and efficient data augmentation method for distillation. Our method
builds on top of representation interpolation augmentation methods to maintain
the diversity of expressions and converts the augmented data to tokens to avoid
shifting decision boundaries. It uses simple operations that come with little
computational overhead. The results on multiple GLUE tasks show that our
methods can improve distillation performance by a large margin at a low time
cost. Codes are available at
https://github.com/google-research/google-research/tree/master/augpro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures. Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Computational Power of Nondeterministic Stack RNNs <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01343v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01343v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian DuSell, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recurrent neural networks (RNNs) have a fixed, finite number of
memory cells. In theory (assuming bounded range and precision), this limits
their formal language recognition power to regular languages, and in practice,
RNNs have been shown to be unable to learn many context-free languages (CFLs).
In order to expand the class of languages RNNs recognize, prior work has
augmented RNNs with a nondeterministic stack data structure, putting them on
par with pushdown automata and increasing their language recognition power to
CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic
CFLs), but in this paper, we show that nondeterminism and the neural controller
interact to produce two more unexpected abilities. First, the nondeterministic
stack RNN can recognize not only CFLs, but also many non-context-free
languages. Second, it can recognize languages with much larger alphabet sizes
than one might expect given the size of its stack alphabet. Finally, to
increase the information capacity in the stack and allow it to solve more
complicated tasks with large alphabet sizes, we propose a new version of the
nondeterministic stack that simulates stacks of vectors rather than discrete
symbols. We demonstrate perplexity improvements with this new model on the Penn
Treebank language modeling benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures. Published at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PowerMat: context-aware recommender system without user item rating
  values that solves the cold-start problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems serves as an important technical asset in many modern
companies. With the increasing demand for higher precision of the technology,
more and more research and investment has been allocated to the field. One
important sub-field of recommender systems that has been stagnating is
context-aware recommender systems. Due to the difficulty of collecting input
dataset, the amount of research on context-aware recommender systems is much
less than other sub-fields of recommender systems. In this paper, we propose a
new algorithm named PowerMat to tackle the context-aware recommendation
problem. We build our theory on matrix factorization and Zipf's law, and also
more recent research work such as DotMat. We prove by experiments that our
method achieves superior results to the classic matrix factorization algorithm
and other context-aware recommender systems such as MovieMat+. In addition, by
theoretical analysis, we show that our algorithm solves the cold-start problem
for context-aware recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Betti Number for Point Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topology is the foundation for many industrial applications ranging from CAD
to simulation analysis. Computational topology mostly focuses on structured
data such as mesh, however unstructured dataset such as point set remains a
virgin land for topology scientists. The significance of point-based topology
can never be overemphasized, especially in the area of reverse engineering,
geometric modeling and algorithmic analysis. In this paper, we propose a novel
approach to compute the Betti number for point set data and illustrate its
usefulness in real world examples. To the best of our knowledge, our work is
pioneering and first of its kind in the fields of computational topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Retention-oriented Recommendation with Decision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, Dawei yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving user retention with reinforcement learning~(RL) has attracted
increasing attention due to its significant importance in boosting user
engagement. However, training the RL policy from scratch without hurting users'
experience is unavoidable due to the requirement of trial-and-error searches.
Furthermore, the offline methods, which aim to optimize the policy without
online interactions, suffer from the notorious stability problem in value
estimation or unbounded variance in counterfactual policy evaluation. To this
end, we propose optimizing user retention with Decision Transformer~(DT), which
avoids the offline difficulty by translating the RL as an autoregressive
problem. However, deploying the DT in recommendation is a non-trivial problem
because of the following challenges: (1) deficiency in modeling the numerical
reward value; (2) data discrepancy between the policy learning and
recommendation generation; (3) unreliable offline performance evaluation. In
this work, we, therefore, contribute a series of strategies for tackling the
exposed issues. We first articulate an efficient reward prompt by weighted
aggregation of meta embeddings for informative reward embedding. Then, we endow
a weighted contrastive learning method to solve the discrepancy between
training and inference. Furthermore, we design two robust offline metrics to
measure user retention. Finally, the significant improvement in the benchmark
datasets demonstrates the superiority of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoMLP: Automated MLP for Sequential Recommendations <span class="chip">WWW'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, Ruocheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems aim to predict users' next interested item
given their historical interactions. However, a long-standing issue is how to
distinguish between users' long/short-term interests, which may be
heterogeneous and contribute differently to the next recommendation. Existing
approaches usually set pre-defined short-term interest length by exhaustive
search or empirical experience, which is either highly inefficient or yields
subpar results. The recent advanced transformer-based models can achieve
state-of-the-art performances despite the aforementioned issue, but they have a
quadratic computational complexity to the length of the input sequence. To this
end, this paper proposes a novel sequential recommender system, AutoMLP, aiming
for better modeling users' long/short-term interests from their historical
interactions. In addition, we design an automated and adaptive search algorithm
for preferable short-term interest length via end-to-end optimization. Through
extensive experiments, we show that AutoMLP has competitive performance against
state-of-the-art methods, while maintaining linear computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'23</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multimodal Information based Speech Processing (MISP) 2022
  Challenge: Audio-Visual Diarization and Recognition <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Shilong Wu, Hang Chen, Mao-Kui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Siniscalchi, Odette Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-modal Information based Speech Processing (MISP) challenge aims to
extend the application of signal processing technology in specific scenarios by
promoting the research into wake-up words, speaker diarization, speech
recognition, and other technologies. The MISP2022 challenge has two tracks: 1)
audio-visual speaker diarization (AVSD), aiming to solve ``who spoken when''
using both audio and visual data; 2) a novel audio-visual diarization and
recognition (AVDR) task that focuses on addressing ``who spoken what when''
with audio-visual speaker diarization results. Both tracks focus on the Chinese
language, and use far-field audio and video in real home-tv scenarios: 2-6
people communicating each other with TV noise in the background. This paper
introduces the dataset, track settings, and baselines of the MISP2022
challenge. Our analyses of experiments and examples indicate the good
performance of AVDR baseline system, and the potential difficulties in this
challenge due to, e.g., the far-field video quality, the presence of TV noise
in the background, and the indistinguishable speakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, to be published in ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution (HR) images are usually downscaled to low-resolution (LR)
ones for better display and afterward upscaled back to the original size to
recover details. Recent work in image rescaling formulates downscaling and
upscaling as a unified task and learns a bijective mapping between HR and LR
via invertible networks. However, in real-world applications (e.g., social
media), most images are compressed for transmission. Lossy compression will
lead to irreversible information loss on LR images, hence damaging the inverse
upscaling procedure and degrading the reconstruction accuracy. In this paper,
we propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware
image rescaling. To tackle the distribution shift, we first develop an
end-to-end asymmetric framework with two separate bijective mappings for
high-quality and compressed LR images, respectively. Then, based on empirical
analysis of this framework, we model the distribution of the lost information
(including downscaling and compression) using isotropic Gaussian mixtures and
propose the Enhanced Invertible Block to derive high-quality/compressed LR
images in one forward pass. Besides, we design a set of losses to regularize
the learned LR images and enhance the invertibility. Extensive experiments
demonstrate the consistent improvements of SAIN across various image rescaling
datasets in terms of both quantitative and qualitative evaluation under
standard image compression formats (i.e., JPEG and WebP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023. Code is available at
  https://github.com/yang-jin-hai/SAIN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMCosine: Multi-Modal Cosine <span class="highlight-title">Loss</span> Towards Balanced Audio-Visual
  Fine-Grained Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruize Xu, Ruoxuan Feng, Shi-Xiong Zhang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual learning helps to comprehensively understand the world by fusing
practical information from multiple modalities. However, recent studies show
that the imbalanced optimization of uni-modal encoders in a joint-learning
model is a bottleneck to enhancing the model's performance. We further find
that the up-to-date imbalance-mitigating methods fail on some audio-visual
fine-grained tasks, which have a higher demand for distinguishable feature
distribution. Fueled by the success of cosine loss that builds hyperspherical
feature spaces and achieves lower intra-class angular variability, this paper
proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$
normalization to features and weights towards balanced and better multi-modal
fine-grained learning. We demonstrate that our method can alleviate the
imbalanced optimization from the perspective of weight norm and fully exploit
the discriminability of the cosine metric. Extensive experiments prove the
effectiveness of our method and the versatility with advanced multi-modal
fusion strategies and up-to-date imbalance-mitigating methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human
  Pose Estimation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxin Lin, Yunwei Chiu, Peiyuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The graph convolutional networks (GCNs) have been applied to model the
physically connected and non-local relations among human joints for 3D human
pose estimation (HPE). In addition, the purely Transformer-based models
recently show promising results in video-based 3D HPE. However, the
single-frame method still needs to model the physically connected relations
among joints because the feature representations transformed only by global
relations via the Transformer neglect information on the human skeleton. To
deal with this problem, we propose a novel method in which the Transformer
encoder and GCN blocks are alternately stacked, namely AMPose, to combine the
global and physically connected relations among joints towards HPE. In the
AMPose, the Transformer encoder is applied to connect each joint with all the
other joints, while GCNs are applied to capture information on physically
connected relations. The effectiveness of our proposed method is evaluated on
the Human3.6M dataset. Our model also shows better generalization ability by
testing on the MPI-INF-3DHP dataset. Code can be retrieved at
https://github.com/erikervalid/AMPose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging <span class="highlight-title">Pre-train</span>ed AudioLDM for Text to Sound <span class="highlight-title">Generation</span>: A
  Benchmark Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yuan, Haohe Liu, Jinhua Liang, Xubo Liu, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have recently achieved breakthroughs in sound generation
with text prompts. Despite their promising performance, current text-to-sound
generation models face issues on small-scale datasets (e.g., overfitting),
significantly limiting their performance. In this paper, we investigate the use
of pre-trained AudioLDM, the state-of-the-art model for text-to-audio
generation, as the backbone for sound generation. Our study demonstrates the
advantages of using pre-trained models for text-to-sound generation, especially
in data-scarcity scenarios. In addition, experiments show that different
training strategies (e.g., training conditions) may affect the performance of
AudioLDM on datasets of different scales. To facilitate future studies, we also
evaluate various text-to-sound generation systems on several frequently used
datasets under the same evaluation protocols, which allow fair comparisons and
benchmarking of these methods on the common ground.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDMIC: Learning-based Distributed Multi-view Image Coding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression plays a critical role in 3D-related
applications. Existing methods adopt a predictive coding architecture, which
requires joint encoding to compress the corresponding disparity as well as
residual information. This demands collaboration among cameras and enforces the
epipolar geometric constraint between different views, which makes it
challenging to deploy these methods in distributed camera systems with randomly
overlapping fields of view. Meanwhile, distributed source coding theory
indicates that efficient data compression of correlated sources can be achieved
by independent encoding and joint decoding, which motivates us to design a
learning-based distributed multi-view image coding (LDMIC) framework. With
independent encoders, LDMIC introduces a simple yet effective joint context
transfer module based on the cross-attention mechanism at the decoder to
effectively capture the global inter-view correlations, which is insensitive to
the geometric relationships between images. Experimental results show that
LDMIC significantly outperforms both traditional and learning-based MIC methods
while enjoying fast encoding speed. Code will be released at
https://github.com/Xinjie-Q/LDMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-10T00:00:00Z">2023-03-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rewarding Chatbots for Real-World Engagement with Millions of Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, William Beauchamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of pretrained large language models has led to the deployment
of a range of social chatbots for chitchat. Although these chatbots demonstrate
language ability and fluency, they are not guaranteed to be engaging and can
struggle to retain users. This work investigates the development of social
chatbots that prioritize user engagement to enhance retention, specifically
examining the use of human feedback to efficiently develop highly engaging
chatbots. The proposed approach uses automatic pseudo-labels collected from
user interactions to train a reward model that can be used to reject
low-scoring sample responses generated by the chatbot model at inference time.
Intuitive evaluation metrics, such as mean conversation length (MCL), are
introduced as proxies to measure the level of engagement of deployed chatbots.
A/B testing on groups of 10,000 new daily chatbot users on the Chai Research
platform shows that this approach increases the MCL by up to 70%, which
translates to a more than 30% increase in user retention for a GPT-J 6B model.
Future work aims to use the reward model to realise a data fly-wheel, where the
latest user conversations can be used to alternately fine-tune the language
model and the reward model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Susceptibility to Influence of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T Mai, Maria Vau, Matthew Caldwell, Augustine Marvor-Parker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two studies tested the hypothesis that a Large Language Model (LLM) can be
used to model psychological change following exposure to influential input. The
first study tested a generic mode of influence - the Illusory Truth Effect
(ITE) - where earlier exposure to a statement (through, for example, rating its
interest) boosts a later truthfulness test rating. Data was collected from 1000
human participants using an online experiment, and 1000 simulated participants
using engineered prompts and LLM completion. 64 ratings per participant were
collected, using all exposure-test combinations of the attributes: truth,
interest, sentiment and importance. The results for human participants
reconfirmed the ITE, and demonstrated an absence of effect for attributes other
than truth, and when the same attribute is used for exposure and test. The same
pattern of effects was found for LLM-simulated participants. The second study
concerns a specific mode of influence - populist framing of news to increase
its persuasion and political mobilization. Data from LLM-simulated participants
was collected and compared to previously published data from a 15-country
experiment on 7286 human participants. Several effects previously demonstrated
from the human study were replicated by the simulated study, including effects
that surprised the authors of the human study by contradicting their
theoretical expectations (anti-immigrant framing of news decreases its
persuasion and mobilization); but some significant relationships found in human
data (modulation of the effectiveness of populist framing according to relative
deprivation of the participant) were not present in the LLM data. Together the
two studies support the view that LLMs have potential to act as models of the
effect of influence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures, 7 tables, 53 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is In-hospital Meta-information Useful for Abstractive Discharge Summary
  <span class="highlight-title">Generation</span>? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenichiro Ando, Mamoru Komachi, Takashi Okumura, Hiromasa Horiguchi, Yuji Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the patient's hospitalization, the physician must record daily
observations of the patient and summarize them into a brief document called
"discharge summary" when the patient is discharged. Automated generation of
discharge summary can greatly relieve the physicians' burden, and has been
addressed recently in the research community. Most previous studies of
discharge summary generation using the sequence-to-sequence architecture focus
on only inpatient notes for input. However, electric health records (EHR) also
have rich structured metadata (e.g., hospital, physician, disease, length of
stay, etc.) that might be useful. This paper investigates the effectiveness of
medical meta-information for summarization tasks. We obtain four types of
meta-information from the EHR systems and encode each meta-information into a
sequence-to-sequence model. Using Japanese EHRs, meta-information encoded
models increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over
the vanilla Longformer. Also, we found that the encoded meta-information
improves the precisions of its related terms in the outputs. Our results showed
the benefit of the use of medical meta-information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust <span class="highlight-title">Knowledge</span> Distillation from RNN-T Models With Noisy Training
  Labels Using Full-Sum <span class="highlight-title">Loss</span> <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zeineldeen, Kartik Audhkhasi, Murali Karthick Baskar, Bhuvana Ramabhadran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies knowledge distillation (KD) and addresses its constraints
for recurrent neural network transducer (RNN-T) models. In hard distillation, a
teacher model transcribes large amounts of unlabelled speech to train a student
model. Soft distillation is another popular KD method that distills the output
logits of the teacher model. Due to the nature of RNN-T alignments, applying
soft distillation between RNN-T architectures having different posterior
distributions is challenging. In addition, bad teachers having high
word-error-rate (WER) reduce the efficacy of KD. We investigate how to
effectively distill knowledge from variable quality ASR teachers, which has not
been studied before to the best of our knowledge. We show that a sequence-level
KD, full-sum distillation, outperforms other distillation methods for RNN-T
models, especially for bad teachers. We also propose a variant of full-sum
distillation that distills the sequence discriminative knowledge of the teacher
leading to further improvement in WER. We conduct experiments on public
datasets namely SpeechStew and LibriSpeech, and on in-house production data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creation and evaluation of timelines for longitudinal user posts <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Hills, Adam Tsakalidis, Federico Nanni, Ioannis Zachos, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest to work with user generated content in social
media, especially textual posts over time. Currently there is no consistent way
of segmenting user posts into timelines in a meaningful way that improves the
quality and cost of manual annotation. Here we propose a set of methods for
segmenting longitudinal user posts into timelines likely to contain interesting
moments of change in a user's behaviour, based on their online posting
activity. We also propose a novel framework for evaluating timelines and show
its applicability in the context of two different social media datasets.
Finally, we present a discussion of the linguistic content of highly ranked
timelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023 (main, long); camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An algebraic approach to translating Japanese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Boboc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use Lambek's pregroups and the framework of compositional distributional
models of language ("DisCoCat") to study translations from Japanese to English
as pairs of functors. Adding decorations to pregroups we show how to handle
word order changes between languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, multiple diagrams and glosses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">Overview</span> on Language Models: Recent Developments and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Wei, Yun-Cheng Wang, Bin Wang, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner. In contrast,
pre-trained language models (PLMs) cover broader concepts and can be used in
both causal sequential modeling and fine-tuning for downstream applications.
PLMs have their own training paradigms (usually self-supervised) and serve as
foundation models in modern NLP systems. This overview paper provides an
introduction to both CLMs and PLMs from five aspects, i.e., linguistic units,
structures, training methods, evaluation methods, and applications.
Furthermore, we discuss the relationship between CLMs and PLMs and shed light
on the future directions of language modeling in the pre-trained era.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aminul Huq, Weiyi Zhang, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems based on deep neural networks are
weak against adversarial perturbations. We propose mixPGD adversarial training
method to improve the robustness of the model for ASR systems. In standard
adversarial training, adversarial samples are generated by leveraging
supervised or unsupervised methods. We merge the capabilities of both
supervised and unsupervised approaches in our method to generate new
adversarial samples which aid in improving model robustness. Extensive
experiments and comparison across various state-of-the-art defense methods and
adversarial attacks have been performed to show that mixPGD gains 4.1% WER of
better performance than previous best performing models under white-box
adversarial attack setting. We tested our proposed defense method against both
white-box and transfer based black-box attack settings to ensure that our
defense strategy is robust against various types of attacks. Empirical results
on several adversarial attacks validate the effectiveness of our proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinical <span class="highlight-title">BERT</span>Score: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler
  and Multiple Choice Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-and-language understanding has a variety of applications in the
industry, such as video question answering, text-video retrieval and
multi-label classification. Existing video-and-language understanding methods
generally adopt heavy multi-modal encoders and feature fusion modules, which
consume large amounts of GPU memory. Especially, they have difficulty dealing
with dense video frames or long text that are prevalent in industrial
applications. In this paper, we propose MuLTI, a highly accurate and
memory-efficient video-and-language understanding model that achieves efficient
and effective feature fusion through feature sampling and attention modules.
Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we
introduce an attention-based adapter to the encoders, which finetunes the
shallow features to improve the model's performance with low GPU memory
consumption. Finally, to further improve the model's performance, we introduce
a new pretraining task named Multiple Choice Modeling to bridge the task gap
between pretraining and downstream tasks and enhance the model's ability to
align the video and the text. Benefiting from the efficient feature fusion
module, the attention-based adapter and the new pretraining task, MuLTI
achieves state-of-the-art performance on multiple datasets. Implementation and
pretrained models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence
  <span class="highlight-title">Reasoning</span> <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Luo, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their similarity-based learning objectives, pretrained sentence
encoders often internalize stereotypical assumptions that reflect the social
biases that exist within their training corpora. In this paper, we describe
several kinds of stereotypes concerning different communities that are present
in popular sentence representation models, including pretrained next sentence
prediction and contrastive sentence representation models. We compare such
models to textual entailment models that learn language logic for a variety of
downstream language understanding tasks. By comparing strong pretrained models
based on text similarity with textual entailment learning, we conclude that the
explicit logic learning with textual entailment can significantly reduce bias
and improve the recognition of social communities, without an explicit
de-biasing process
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUTODIAL: Efficient Asynchronous Task-Oriented <span class="highlight-title">Dialogue</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajjwal Bhargava, Pooyan Amini, Shahin Shayandeh, Chinnadhurai Sankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large dialogue models become commonplace in practice, the problems
surrounding high compute requirements for training, inference and larger memory
footprint still persists. In this work, we present AUTODIAL, a multi-task
dialogue model that addresses the challenges of deploying dialogue model.
AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act
prediction, domain prediction, intent prediction, and dialogue state tracking.
Using classification decoders over generative decoders allows AUTODIAL to
significantly reduce memory footprint and achieve faster inference times
compared to existing generative approach namely SimpleTOD. We demonstrate that
AUTODIAL provides 3-6x speedups during inference while having 11x fewer
parameters on three dialogue tasks compared to SimpleTOD. Our results show that
extending current dialogue models to have parallel decoders can be a viable
alternative for deploying them in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Query Focused Summaries without Fine-tuning the
  <span class="highlight-title">Transformer</span>-based <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deen Abdullah, Shamanth Nayak, Gandharv Suri, Yllias Chali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning the Natural Language Processing (NLP) models for each new data
set requires higher computational time associated with increased carbon
footprint and cost. However, fine-tuning helps the pre-trained models adapt to
the latest data sets; what if we avoid the fine-tuning steps and attempt to
generate summaries using just the pre-trained models to reduce computational
time and cost. In this paper, we tried to omit the fine-tuning steps and
investigate whether the Marginal Maximum Relevance (MMR)-based approach can
help the pre-trained models to obtain query-focused summaries directly from a
new data set that was not used to pre-train the models. First, we used topic
modelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to
generate queries for summarization tasks. Then, using MMR, we ranked the
sentences of the documents according to the queries. Next, we passed the ranked
sentences to seven transformer-based pre-trained models to perform the
summarization tasks. Finally, we used the MMR approach again to select the
query relevant sentences from the generated summaries of individual pre-trained
models and constructed the final summary. As indicated by the experimental
results, our MMR-based approach successfully ranked and selected the most
relevant sentences as summaries and showed better performance than the
individual pre-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert
  (MoE) Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S. Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, Benjamin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models have recently gained steam in achieving the
state-of-the-art performance in a wide range of tasks in computer vision and
natural language processing. They effectively expand the model capacity while
incurring a minimal increase in computation cost during training. However,
deploying such models for inference is difficult due to their large model size
and complex communication pattern. In this work, we provide a characterization
of two MoE workloads, namely Language Modeling (LM) and Machine Translation
(MT) and identify their sources of inefficiencies at deployment.
  We propose three optimization techniques to mitigate sources of
inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert
load balancing. We show that dynamic gating improves execution time by
1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT
Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to
1.1$\times$ for MT. We further propose Expert Buffering, a new caching
mechanism that only keeps hot, active experts in GPU memory while buffering the
rest in CPU memory. This reduces static memory allocation by 1.47$\times$. We
finally propose a load balancing methodology that provides additional
robustness to the workload. The code will be open-sourced upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Ghost in the Research Shell: Large Language Models and
  Academic <span class="highlight-title">Knowledge</span> Creation in Management Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper looks at the role of large language models in academic knowledge
creation based on a scoping review (2018 to January 2023) of how researchers
have previously used the language model GPT to assist in the performance of
academic knowledge creation tasks beyond data analysis. These tasks include
writing, editing, reviewing, dataset creation and curation, which have been
difficult to perform using earlier ML tools. Based on a synthesis of these
papers, this study identifies pathways for a future academic research landscape
that incorporates wider usage of large language models based on the current
modes of adoption in published articles as a Co-Writer, Research Assistant and
Respondent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arabic aspect sentiment polarity classification using <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.13290v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.13290v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed M. Abdelgwad, Taysir Hassan A Soliman, Ahmed I. Taloba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic aspect sentiment polarity classification task. In
particular, we develop a simple but effective BERT-based neural baseline to
handle this task. Our BERT architecture with a simple linear classification
layer surpassed the state-of-the-art works, according to the experimental
results on three different Arabic datasets. Achieving an accuracy of 89.51% on
the Arabic hotel reviews dataset, 73% on the Human annotated book reviews
dataset, and 85.73% on the Arabic news dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-3-driven pedagogical agents for training children's curious
  question-asking skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14228v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14228v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to train children's ability to ask curiosity-driven questions,
previous research has explored designing specific exercises relying on
providing semantic and linguistic cues to help formulate such questions. But
despite showing pedagogical efficiency, this method is still limited as it
relies on generating the said cues by hand, which can be a very costly process.
In this context, we propose to leverage advances in the natural language
processing field (NLP) and investigate the efficiency of using a large language
model (LLM) for automating the production of the pedagogical content of a
curious question-asking (QA) training. We study generating the said content
using the "prompt-based" method that consists of explaining the task to the LLM
in natural text. We evaluate the output using human experts annotations and
comparisons with hand-generated content. Results suggested indeed the relevance
and usefulness of this content. We also conduct a field study in primary school
(75 children aged 9-10), where we evaluate children's QA performance when
having this training. We compare 3 types of content : 1) hand-generated content
that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated
content that proposes the same type of cues; 3) GPT-3-generated content that
proposes "open" cues leading to several possible questions. We see a similar QA
performance between the two "closed" trainings (showing the scalability of the
approach using GPT-3), and a better one for participants with the "open"
training. These results suggest the efficiency of using LLMs to support
children in generating more curious questions, using a natural language
prompting approach that affords usability by teachers and other users not
specialists of AI techniques. Furthermore, results also show that open-ended
content may be more suitable for training curious question-asking skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Human-Level <span class="highlight-title">Prompt</span> Engineers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the "program," optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Kind Introduction to Lexical and Grammatical Aspect, with a <span class="highlight-title">Survey</span> of
  Computational Approaches <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annemarie Friedrich, Nianwen Xue, Alexis Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspectual meaning refers to how the internal temporal structure of situations
is presented. This includes whether a situation is described as a state or as
an event, whether the situation is finished or ongoing, and whether it is
viewed as a whole or with a focus on a particular phase. This survey gives an
overview of computational approaches to modeling lexical and grammatical aspect
along with intuitive explanations of the necessary linguistic concepts and
terminology. In particular, we describe the concepts of stativity, telicity,
habituality, perfective and imperfective, as well as influential inventories of
eventuality and situation types. We argue that because aspect is a crucial
component of semantics, especially when it comes to reporting the temporal
structure of situations in a precise way, future NLP approaches need to be able
to handle and evaluate it systematically in order to achieve human-level
language understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023, camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Adaptive Named Entity Recognition by Retrieving Unstructured
  <span class="highlight-title">Knowledge</span> <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Naoki Yoshinaga, Kyosuke Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although named entity recognition (NER) helps us to extract domain-specific
entities from text (e.g., artists in the music domain), it is costly to create
a large amount of training data or a structured knowledge base to perform
accurate NER in the target domain. Here, we propose self-adaptive NER, which
retrieves external knowledge from unstructured text to learn the usages of
entities that have not been learned well. To retrieve useful knowledge for NER,
we design an effective two-stage model that retrieves unstructured knowledge
using uncertain entities as queries. Our model predicts the entities in the
input and then finds those of which the prediction is not confident. Then, it
retrieves knowledge by using these uncertain entities as queries and
concatenates the retrieved text to the original input to revise the prediction.
Experiments on CrossNER datasets demonstrated that our model outperforms strong
baselines by 2.35 points in F1 metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL2023 (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case
  Study using Latent Dirichlet Allocation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03029v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03029v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernadeta Griciūtė, Lifeng Han, Hao Li, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data and source code are openly available at
https://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation
(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;
BERT-topic
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fillers in Spoken Language Understanding: Computational and
  Psycholinguistic Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanvi Dinkar, Chloé Clavel, Ioana Vasilescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disfluencies (i.e. interruptions in the regular flow of speech), are
ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that
occur the most frequently compared to other kinds of disfluencies. Yet, to the
best of our knowledge, there isn't a resource that brings together the research
perspectives influencing Spoken Language Understanding (SLU) on these speech
events. This aim of this article is to survey a breadth of perspectives in a
holistic way; i.e. from considering underlying (psycho)linguistic theory, to
their annotation and consideration in Automatic Speech Recognition (ASR) and
SLU systems, to lastly, their study from a generation standpoint. This article
aims to present the perspectives in an approachable way to the SLU and
Conversational AI community, and discuss moving forward, what we believe are
the trends and challenges in each area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in TAL Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word-Graph2vec: An efficient word embedding approach on word
  co-occurrence graph using random walk sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Li, Yuanzhe Cai, Jiahong Xue, Xi Zhang, Huacan Chen, Zeyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embedding has become ubiquitous and is widely used in various text
mining and natural language processing (NLP) tasks, such as information
retrieval, semantic analysis, and machine translation, among many others.
Unfortunately, it is prohibitively expensive to train the word embedding in a
relatively large corpus. We propose a graph-based word embedding algorithm,
called Word-Graph2vec, which converts the large corpus into a word
co-occurrence graph, then takes the word sequence samples from this graph by
randomly traveling and trains the word embedding on this sampling corpus in the
end. We posit that because of the stable vocabulary, relative idioms, and fixed
expressions in English, the size and density of the word co-occurrence graph
change slightly with the increase in the training corpus. So that
Word-Graph2vec has stable runtime on the large scale data set, and its
performance advantage becomes more and more obvious with the growth of the
training corpus. Extensive experiments conducted on real-world datasets show
that the proposed algorithm outperforms traditional Skip-Gram by four-five
times in terms of efficiency, while the error generated by the random walk
sampling is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span>-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19
  Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Hassannataj Joloudari, Sadiq Hussain, Mohammad Ali Nematollahi, Rouhollah Bagheri, Fatemeh Fazl, Roohallah Alizadehsani, Reza Lashgari, Ashis Talukder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach
  for Speech Emotion Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Ye, Xin-cheng Wen, Yujie Wei, Yong Xu, Kunhong Liu, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) plays a vital role in improving the
interactions between humans and machines by inferring human emotion and
affective states from speech signals. Whereas recent works primarily focus on
mining spatiotemporal information from hand-crafted features, we explore how to
model the temporal patterns of speech emotions from dynamic temporal scales.
Towards that goal, we introduce a novel temporal emotional modeling approach
for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),
which learns multi-scale contextual affective representations from various time
scales. Specifically, TIM-Net first employs temporal-aware blocks to learn
temporal affective representation, then integrates complementary information
from the past and the future to enrich contextual representations, and finally,
fuses multiple time scale features for better adaptation to the emotional
variation. Extensive experimental results on six benchmark SER datasets
demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%
improvements of the average UAR and WAR over the second-best on each corpus.
The source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoSyn: Detecting Implicit Hate Speech in Online <span class="highlight-title">Conversation</span>s Using a
  Context Synergized Hyperbolic Network <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tremendous growth of social media users interacting in online
conversations has also led to significant growth in hate speech. Most of the
prior works focus on detecting explicit hate speech, which is overt and
leverages hateful phrases, with very little work focusing on detecting hate
speech that is implicit or denotes hatred through indirect or coded language.
In this paper, we present CoSyn, a user- and conversational-context synergized
network for detecting implicit hate speech in online conversation trees. CoSyn
first models the user's personal historical and social context using a novel
hyperbolic Fourier attention mechanism and hyperbolic graph convolution
network. Next, we jointly model the user's personal context and the
conversational context using a novel context interaction mechanism in the
hyperbolic space that clearly captures the interplay between the two and makes
independent assessments on the amounts of information to be retrieved from both
contexts. CoSyn performs all operations in the hyperbolic space to account for
the scale-free dynamics of social media. We demonstrate the effectiveness of
CoSyn both qualitatively and quantitatively on an open-source hate speech
dataset with Twitter conversations and show that CoSyn outperforms all our
baselines in detecting implicit hate speech with absolute improvements in the
range of 8.15% - 19.50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Attention Networks Can Process Bounded Hierarchical Languages <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive performance in NLP, self-attention networks were
recently proved to be limited for processing formal languages with hierarchical
structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested
parentheses of $k$ types. This suggested that natural language can be
approximated well with models that are too weak for formal languages, or that
the role of hierarchy and recursion in natural language might be limited. We
qualify this implication by proving that self-attention networks can process
$\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by
$D$, which arguably better captures the bounded hierarchical structure of
natural language. Specifically, we construct a hard-attention network with
$D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes
$\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and
$O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show
that self-attention networks trained on $\mathsf{Dyck}_{k, D}$ generalize to
longer inputs with near-perfect accuracy, and also verify the theoretical
memory advantage of self-attention networks over recurrent networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2021. 19 pages with extended appendix. v2 fixed a small typo in
  the formula at the end of page 5 (thank to Gabriel Faria). Code:
  https://github.com/princeton-nlp/dyck-transformer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReAct: Synergizing <span class="highlight-title">Reasoning</span> and Acting in Language Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3 is the ICLR camera ready version with some typos fixed. Project
  site with code: https://react-lm.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-based News Narrative Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Keith Norambuena, Tanushree Mitra, Chris North
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are fundamental to our understanding of the world, providing us
with a natural structure for knowledge representation over time. Computational
narrative extraction is a subfield of artificial intelligence that makes heavy
use of information retrieval and natural language processing techniques.
Despite the importance of computational narrative extraction, relatively little
scholarly work exists on synthesizing previous research and strategizing future
research in the area. In particular, this article focuses on extracting news
narratives from an event-centric perspective. Extracting narratives from news
data has multiple applications in understanding the evolving information
landscape. This survey presents an extensive study of research in the area of
event-based news narrative extraction. In particular, we screened over 900
articles that yielded 54 relevant articles. These articles are synthesized and
organized by representation model, extraction criteria, and evaluation
approaches. Based on the reviewed studies, we identify recent trends, open
challenges, and potential research lines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures, to be published in the journal ACM CSUR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning the Legibility of Visual Text Perturbations <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dev Seth, Rickard Stureborg, Danish Pruthi, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many adversarial attacks in NLP perturb inputs to produce visually similar
strings ('ergo' $\rightarrow$ '$\epsilon$rgo') which are legible to humans but
degrade model performance. Although preserving legibility is a necessary
condition for text perturbation, little work has been done to systematically
characterize it; instead, legibility is typically loosely enforced via
intuitions around the nature and extent of perturbations. Particularly, it is
unclear to what extent can inputs be perturbed while preserving legibility, or
how to quantify the legibility of a perturbed string. In this work, we address
this gap by learning models that predict the legibility of a perturbed string,
and rank candidate perturbations based on their legibility. To do so, we
collect and release LEGIT, a human-annotated dataset comprising the legibility
of visually perturbed text. Using this dataset, we build both text- and
vision-based models which achieve up to $0.91$ F1 score in predicting whether
an input is legible, and an accuracy of $0.86$ in predicting which of two given
perturbations is more legible. Additionally, we discover that legible
perturbations from the LEGIT dataset are more effective at lowering the
performance of NLP models than best-known attack strategies, suggesting that
current models may be vulnerable to a broad range of perturbations beyond what
is captured by existing visual attacks. Data, code, and models are available at
https://github.com/dvsth/learning-legibility-2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures. Accepted at EACL 2023 (main, long)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiNet: A Novel Multi-Scenario & Multi-Task Learning Approach with
  Hierarchical Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhou, Xianshuai Cao, Wenhao Li, Kun Zhang, Chuan Luo, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-scenario & multi-task learning has been widely applied to many
recommendation systems in industrial applications, wherein an effective and
practical approach is to carry out multi-scenario transfer learning on the
basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based
method, which aims to project all information in the same feature space, cannot
effectively deal with the complex relationships inherent among various
scenarios and tasks, resulting in unsatisfactory performance. To tackle the
problem, we propose a Hierarchical information extraction Network (HiNet) for
multi-scenario and multi-task recommendation, which achieves hierarchical
extraction based on coarse-to-fine knowledge transfer scheme. The multiple
extraction layers of the hierarchical network enable the model to enhance the
capability of transferring valuable information across scenarios while
preserving specific features of scenarios and tasks. Furthermore, a novel
scenario-aware attentive network module is proposed to model correlations
between scenarios explicitly. Comprehensive experiments conducted on real-world
industrial datasets from Meituan Meishi platform demonstrate that HiNet
achieves a new state-of-the-art performance and significantly outperforms
existing solutions. HiNet is currently fully deployed in two scenarios and has
achieved 2.87% and 1.75% order quantity gain respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Coordination for Quantifying and Maximizing <span class="highlight-title">Knowledge</span>
  Transference in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua Yang, Jianxin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been widely applied in online advertising and
recommender systems. To address the negative transfer issue, recent studies
have proposed optimization methods that thoroughly focus on the gradient
alignment of directions or magnitudes. However, since prior study has proven
that both general and specific knowledge exist in the limited shared capacity,
overemphasizing on gradient alignment may crowd out task-specific knowledge,
and vice versa. In this paper, we propose a transference-driven approach CoGrad
that adaptively maximizes knowledge transference via Coordinated Gradient
modification. We explicitly quantify the transference as loss reduction from
one task to another, and then derive an auxiliary gradient from optimizing it.
We perform the optimization by incorporating this gradient into original task
gradients, making the model automatically maximize inter-task transfer and
minimize individual losses. Thus, CoGrad can harmonize between general and
specific knowledge to boost overall performance. Besides, we introduce an
efficient approximation of the Hessian matrix, making CoGrad computationally
efficient and simple to implement. Both offline and online experiments verify
that CoGrad significantly outperforms previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Adversarial Learning for Complementary Item
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koby Bibas, Oren Sar Shalom, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complementary item recommendations are a ubiquitous feature of modern
e-commerce sites. Such recommendations are highly effective when they are based
on collaborative signals like co-purchase statistics. In certain online
marketplaces, however, e.g., on online auction sites, constantly new items are
added to the catalog. In such cases, complementary item recommendations are
often based on item side-information due to a lack of interaction data. In this
work, we propose a novel approach that can leverage both item side-information
and labeled complementary item pairs to generate effective complementary
recommendations for cold items, i.e., for items for which no co-purchase
statistics yet exist. Given that complementary items typically have to be of a
different category than the seed item, we technically maintain a latent space
for each item category. Simultaneously, we learn to project distributed item
representations into these category spaces to determine suitable
recommendations. The main learning process in our architecture utilizes labeled
pairs of complementary items. In addition, we adopt ideas from Cycle Generative
Adversarial Networks (CycleGAN) to leverage available item information even in
case no labeled data exists for a given item and category. Experiments on three
e-commerce datasets show that our method is highly effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pacos: Modeling Users' Interpretable and Context-Dependent Choices in
  Preference Reversals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choice problems refer to selecting the best choices from several items, and
learning users' preferences in choice problems is of great significance in
understanding the decision making mechanisms and providing personalized
services. Existing works typically assume that people evaluate items
independently. In practice, however, users' preferences depend on the market in
which items are placed, which is known as context effects; and the order of
users' preferences for two items may even be reversed, which is referred to
preference reversals. In this work, we identify three factors contributing to
context effects: users' adaptive weights, the inter-item comparison, and
display positions. We propose a context-dependent preference model named Pacos
as a unified framework for addressing three factors simultaneously, and
consider two design methods including an additive method with high
interpretability and an ANN-based method with high accuracy. We study the
conditions for preference reversals to occur and provide an theoretical proof
of the effectiveness of Pacos in addressing preference reversals. Experimental
results show that the proposed method has better performance than prior works
in predicting users' choices, and has great interpretability to help understand
the cause of preference reversals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Task Recommendations with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multi-task Learning (MTL) has yielded immense success in
Recommender System (RS) applications. However, current MTL-based recommendation
models tend to disregard the session-wise patterns of user-item interactions
because they are predominantly constructed based on item-wise datasets.
Moreover, balancing multiple objectives has always been a challenge in this
field, which is typically avoided via linear estimations in existing works. To
address these issues, in this paper, we propose a Reinforcement Learning (RL)
enhanced MTL framework, namely RMTL, to combine the losses of different
recommendation tasks using dynamic weights. To be specific, the RMTL structure
can address the two aforementioned issues by (i) constructing an MTL
environment from session-wise interactions and (ii) training multi-task
actor-critic network structure, which is compatible with most existing
MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL
loss function using the weights generated by critic networks. Experiments on
two real-world public datasets demonstrate the effectiveness of RMTL with a
higher AUC against state-of-the-art MTL-based recommendation models.
Additionally, we evaluate and validate RMTL's compatibility and transferability
across various MTL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achievable Rates and Low-Complexity Encoding of Posterior Matching for
  the BSC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaael Antonini, Rita Gimelshein, Richard Wesel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Horstein, Burnashev, Shayevitz and Feder, Naghshvar et al. and others have
studied sequential transmission of a K-bit message over the binary symmetric
channel (BSC) with full, noiseless feedback using posterior matching. Yang et
al. provide an improved lower bound on the achievable rate using martingale
analysis that relies on the small-enough difference (SED) partitioning
introduced by Naghshvar et al. SED requires a relatively complex encoder and
decoder. To reduce complexity, this paper replaces SED with relaxed constraints
that admit the small enough absolute difference (SEAD) partitioning rule. The
main analytical results show that achievable-rate bounds higher than those
found by Yang et al. are possible even under the new constraints, which are
less restrictive than SED. The new analysis does not use martingale theory for
the confirmation phase and applies a surrogate channel technique to tighten the
results. An initial systematic transmission further increases the achievable
rate bound. The simplified encoder associated with SEAD has a complexity below
order O(K^2) and allows simulations for message sizes of at least 1000 bits.
For example, simulations achieve 99% of of the channel's 0.50-bit capacity with
an average block size of 200 bits for a target codeword error rate of 10^(-3).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper consists of 26 pages and contains 6 figures. An earlier
  version of the algorithm included in this paper was published at the 2020
  IEEE International Symposium on Information Theory (ISIT), (DOI:
  10.1109/ISIT44484.2020.9174232)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-based News Narrative Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Keith Norambuena, Tanushree Mitra, Chris North
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are fundamental to our understanding of the world, providing us
with a natural structure for knowledge representation over time. Computational
narrative extraction is a subfield of artificial intelligence that makes heavy
use of information retrieval and natural language processing techniques.
Despite the importance of computational narrative extraction, relatively little
scholarly work exists on synthesizing previous research and strategizing future
research in the area. In particular, this article focuses on extracting news
narratives from an event-centric perspective. Extracting narratives from news
data has multiple applications in understanding the evolving information
landscape. This survey presents an extensive study of research in the area of
event-based news narrative extraction. In particular, we screened over 900
articles that yielded 54 relevant articles. These articles are synthesized and
organized by representation model, extraction criteria, and evaluation
approaches. Based on the reviewed studies, we identify recent trends, open
challenges, and potential research lines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures, to be published in the journal ACM CSUR</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exphormer: Sparse <span class="highlight-title">Transformer</span>s for Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J. Sutherland, Ali Kemal Sinop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph transformers have emerged as a promising architecture for a variety of
graph learning and representation tasks. Despite their successes, though, it
remains challenging to scale graph transformers to large graphs while
maintaining accuracy competitive with message-passing networks. In this paper,
we introduce Exphormer, a framework for building powerful and scalable graph
transformers. Exphormer consists of a sparse attention mechanism based on two
mechanisms: virtual global nodes and expander graphs, whose mathematical
characteristics, such as spectral expansion, pseduorandomness, and sparsity,
yield graph transformers with complexity only linear in the size of the graph,
while allowing us to prove desirable theoretical properties of the resulting
transformer models. We show that incorporating \textsc{Exphormer} into the
recently-proposed GraphGPS framework produces models with competitive empirical
results on a wide variety of graph datasets, including state-of-the-art results
on three datasets. We also show that \textsc{Exphormer} can scale to datasets
on larger graphs than shown in previous graph transformer architectures. Code
can be found at https://github.com/hamed1375/Exphormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in face manipulation using StyleGAN have produced impressive
results. However, StyleGAN is inherently limited to cropped aligned faces at a
fixed image resolution it is pre-trained on. In this paper, we propose a simple
and effective solution to this limitation by using dilated convolutions to
rescale the receptive fields of shallow layers in StyleGAN, without altering
any model parameters. This allows fixed-size small features at shallow layers
to be extended into larger ones that can accommodate variable resolutions,
making them more robust in characterizing unaligned faces. To enable real face
inversion and manipulation, we introduce a corresponding encoder that provides
the first-layer feature of the extended StyleGAN in addition to the latent
style code. We validate the effectiveness of our method using unaligned face
inputs of various resolutions in a diverse set of face manipulation tasks,
including facial attribute editing, super-resolution, sketch/mask-to-face
translation, and face toonification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/williamyang1991/StyleGANEX Project page:
  https://www.mmlab-ntu.com/project/styleganex/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Hands Make Light Work: Enhancing Quality and Diversity using
  MAP-Elites with Multiple Parallel Evolution Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manon Flageat, Bryan Lim, Antoine Cully
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of hardware accelerators and their corresponding tools,
evaluations have become more affordable through fast and massively parallel
evaluations in some applications. This advancement has drastically sped up the
runtime of evolution-inspired algorithms such as Quality-Diversity
optimization, creating tremendous potential for algorithmic innovation through
scale. In this work, we propose MAP-Elites-Multi-ES (MEMES), a novel QD
algorithm based on Evolution Strategies (ES) designed for fast parallel
evaluations. ME-Multi-ES builds on top of the existing MAP-Elites-ES algorithm,
scaling it by maintaining multiple independent ES threads with massive
parallelization. We also introduce a new dynamic reset procedure for the
lifespan of the independent ES to autonomously maximize the improvement of the
QD population. We show experimentally that MEMES outperforms existing
gradient-based and objective-agnostic QD algorithms when compared in terms of
generations. We perform this comparison on both black-box optimization and
QD-Reinforcement Learning tasks, demonstrating the benefit of our approach
across different problems and domains. Finally, we also find that our approach
intrinsically enables optimization of fitness locally around a niche, a
phenomenon not observed in other QD algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rewarding Chatbots for Real-World Engagement with Millions of Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, William Beauchamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of pretrained large language models has led to the deployment
of a range of social chatbots for chitchat. Although these chatbots demonstrate
language ability and fluency, they are not guaranteed to be engaging and can
struggle to retain users. This work investigates the development of social
chatbots that prioritize user engagement to enhance retention, specifically
examining the use of human feedback to efficiently develop highly engaging
chatbots. The proposed approach uses automatic pseudo-labels collected from
user interactions to train a reward model that can be used to reject
low-scoring sample responses generated by the chatbot model at inference time.
Intuitive evaluation metrics, such as mean conversation length (MCL), are
introduced as proxies to measure the level of engagement of deployed chatbots.
A/B testing on groups of 10,000 new daily chatbot users on the Chai Research
platform shows that this approach increases the MCL by up to 70%, which
translates to a more than 30% increase in user retention for a GPT-J 6B model.
Future work aims to use the reward model to realise a data fly-wheel, where the
latest user conversations can be used to alternately fine-tune the language
model and the reward model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ignorance is Bliss: Robust Control via Information Gating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manan Tomar, Riashat Islam, Sergey Levine, Philip Bachman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Informational parsimony -- i.e., using the minimal information required for a
task, -- provides a useful inductive bias for learning representations that
achieve better generalization by being robust to noise and spurious
correlations. We propose information gating in the pixel space as a way to
learn more parsimonious representations. Information gating works by learning
masks that capture only the minimal information required to solve a given task.
Intuitively, our models learn to identify which visual cues actually matter for
a given task. We gate information using a differentiable parameterization of
the signal-to-noise ratio, which can be applied to arbitrary values in a
network, e.g.~masking out pixels at the input layer. We apply our approach,
which we call InfoGating, to various objectives such as: multi-step forward and
inverse dynamics, Q-learning, behavior cloning, and standard self-supervised
tasks. Our experiments show that learning to identify and use minimal
information can improve generalization in downstream tasks -- e.g., policies
based on info-gated images are considerably more robust to
distracting/irrelevant visual features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Fusion Strategies for Federated Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Kayaalp, Yunus Inan, Visa Koivunen, Emre Telatar, Ali H. Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of information aggregation in federated decision
making, where a group of agents collaborate to infer the underlying state of
nature without sharing their private data with the central processor or each
other. We analyze the non-Bayesian social learning strategy in which agents
incorporate their individual observations into their opinions (i.e.,
soft-decisions) with Bayes rule, and the central processor aggregates these
opinions by arithmetic or geometric averaging. Building on our previous work,
we establish that both pooling strategies result in asymptotic normality
characterization of the system, which, for instance, can be utilized in order
to give approximate expressions for the error probability. We verify the
theoretical findings with simulations and compare both strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-tailed Classification from a Bayesian-decision-theory Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolian Li, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed classification poses a challenge due to its heavy imbalance in
class probabilities and tail-sensitivity risks with asymmetric misprediction
costs. Recent attempts have used re-balancing loss and ensemble methods, but
they are largely heuristic and depend heavily on empirical results, lacking
theoretical explanation. Furthermore, existing methods overlook the decision
loss, which characterizes different costs associated with tailed classes. This
paper presents a general and principled framework from a
Bayesian-decision-theory perspective, which unifies existing techniques
including re-balancing and ensemble methods, and provides theoretical
justifications for their effectiveness. From this perspective, we derive a
novel objective based on the integrated risk and a Bayesian deep-ensemble
approach to improve the accuracy of all classes, especially the ``tail".
Besides, our framework allows for task-adaptive decision loss which provides
provably optimal decisions in varying task scenarios, along with the capability
to quantify uncertainty. Finally, We conduct comprehensive experiments,
including standard classification, tail-sensitive classification with a new
False Head Rate metric, calibration, and ablation studies. Our framework
significantly improves the current SOTA even on large-scale real-world datasets
like ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Events and Interactions through Temporal Processes -- A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelica Liguori, Luciano Caroprese, Marco Minici, Bruno Veloso, Francesco Spinnato, Mirco Nanni, Giuseppe Manco, Joao Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenario, many phenomena produce a collection of events that
occur in continuous time. Point Processes provide a natural mathematical
framework for modeling these sequences of events. In this survey, we
investigate probabilistic models for modeling event sequences through temporal
processes. We revise the notion of event modeling and provide the mathematical
foundations that characterize the literature on the topic. We define an
ontology to categorize the existing approaches in terms of three families:
simple, marked, and spatio-temporal point processes. For each family, we
systematically review the existing approaches based based on deep learning.
Finally, we analyze the scenarios where the proposed techniques can be used for
addressing prediction and modeling aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Recipe for the Analysis of Randomized Multi-Armed Bandit
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorian Baudry, Kazuya Suzuki, Junya Honda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a general methodology to derive regret bounds for
randomized multi-armed bandit algorithms. It consists in checking a set of
sufficient conditions on the sampling probability of each arm and on the family
of distributions to prove a logarithmic regret. As a direct application we
revisit two famous bandit algorithms, Minimum Empirical Divergence (MED) and
Thompson Sampling (TS), under various models for the distributions including
single parameter exponential families, Gaussian distributions, bounded
distributions, or distributions satisfying some conditions on their moments. In
particular, we prove that MED is asymptotically optimal for all these models,
but also provide a simple regret analysis of some TS algorithms for which the
optimality is already known. We then further illustrate the interest of our
approach, by analyzing a new Non-Parametric TS algorithm (h-NPTS), adapted to
some families of unbounded reward distributions with a bounded h-moment. This
model can for instance capture some non-parametric families of distributions
whose variance is upper bounded by a known constant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSMixer: An all-MLP Architecture for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world time-series datasets are often multivariate with complex dynamics.
Commonly-used high capacity architectures like recurrent- or attention-based
sequential models have become popular. However, recent work demonstrates that
simple univariate linear models can outperform those deep alternatives. In this
paper, we investigate the capabilities of linear models for time-series
forecasting and present Time-Series Mixer (TSMixer), an architecture designed
by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing
operations along time and feature dimensions to extract information
efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is
comparable to specialized state-of-the-art models that leverage the inductive
biases of specific benchmarks. On the challenging and large scale M5 benchmark,
a real-world retail dataset, TSMixer demonstrates superior performance compared
to the state-of-the-art alternatives. Our results underline the importance of
efficiently utilizing cross-variate and auxiliary information for improving the
performance of time series forecasting. The design paradigms utilized in
TSMixer are expected to open new horizons for deep learning-based time series
forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tactile-Filter: Interactive Tactile Perception for Part Mating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kei Ota, Devesh K. Jha, Hsiao-Yu Tung, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans rely on touch and tactile sensing for a lot of dexterous manipulation
tasks. Our tactile sensing provides us with a lot of information regarding
contact formations as well as geometric information about objects during any
interaction. With this motivation, vision-based tactile sensors are being
widely used for various robotic perception and control tasks. In this paper, we
present a method for interactive perception using vision-based tactile sensors
for multi-object assembly. In particular, we are interested in tactile
perception during part mating, where a robot can use tactile sensors and a
feedback mechanism using particle filter to incrementally improve its estimate
of objects that fit together for assembly. To do this, we first train a deep
neural network that makes use of tactile images to predict the probabilistic
correspondence between arbitrarily shaped objects that fit together. The
trained model is used to design a particle filter which is used twofold. First,
given one partial (or non-unique) observation of the hole, it incrementally
improves the estimate of the correct peg by sampling more tactile observations.
Second, it selects the next action for the robot to sample the next touch (and
thus image) which results in maximum uncertainty reduction to minimize the
number of interactions during the perception task. We evaluate our method on
several part-mating tasks for assembly using a robot equipped with a
vision-based tactile sensor. We also show the efficiency of the proposed action
selection method against a naive method. See supplementary video at
https://www.youtube.com/watch?v=jMVBg_e3gLw .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A hybrid deep-learning-metaheuristic framework to approximate discrete
  road network design problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahman Madadi, Goncalo Homem de Almeida Correia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a hybrid deep-learning-metaheuristic framework with a
bi-level architecture to solve road network design problems (NDPs). We train a
graph neural network (GNN) to approximate the solution of the user equilibrium
(UE) traffic assignment problem, and use inferences made by the trained model
to calculate fitness function evaluations of a genetic algorithm (GA) to
approximate solutions for NDPs. Using two NDP variants and an exact solver as
benchmark, we show that our proposed framework can provide solutions within 5%
gap of the global optimum results given less than 1% of the time required for
finding the optimal results. Moreover, we observe many interesting future
directions, thus we propose a brief research agenda for this topic. The key
observation inspiring influential future research was that fitness function
evaluation time using the inferences made by the GNN model for the genetic
algorithm was in the order of milliseconds, which points to an opportunity and
a need for novel heuristics that 1) can cope well with noisy fitness function
values provided by neural networks, and 2) can use the significantly higher
computation time provided to them to explore the search space effectively
(rather than efficiently). This opens a new avenue for a modern class of
metaheuristics that are crafted for use with AI-powered predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning for sports betting: should forecasting models be
  optimised for accuracy or calibration? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conor Walsh, Alok Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sports betting's recent federal legalisation in the USA coincides with the
golden age of machine learning. If bettors can leverage data to accurately
predict the probability of an outcome, they can recognise when the bookmaker's
odds are in their favour. As sports betting is a multi-billion dollar industry
in the USA alone, identifying such opportunities could be extremely lucrative.
Many researchers have applied machine learning to the sports outcome prediction
problem, generally using accuracy to evaluate the performance of forecasting
models. We hypothesise that for the sports betting problem, model calibration
is more important than accuracy. To test this hypothesis, we train models on
NBA data over several seasons and run betting experiments on a single season,
using published odds. Evaluating various betting systems, we show that
optimising the forecasting model for calibration leads to greater returns than
optimising for accuracy, on average (return on investment of $110.42\%$ versus
$2.98\%$) and in the best case ($902.01\%$ versus $222.84\%$). These findings
suggest that for sports betting (or any forecasting problem where decisions are
made based on the predicted probability of each outcome), calibration is a more
important metric than accuracy. Sports bettors who wish to increase profits
should therefore optimise their forecasting model for calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages (including bibliography). 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Solar Irradiance without Direct Observation: An Empirical
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Cargan, Dario Landa-Silva, Isaac Triguero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the use of solar power increases, having accurate and timely forecasters
will be essential for smooth grid operators. There are many proposed methods
for forecasting solar irradiance / solar power production. However, many of
these methods formulate the problem as a time-series, relying on near real-time
access to observations at the location of interest to generate forecasts. This
requires both access to a real-time stream of data and enough historical
observations for these methods to be deployed. In this paper, we conduct a
thorough analysis of effective ways to formulate the forecasting problem
comparing classical machine learning approaches to state-of-the-art deep
learning. Using data from 20 locations distributed throughout the UK and
commercially available weather data, we show that it is possible to build
systems that do not require access to this data. Leveraging weather
observations and measurements from other locations we show it is possible to
create models capable of accurately forecasting solar irradiance at new
locations. We utilise compare both satellite and ground observations (e.g.
temperature, pressure) of weather data. This could facilitate use planning and
optimisation for both newly deployed solar farms and domestic installations
from the moment they come online. Additionally, we show that training a single
global model for multiple locations can produce a more robust model with more
consistent and accurate results across locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Gromov-Wasserstein Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Nekrashevich, Alexander Korotin, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a scalable neural method to solve the Gromov-Wasserstein (GW)
Optimal Transport (OT) problem with the inner product cost. In this problem,
given two distributions supported on (possibly different) spaces, one has to
find the most isometric map between them. Our proposed approach uses neural
networks and stochastic mini-batch optimization which allows to overcome the
limitations of existing GW methods such as their poor scalability with the
number of samples and the lack of out-of-sample estimation. To demonstrate the
effectiveness of our proposed method, we conduct experiments on the synthetic
data and explore the practical applicability of our method to the popular task
of the unsupervised alignment of word embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying the evolution of COVID-19 severity on patients with combined
  dynamic Bayesian networks and neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Quesada, Pedro Larrañaga, Concha Bielza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When we face patients arriving to a hospital suffering from the effects of
some illness, one of the main problems we can encounter is evaluating whether
or not said patients are going to require intensive care in the near future.
This intensive care requires allotting valuable and scarce resources, and
knowing beforehand the severity of a patients illness can improve both its
treatment and the organization of resources. We illustrate this issue in a
dataset consistent of Spanish COVID-19 patients from the sixth epidemic wave
where we label patients as critical when they either had to enter the intensive
care unit or passed away. We then combine the use of dynamic Bayesian networks,
to forecast the vital signs and the blood analysis results of patients over the
next 40 hours, and neural networks, to evaluate the severity of a patients
disease in that interval of time. Our empirical results show that the
transposition of the current state of a patient to future values with the DBN
for its subsequent use in classification obtains better the accuracy and g-mean
score than a direct application with a classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Constructing Latent Modality Structures in Multi-modal
  Representation Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, Trishul Chilimbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive loss has been increasingly used in learning representations from
multiple modalities. In the limit, the nature of the contrastive loss
encourages modalities to exactly match each other in the latent space. Yet it
remains an open question how the modality alignment affects the downstream task
performance. In this paper, based on an information-theoretic argument, we
first prove that exact modality alignment is sub-optimal in general for
downstream prediction tasks. Hence we advocate that the key of better
performance lies in meaningful latent modality structures instead of perfect
modality alignment. To this end, we propose three general approaches to
construct latent modality structures. Specifically, we design 1) a deep feature
separation loss for intra-modality regularization; 2) a Brownian-bridge loss
for inter-modality regularization; and 3) a geometric consistency loss for both
intra- and inter-modality regularization. Extensive experiments are conducted
on two popular multi-modal representation learning frameworks: the CLIP-based
two-tower model and the ALBEF-based fusion model. We test our model on a
variety of tasks including zero/few-shot image classification, image-text
retrieval, visual question answering, visual reasoning, and visual entailment.
Our method achieves consistent improvements over existing methods,
demonstrating the effectiveness and generalizability of our proposed approach
on latent modality structure regularization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figure, CVPR 2023 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automotive Perception Software Development: An Empirical Investigation
  into Data, Annotation, and Ecosystem Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans-Martin Heyn, Khan Mohammad Habibullah, Eric Knauss, Jennifer Horkoff, Markus Borg, Alessia Knauss, Polly Jing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software that contains machine learning algorithms is an integral part of
automotive perception, for example, in driving automation systems. The
development of such software, specifically the training and validation of the
machine learning components, require large annotated datasets. An industry of
data and annotation services has emerged to serve the development of such
data-intensive automotive software components. Wide-spread difficulties to
specify data and annotation needs challenge collaborations between OEMs
(Original Equipment Manufacturers) and their suppliers of software components,
data, and annotations. This paper investigates the reasons for these
difficulties for practitioners in the Swedish automotive industry to arrive at
clear specifications for data and annotations. The results from an interview
study show that a lack of effective metrics for data quality aspects,
ambiguities in the way of working, unclear definitions of annotation quality,
and deficits in the business ecosystems are causes for the difficulty in
deriving the specifications. We provide a list of recommendations that can
mitigate challenges when deriving specifications and we propose future research
opportunities to overcome these challenges. Our work contributes towards the
on-going research on accountability of machine learning as applied to complex
software systems, especially for high-stake applications such as automated
driving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating friction coefficient using generative modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Otoofi, William J. B. Midgley, Leo Laine, Henderson Leon, Laura Justham, James Fleming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is common to utilise dynamic models to measure the tyre-road friction in
real-time. Alternatively, predictive approaches estimate the tyre-road friction
by identifying the environmental factors affecting it. This work aims to
formulate the problem of friction estimation as a visual perceptual learning
task. The problem is broken down into detecting surface characteristics by
applying semantic segmentation and using the extracted features to predict the
frictional force. This work for the first time formulates the friction
estimation problem as a regression from the latent space of a semantic
segmentation model. The preliminary results indicate that this approach can
estimate frictional force.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ICM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong Machine Learning Potentials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Eckhoff, Markus Reiher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning potentials (MLPs) trained on accurate quantum chemical data
can retain the high accuracy, while inflicting little computational demands. On
the downside, they need to be trained for each individual system. In recent
years, a vast number of MLPs has been trained from scratch because learning
additional data typically requires to train again on all data to not forget
previously acquired knowledge. Additionally, most common structural descriptors
of MLPs cannot represent efficiently a large number of different chemical
elements. In this work, we tackle these problems by introducing
element-embracing atom-centered symmetry functions (eeACSFs) which combine
structural properties and element information from the periodic table. These
eeACSFs are a key for our development of a lifelong machine learning potential
(lMLP). Uncertainty quantification can be exploited to transgress a fixed,
pre-trained MLP to arrive at a continuously adapting lMLP, because a predefined
level of accuracy can be ensured. To extend the applicability of an lMLP to new
systems, we apply continual learning strategies to enable autonomous and
on-the-fly training on a continuous stream of new data. For the training of
deep neural networks, we propose the continual resilient (CoRe) optimizer and
incremental learning strategies relying on rehearsal of data, regularization of
parameters, and the architecture of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Product Jacobi-Theta Boltzmann machines with score matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Pasquale, Daniel Krefl, Stefano Carrazza, Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of probability density functions is a non trivial task that
over the last years has been tackled with machine learning techniques.
Successful applications can be obtained using models inspired by the Boltzmann
machine (BM) architecture. In this manuscript, the product Jacobi-Theta
Boltzmann machine (pJTBM) is introduced as a restricted version of the
Riemann-Theta Boltzmann machine (RTBM) with diagonal hidden sector connection
matrix. We show that score matching, based on the Fisher divergence, can be
used to fit probability densities with the pJTBM more efficiently than with the
original RTBM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, ACAT22 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Anomaly Detection on Tennessee Eastman Process Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Hartung, Billy Joe Franks, Tobias Michels, Dennis Wagner, Philipp Liznerski, Steffen Reithermann, Sophie Fellenz, Fabian Jirasek, Maja Rudolph, Daniel Neider, Heike Leitte, Chen Song, Benjamin Kloepper, Stephan Mandt, Michael Bortz, Jakob Burger, Hans Hasse, Marius Kloft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides the first comprehensive evaluation and analysis of modern
(deep-learning) unsupervised anomaly detection methods for chemical process
data. We focus on the Tennessee Eastman process dataset, which has been a
standard litmus test to benchmark anomaly detection methods for nearly three
decades. Our extensive study will facilitate choosing appropriate anomaly
detection methods in industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution Preserving Source Separation With Time Frequency Predictive
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Villasana T., Janusz Klejsa, Lars Villemoes, Per Hedelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide an example of a distribution preserving source separation method,
which aims at addressing perceptual shortcomings of state-of-the-art methods.
Our approach uses unconditioned generative models of signal sources.
Reconstruction is achieved by means of mix-consistent sampling from a
distribution conditioned on a realization of a mix. The separated signals
follow their respective source distributions, which provides an advantage when
separation results are evaluated in a listening test.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, pre-review version submitted to EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulation-based Bayesian inference for robotic grasping <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Norman Marlier, Olivier Brüls, Gilles Louppe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General robotic grippers are challenging to control because of their rich
nonsmooth contact dynamics and the many sources of uncertainties due to the
environment or sensor noise. In this work, we demonstrate how to compute 6-DoF
grasp poses using simulation-based Bayesian inference through the full
stochastic forward simulation of the robot in its environment while robustly
accounting for many of the uncertainties in the system. A Riemannian manifold
optimization procedure preserving the nonlinearity of the rotation space is
used to compute the maximum a posteriori grasp pose. Simulation and physical
benchmarks show the promising high success rate of the approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, IROS 2022 Probabilistic Robotics at the age of
  Deep Learning workshop. arXiv admin note: substantial text overlap with
  arXiv:2109.14275</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate Real-time Polyp Detection in Videos from Concatenation of
  Latent Features Extracted from Consecutive Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemin Ali Qadir, Younghak Shin, Jacob Bergsland, Ilangko Balasingham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An efficient deep learning model that can be implemented in real-time for
polyp detection is crucial to reducing polyp miss-rate during screening
procedures. Convolutional neural networks (CNNs) are vulnerable to small
changes in the input image. A CNN-based model may miss the same polyp appearing
in a series of consecutive frames and produce unsubtle detection output due to
changes in camera pose, lighting condition, light reflection, etc. In this
study, we attempt to tackle this problem by integrating temporal information
among neighboring frames. We propose an efficient feature concatenation method
for a CNN-based encoder-decoder model without adding complexity to the model.
The proposed method incorporates extracted feature maps of previous frames to
detect polyps in the current frame. The experimental results demonstrate that
the proposed method of feature concatenation improves the overall performance
of automatic polyp detection in videos. The following results are obtained on a
public video dataset: sensitivity 90.94\%, precision 90.53\%, and specificity
92.46%
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Quantum Neural Networks (VQNNS) in Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meghashrita Das, Tirupati Bolisetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning has established as an interdisciplinary field to
overcome limitations of classical machine learning and neural networks. This is
a field of research which can prove that quantum computers are able to solve
problems with complex correlations between inputs that can be hard for
classical computers. This suggests that learning models made on quantum
computers may be more powerful for applications, potentially faster computation
and better generalization on less data. The objective of this paper is to
investigate how training of quantum neural network (QNNs) can be done using
quantum optimization algorithms for improving the performance and time
complexity of QNNs. A classical neural network can be partially quantized to
create a hybrid quantum-classical neural network which is used mainly in
classification and image recognition. In this paper, a QNN structure is made
where a variational parameterized circuit is incorporated as an input layer
named as Variational Quantum Neural Network (VQNNs). We encode the cost
function of QNNs onto relative phases of a superposition state in the Hilbert
space of the network parameters. The parameters are tuned with an iterative
quantum approximate optimisation (QAOA) mixer and problem hamiltonians. VQNNs
is experimented with MNIST digit recognition (less complex) and crack image
classification datasets (more complex) which converges the computation in
lesser time than QNN with decent training accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision-Making Under Uncertainty: Beyond Probabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thom Badings, Thiago D. Simão, Marnix Suilen, Nils Jansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper reflects on the state-of-the-art in decision-making under
uncertainty. A classical assumption is that probabilities can sufficiently
capture all uncertainty in a system. In this paper, the focus is on the
uncertainty that goes beyond this classical interpretation, particularly by
employing a clear distinction between aleatoric and epistemic uncertainty. The
paper features an overview of Markov decision processes (MDPs) and extensions
to account for partial observability and adversarial behavior. These models
sufficiently capture aleatoric uncertainty but fail to account for epistemic
uncertainty robustly. Consequently, we present a thorough overview of so-called
uncertainty models that exhibit uncertainty in a more robust interpretation. We
show several solution techniques for both discrete and continuous models,
ranging from formal verification, over control-based abstractions, to
reinforcement learning. As an integral part of this paper, we list and discuss
several key challenges that arise when dealing with rich types of uncertainty
in a model-based fashion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Coordination for Quantifying and Maximizing <span class="highlight-title">Knowledge</span>
  Transference in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua Yang, Jianxin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been widely applied in online advertising and
recommender systems. To address the negative transfer issue, recent studies
have proposed optimization methods that thoroughly focus on the gradient
alignment of directions or magnitudes. However, since prior study has proven
that both general and specific knowledge exist in the limited shared capacity,
overemphasizing on gradient alignment may crowd out task-specific knowledge,
and vice versa. In this paper, we propose a transference-driven approach CoGrad
that adaptively maximizes knowledge transference via Coordinated Gradient
modification. We explicitly quantify the transference as loss reduction from
one task to another, and then derive an auxiliary gradient from optimizing it.
We perform the optimization by incorporating this gradient into original task
gradients, making the model automatically maximize inter-task transfer and
minimize individual losses. Thus, CoGrad can harmonize between general and
specific knowledge to boost overall performance. Besides, we introduce an
efficient approximation of the Hessian matrix, making CoGrad computationally
efficient and simple to implement. Both offline and online experiments verify
that CoGrad significantly outperforms previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Language-Image <span class="highlight-title">Pretrain</span>ed (CLIP) Models are Powerful
  Out-of-Distribution Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Michels, Nikolas Adaloglou, Tim Kaiser, Markus Kollmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive experimental study on pretrained feature
extractors for visual out-of-distribution (OOD) detection. We examine several
setups, based on the availability of labels or image captions and using
different combinations of in- and out-distributions. Intriguingly, we find that
(i) contrastive language-image pretrained models achieve state-of-the-art
unsupervised out-of-distribution performance using nearest neighbors feature
similarity as the OOD detection score, (ii) supervised state-of-the-art OOD
detection performance can be obtained without in-distribution fine-tuning,
(iii) even top-performing billion-scale vision transformers trained with
natural language supervision fail at detecting adversarially manipulated OOD
images. Finally, we argue whether new benchmarks for visual anomaly detection
are needed based on our experiments. Using the largest publicly available
vision transformer, we achieve state-of-the-art performance across all $18$
reported OOD benchmarks, including an AUROC of 87.6\% (9.2\% gain,
unsupervised) and 97.4\% (1.2\% gain, supervised) for the challenging task of
CIFAR100 $\rightarrow$ CIFAR10 OOD detection. The code will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Adversarial Learning for Complementary Item
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koby Bibas, Oren Sar Shalom, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complementary item recommendations are a ubiquitous feature of modern
e-commerce sites. Such recommendations are highly effective when they are based
on collaborative signals like co-purchase statistics. In certain online
marketplaces, however, e.g., on online auction sites, constantly new items are
added to the catalog. In such cases, complementary item recommendations are
often based on item side-information due to a lack of interaction data. In this
work, we propose a novel approach that can leverage both item side-information
and labeled complementary item pairs to generate effective complementary
recommendations for cold items, i.e., for items for which no co-purchase
statistics yet exist. Given that complementary items typically have to be of a
different category than the seed item, we technically maintain a latent space
for each item category. Simultaneously, we learn to project distributed item
representations into these category spaces to determine suitable
recommendations. The main learning process in our architecture utilizes labeled
pairs of complementary items. In addition, we adopt ideas from Cycle Generative
Adversarial Networks (CycleGAN) to leverage available item information even in
case no labeled data exists for a given item and category. Experiments on three
e-commerce datasets show that our method is highly effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributionally Robust Optimization with Probabilistic Group <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Suvra Ghosal, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning models may be susceptible to learning spurious
correlations that hold on average but not for the atypical group of samples. To
address the problem, previous approaches minimize the empirical worst-group
risk. Despite the promise, they often assume that each sample belongs to one
and only one group, which does not allow expressing the uncertainty in group
labeling. In this paper, we propose a novel framework PG-DRO, which explores
the idea of probabilistic group membership for distributionally robust
optimization. Key to our framework, we consider soft group membership instead
of hard group annotations. The group probabilities can be flexibly generated
using either supervised learning or zero-shot approaches. Our framework
accommodates samples with group membership ambiguity, offering stronger
flexibility and generality than the prior art. We comprehensively evaluate
PG-DRO on both image classification and natural language processing benchmarks,
establishing superior performance
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Bonet, Benoît Malézieux, Alain Rakotomamonjy, Lucas Drumetz, Thomas Moreau, Matthieu Kowalski, Nicolas Courty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When dealing with electro or magnetoencephalography records, many supervised
prediction tasks are solved by working with covariance matrices to summarize
the signals. Learning with these matrices requires using Riemanian geometry to
account for their structure. In this paper, we propose a new method to deal
with distributions of covariance matrices and demonstrate its computational
efficiency on M/EEG multivariate time series. More specifically, we define a
Sliced-Wasserstein distance between measures of symmetric positive definite
matrices that comes with strong theoretical guarantees. Then, we take advantage
of its properties and kernel methods to apply this distance to brain-age
prediction from MEG data and compare it to state-of-the-art algorithms based on
Riemannian geometry. Finally, we show that it is an efficient surrogate to the
Wasserstein distance in domain adaptation for Brain Computer Interface
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training, Architecture, and Prior for Deterministic Uncertainty Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bertrand Charpentier, Chenxiang Zhang, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient uncertainty estimation is crucial to build reliable
Machine Learning (ML) models capable to provide calibrated uncertainty
estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this
end, Deterministic Uncertainty Methods (DUMs) is a promising model family
capable to perform uncertainty estimation in a single forward pass. This work
investigates important design choices in DUMs: (1) we show that training
schemes decoupling the core architecture and the uncertainty head schemes can
significantly improve uncertainty performances. (2) we demonstrate that the
core architecture expressiveness is crucial for uncertainty performance and
that additional architecture constraints to avoid feature collapse can
deteriorate the trade-off between OOD generalization and detection. (3)
Contrary to other Bayesian models, we show that the prior defined by DUMs do
not have a strong effect on the final performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Fixed-filter Active Noise Control <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengding Luo, Dongyuan Shi, Xiaoyi Shen, Junwei Ji, Woon-Seng Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the slow convergence and poor tracking ability, conventional LMS-based
adaptive algorithms are less capable of handling dynamic noises. Selective
fixed-filter active noise control (SFANC) can significantly reduce response
time by selecting appropriate pre-trained control filters for different noises.
Nonetheless, the limited number of pre-trained control filters may affect noise
reduction performance, especially when the incoming noise differs much from the
initial noises during pre-training. Therefore, a generative fixed-filter active
noise control (GFANC) method is proposed in this paper to overcome the
limitation. Based on deep learning and a perfect-reconstruction filter bank,
the GFANC method only requires a few prior data (one pre-trained broadband
control filter) to automatically generate suitable control filters for various
noises. The efficacy of the GFANC method is demonstrated by numerical
simulations on real-recorded noises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023. Code will be available after publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Up 3D Kernels with Bayesian Frequency Re-parameterization for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Hin Lee, Quan Liu, Shunxing Bao, Qi Yang, Xin Yu, Leon Y. Cai, Thomas Li, Yuankai Huo, Xenofon Koutsoukos, Bennett A. Landman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the inspiration of vision transformers, the concept of depth-wise
convolution revisits to provide a large Effective Receptive Field (ERF) using
Large Kernel (LK) sizes for medical image segmentation. However, the
segmentation performance might be saturated and even degraded as the kernel
sizes scaled up (e.g., $21\times 21\times 21$) in a Convolutional Neural
Network (CNN). We hypothesize that convolution with LK sizes is limited to
maintain an optimal convergence for locality learning. While Structural
Re-parameterization (SR) enhances the local convergence with small kernels in
parallel, optimal small kernel branches may hinder the computational efficiency
for training. In this work, we propose RepUX-Net, a pure CNN architecture with
a simple large kernel block design, which competes favorably with current
network state-of-the-art (SOTA) (e.g., 3D UX-Net, SwinUNETR) using 6
challenging public datasets. We derive an equivalency between kernel
re-parameterization and the branch-wise variation in kernel convergence.
Inspired by the spatial frequency in the human visual system, we extend to vary
the kernel convergence into element-wise setting and model the spatial
frequency as a Bayesian prior to re-parameterize convolutional weights during
training. Specifically, a reciprocal function is leveraged to estimate a
frequency-weighted value, which rescales the corresponding kernel element for
stochastic gradient descent. From the experimental results, RepUX-Net
consistently outperforms 3D SOTA benchmarks with internal validation (FLARE:
0.929 to 0.944), external validation (MSD: 0.901 to 0.932, KiTS: 0.815 to
0.847, LiTS: 0.933 to 0.949, TCIA: 0.736 to 0.779) and transfer learning (AMOS:
0.880 to 0.911) scenarios in Dice Score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Both codes and pretrained models are available at:
  https://github.com/MASILab/RepUX-Net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> CSF Inpainting with Synthetic Atrophy for Improved
  Accuracy Validation of Cortical Surface Analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wang, Kathleen E. Larson, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accuracy validation of cortical thickness measurement is a difficult problem
due to the lack of ground truth data. To address this need, many methods have
been developed to synthetically induce gray matter (GM) atrophy in an MRI via
deformable registration, creating a set of images with known changes in
cortical thickness. However, these methods often cause blurring in atrophied
regions, and cannot simulate realistic atrophy within deep sulci where
cerebrospinal fluid (CSF) is obscured or absent. In this paper, we present a
solution using a self-supervised inpainting model to generate CSF in these
regions and create images with more plausible GM/CSF boundaries. Specifically,
we introduce a novel, 3D GAN model that incorporates patch-based dropout
training, edge map priors, and sinusoidal positional encoding, all of which are
established methods previously limited to 2D domains. We show that our
framework significantly improves the quality of the resulting synthetic images
and is adaptable to unseen data with fine-tuning. We also demonstrate that our
resulting dataset can be employed for accuracy validation of cortical
segmentation and thickness measurement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Medical Imaging with Deep Learning (MIDL) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NFL Career Success as Predicted by NFL Scouting Combine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Szekely, Christian Sinnott, Savannah Halow, Gregory Ryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The National Football League (NFL) Scouting Combine serves as a tool to
evaluate the skills of prospective players and assess their readiness to play
in the NFL. The development of machine learning brings new opportunities in
assessing the utility of the Scouting Combine. Using machine and statistical
learning, it may be possible to predict future success of prospective athletes,
as well as predict which Scouting Combine tests are the most important. Results
from statistical learning research have been contradicting whether the Scouting
combine is a useful metric for player success. In this study, we investigate if
machine learning can be used to determine matriculation and future success in
the NFL. Using Scouting Combine data, we evaluate six different algorithms'
ability to predict whether a potential draft pick will play a single NFL snap
(matriculation). If a player is drafted, we predict how many snaps they go on
to play (success). We are able to predict matriculation with 83% accuracy;
however, we are unable to predict later success. Our best performing algorithm
returns large error and low explained variance (RMSE=1,210 snaps;
${R}^2$=0.17). These findings indicate that while the Scouting Combine can
predict NFL matriculation, it may not be a reliable predictor of long-term
player success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Chen, Dawn Song, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved great success in a range of tasks, such as
image synthesis and molecule design. As such successes hinge on large-scale
training data collected from diverse sources, the trustworthiness of these
collected data is hard to control or audit. In this work, we aim to explore the
vulnerabilities of diffusion models under potential training data manipulations
and try to answer: How hard is it to perform Trojan attacks on well-trained
diffusion models? What are the adversarial targets that such Trojan attacks can
achieve? To answer these questions, we propose an effective Trojan attack
against diffusion models, TrojDiff, which optimizes the Trojan diffusion and
generative processes during training. In particular, we design novel
transitions during the Trojan diffusion process to diffuse adversarial targets
into a biased Gaussian distribution and propose a new parameterization of the
Trojan generative process that leads to an effective training objective for the
attack. In addition, we consider three types of adversarial targets: the
Trojaned diffusion models will always output instances belonging to a certain
class from the in-domain distribution (In-D2D attack), out-of-domain
distribution (Out-D2D-attack), and one specific instance (D2I attack). We
evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM
diffusion models. We show that TrojDiff always achieves high attack performance
under different adversarial targets using different types of triggers, while
the performance in benign environments is preserved. The code is available at
https://github.com/chenweixin107/TrojDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungjin Chung, Suhyeon Lee, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown exceptional performance in solving inverse
problems. However, one major limitation is the slow inference time. While
faster diffusion samplers have been developed for unconditional sampling, there
has been limited research on conditional sampling in the context of inverse
problems. In this study, we propose a novel and efficient diffusion sampling
strategy that employs the geometric decomposition of diffusion sampling.
Specifically, we discover that the samples generated from diffusion models can
be decomposed into two orthogonal components: a ``denoised" component obtained
by projecting the sample onto the clean data manifold, and a ``noise" component
that induces a transition to the next lower-level noisy manifold with the
addition of stochastic noise. Furthermore, we prove that, under some conditions
on the clean data manifold, the conjugate gradient update for imposing
conditioning from the denoised signal belongs to the clean manifold, resulting
in a much faster and more accurate diffusion sampling. Our method is applicable
regardless of the parameterization and setting (i.e., VE, VP). Notably, we
achieve state-of-the-art reconstruction quality on challenging real-world
medical inverse imaging problems, including multi-coil MRI reconstruction and
3D CT reconstruction. Moreover, our proposed method achieves more than 80 times
faster inference time than the previous state-of-the-art method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phase Aberration Correction without Reference Data: An Adaptive Mixed
  <span class="highlight-title">Loss</span> Deep Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phase aberration is one of the primary sources of image quality degradation
in ultrasound, which is induced by spatial variations in sound speed across the
heterogeneous medium. This effect disrupts transmitted waves and prevents
coherent summation of echo signals, resulting in suboptimal image quality. In
real experiments, obtaining non-aberrated ground truths can be extremely
challenging, if not infeasible. It hinders the performance of deep
learning-based phase aberration correction techniques due to sole reliance on
simulated data and the presence of domain shift between simulated and
experimental data. Here, for the first time, we propose a deep learning-based
method that does not require reference data to compensate for the phase
aberration effect. We train a network wherein both input and target output are
randomly aberrated radio frequency (RF) data. Moreover, we demonstrate that a
conventional loss function such as mean square error is inadequate for training
the network to achieve optimal performance. Instead, we propose an adaptive
mixed loss function that employs both B-mode and RF data, resulting in more
efficient convergence and enhanced performance. Source code is available at
\url{http://code.sonography.ai}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Semi-Supervised <span class="highlight-title">Few-Shot</span> Object Detection with SoftER Teacher 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phi Vu Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection is an emerging problem aimed at detecting novel
concepts from few exemplars. Existing approaches to few-shot detection assume
abundant base labels to adapt to novel objects. This paper explores the task of
semi-supervised few-shot detection by considering a realistic scenario which
lacks abundant labels for both base and novel objects. Motivated by this unique
problem, we introduce SoftER Teacher, a robust detector combining the
advantages of pseudo-labeling with representation learning on region proposals.
SoftER Teacher harnesses unlabeled data to jointly optimize for semi-supervised
few-shot detection without explicitly relying on abundant base labels.
Extensive experiments show that SoftER Teacher matches the novel class
performance of a strong supervised detector using only 10% of base labels. Our
work also sheds insight into a previously unknown relationship between
semi-supervised and few-shot detection to suggest that a stronger
semi-supervised detector leads to a more label-efficient few-shot detector.
Code and models are available at
https://github.com/lexisnexis-risk-open-source/ledetection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinical <span class="highlight-title">BERT</span>Score: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware Acceleration of Neural Graphics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Husnain Mubarik, Ramakrishna Kanungo, Tobias Zirr, Rakesh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rendering and inverse-rendering algorithms that drive conventional computer
graphics have recently been superseded by neural representations (NR). NRs have
recently been used to learn the geometric and the material properties of the
scenes and use the information to synthesize photorealistic imagery, thereby
promising a replacement for traditional rendering algorithms with scalable
quality and predictable performance. In this work we ask the question: Does
neural graphics (NG) need hardware support? We studied representative NG
applications showing that, if we want to render 4k res. at 60FPS there is a gap
of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,
there is an even larger gap of 2-4 OOM between the desired performance and the
required system power. We identify that the input encoding and the MLP kernels
are the performance bottlenecks, consuming 72%,60% and 59% of application time
for multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,
respectively. We propose a NG processing cluster, a scalable and flexible
hardware architecture that directly accelerates the input encoding and MLP
kernels through dedicated engines and supports a wide range of NG applications.
We also accelerate the rest of the kernels by fusing them together in Vulkan,
which leads to 9.94X kernel-level performance improvement compared to un-fused
implementation of the pre-processing and the post-processing kernels. Our
results show that, NGPC gives up to 58X end-to-end application-level
performance improvement, for multi res. hashgrid encoding on average across the
four NG applications, the performance benefits are 12X,20X,33X and 39X for the
scaling factor of 8,16,32 and 64, respectively. Our results show that with
multi res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS
for NeRF and 8k res. at 120FPS for all our other NG applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Efficient Model-Free Algorithms for Non-stationary CMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Wei, Arnob Ghosh, Ness Shroff, Lei Ying, Xingyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study model-free reinforcement learning (RL) algorithms in episodic
non-stationary constrained Markov Decision Processes (CMDPs), in which an agent
aims to maximize the expected cumulative reward subject to a cumulative
constraint on the expected utility (cost). In the non-stationary environment,
reward, utility functions, and transition kernels can vary arbitrarily over
time as long as the cumulative variations do not exceed certain variation
budgets. We propose the first model-free, simulator-free RL algorithms with
sublinear regret and zero constraint violation for non-stationary CMDPs in both
tabular and linear function approximation settings with provable performance
guarantees. Our results on regret bound and constraint violation for the
tabular case match the corresponding best results for stationary CMDPs when the
total budget is known. Additionally, we present a general framework for
addressing the well-known challenges associated with analyzing non-stationary
CMDPs, without requiring prior knowledge of the variation budget. We apply the
approach for both tabular and linear approximation settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and
  its Application to Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Yoshida, Sumio Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor decomposition is now being used for data analysis, information
compression, and knowledge recovery. However, the mathematical property of
tensor decomposition is not yet fully clarified because it is one of singular
learning machines. In this paper, we give the upper bound of its real log
canonical threshold (RLCT) of the tensor decomposition by using an algebraic
geometrical method and derive its Bayesian generalization error theoretically.
We also give considerations about its mathematical property through numerical
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Model Confidence Using Counterfactuals <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Le, Tim Miller, Ronal Singh, Liz Sonenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Displaying confidence scores in human-AI interaction has been shown to help
build trust between humans and AI systems. However, most existing research uses
only the confidence score as a form of communication. As confidence scores are
just another model output, users may want to understand why the algorithm is
confident to determine whether to accept the confidence score. In this paper,
we show that counterfactual explanations of confidence scores help study
participants to better understand and better trust a machine learning model's
prediction. We present two methods for understanding model confidence using
counterfactual explanation: (1) based on counterfactual examples; and (2) based
on visualisation of the counterfactual space. Both increase understanding and
trust for study participants over a baseline of no explanation, but qualitative
results show that they are used quite differently, leading to recommendations
of when to use each one and directions of designing better explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023 Main Track. arXiv admin note: substantial text overlap with
  arXiv:2206.02790</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the effectiveness of neural priors in modeling dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameera Ramasinghe, Hemanth Saratchandran, Violetta Shevchenko, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling dynamical systems is an integral component for understanding the
natural world. To this end, neural networks are becoming an increasingly
popular candidate owing to their ability to learn complex functions from large
amounts of data. Despite this recent progress, there has not been an adequate
discussion on the architectural regularization that neural networks offer when
learning such systems, hindering their efficient usage. In this paper, we
initiate a discussion in this direction using coordinate networks as a test
bed. We interpret dynamical systems and coordinate networks from a signal
processing lens, and show that simple coordinate networks with few layers can
be used to solve multiple problems in modelling dynamical systems, without any
explicit regularizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Attacks by Leveraging Decision Boundary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boheng Zeng, LianLi Gao, QiLong Zhang, ChaoQun Li, JingKuan Song, ShuaiQi Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the gap between a substitute model and a victim model, the
gradient-based noise generated from a substitute model may have low
transferability for a victim model since their gradients are different.
Inspired by the fact that the decision boundaries of different models do not
differ much, we conduct experiments and discover that the gradients of
different models are more similar on the decision boundary than in the original
position. Moreover, since the decision boundary in the vicinity of an input
image is flat along most directions, we conjecture that the boundary gradients
can help find an effective direction to cross the decision boundary of the
victim models. Based on it, we propose a Boundary Fitting Attack to improve
transferability. Specifically, we introduce a method to obtain a set of
boundary points and leverage the gradient information of these points to update
the adversarial examples. Notably, our method can be combined with existing
gradient-based methods. Extensive experiments prove the effectiveness of our
method, i.e., improving the success rate by 5.6% against normally trained CNNs
and 14.9% against defense CNNs on average compared to state-of-the-art
transfer-based attacks. Further we compare transformers with CNNs, the results
indicate that transformers are more robust than CNNs. However, our method still
outperforms existing methods when attacking transformers. Specifically, when
using CNNs as substitute models, our method obtains an average attack success
rate of 58.2%, which is 10.8% higher than other state-of-the-art transfer-based
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tradeoff of generalization error in unsupervised learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilhan Kim, Hojun Lee, Junghyo Jo, Yongjoo Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding the optimal model complexity that minimizes the generalization error
(GE) is a key issue of machine learning. For the conventional supervised
learning, this task typically involves the bias-variance tradeoff: lowering the
bias by making the model more complex entails an increase in the variance.
Meanwhile, little has been studied about whether the same tradeoff exists for
unsupervised learning. In this study, we propose that unsupervised learning
generally exhibits a two-component tradeoff of the GE, namely the model error
and the data error -- using a more complex model reduces the model error at the
cost of the data error, with the data error playing a more significant role for
a smaller training dataset. This is corroborated by training the restricted
Boltzmann machine to generate the configurations of the two-dimensional Ising
model at a given temperature and the totally asymmetric simple exclusion
process with given entry and exit rates. Our results also indicate that the
optimal model tends to be more complex when the data to be learned are more
complex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified and Efficient Coordinating Framework for Autonomous DBMS
  Tuning <span class="chip">SIGMOD '23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zhang, Zhuo Chang, Hong Wu, Yang Li, Jia Chen, Jian Tan, Feifei Li, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently using machine learning (ML) based techniques to optimize modern
database management systems has attracted intensive interest from both industry
and academia. With an objective to tune a specific component of a DBMS (e.g.,
index selection, knobs tuning), the ML-based tuning agents have shown to be
able to find better configurations than experienced database administrators.
However, one critical yet challenging question remains unexplored -- how to
make those ML-based tuning agents work collaboratively. Existing methods do not
consider the dependencies among the multiple agents, and the model used by each
agent only studies the effect of changing the configurations in a single
component. To tune different components for DBMS, a coordinating mechanism is
needed to make the multiple agents cognizant of each other. Also, we need to
decide how to allocate the limited tuning budget among the agents to maximize
the performance. Such a decision is difficult to make since the distribution of
the reward for each agent is unknown and non-stationary. In this paper, we
study the above question and present a unified coordinating framework to
efficiently utilize existing ML-based agents. First, we propose a message
propagation protocol that specifies the collaboration behaviors for agents and
encapsulates the global tuning messages in each agent's model. Second, we
combine Thompson Sampling, a well-studied reinforcement learning algorithm with
a memory buffer so that our framework can allocate budget judiciously in a
non-stationary environment. Our framework defines the interfaces adapted to a
broad class of ML-based tuning agents, yet simple enough for integration with
existing implementations and future extensions. We show that it can effectively
utilize different ML-based agents and find better configurations with 1.4~14.1X
speedups on the workload execution time compared with baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2023 International Conference on Management of Data
  (SIGMOD '23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Unlearning for Generative Models via Implicit Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saemi Moon, Seunghyuk Cho, Dongwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of feature unlearning from a pretrained image
generative model. Unlike a common unlearning task where an unlearning target is
a subset of the training set, we aim to unlearn a specific feature, such as
hairstyle from facial images, from the pretrained generative models. As the
target feature is only presented in a local region of an image, unlearning the
entire image from the pretrained model may result in losing other details in
the remaining region of the image. To specify which features to unlearn, we
develop an implicit feedback mechanism where a user can select images
containing the target feature. From the implicit feedback, we identify a latent
representation corresponding to the target feature and then use the
representation to unlearn the generative model. Our framework is generalizable
for the two well-known families of generative models: GANs and VAEs. Through
experiments on MNIST and CelebA datasets, we show that target features are
successfully removed while keeping the fidelity of the original models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness-enhancing deep learning for ride-hailing demand prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Shenhao Wang, Jinhua Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short-term demand forecasting for on-demand ride-hailing services is one of
the fundamental issues in intelligent transportation systems. However, previous
travel demand forecasting research predominantly focused on improving
prediction accuracy, ignoring fairness issues such as systematic
underestimations of travel demand in disadvantaged neighborhoods. This study
investigates how to measure, evaluate, and enhance prediction fairness between
disadvantaged and privileged communities in spatial-temporal demand forecasting
of ride-hailing services. A two-pronged approach is taken to reduce the demand
prediction bias. First, we develop a novel deep learning model architecture,
named socially aware neural network (SA-Net), to integrate the
socio-demographics and ridership information for fair demand prediction through
an innovative socially-aware convolution operation. Second, we propose a
bias-mitigation regularization method to mitigate the mean percentage
prediction error gap between different groups. The experimental results,
validated on the real-world Chicago Transportation Network Company (TNC) data,
show that the de-biasing SA-Net can achieve better predictive performance in
both prediction accuracy and fairness. Specifically, the SA-Net improves
prediction accuracy for both the disadvantaged and privileged groups compared
with the state-of-the-art models. When coupled with the bias mitigation
regularization method, the de-biasing SA-Net effectively bridges the mean
percentage prediction error gap between the disadvantaged and privileged
groups, and also protects the disadvantaged regions against systematic
underestimation of TNC demand. Our proposed de-biasing method can be adopted in
many existing short-term travel demand estimation models, and can be utilized
for various other spatial-temporal prediction tasks such as crime incidents
predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Semantic Medical Image Segmentation with Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Siyu Liu, Craig B. Engstrom, Shekhar S. Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic medical image segmentation using deep learning has recently achieved
high accuracy, making it appealing to clinical problems such as radiation
therapy. However, the lack of high-quality semantically labelled data remains a
challenge leading to model brittleness to small shifts to input data. Most
works require extra data for semi-supervised learning and lack the
interpretability of the boundaries of the training data distribution during
training, which is essential for model deployment in clinical practice. We
propose a fully supervised generative framework that can achieve generalisable
segmentation with only limited labelled data by simultaneously constructing an
explorable manifold during training. The proposed approach creates medical
image style paired with a segmentation task driven discriminator incorporating
end-to-end adversarial training. The discriminator is generalised to small
domain shifts as much as permissible by the training data, and the generator
automatically diversifies the training samples using a manifold of input
features learnt during segmentation. All the while, the discriminator guides
the manifold learning by supervising the semantic content and fine-grained
features separately during the image diversification. After training,
visualisation of the learnt manifold from the generator is available to
interpret the model limits. Experiments on a fully semantic, publicly available
pelvis dataset demonstrated that our method is more generalisable to shifts
than other state-of-the-art methods while being more explainable using an
explorable manifold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Max-Value Entropy Search for Multi-Agent Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Ma, Tianpeng Zhang, Yixuan Wu, Flavio P. Calmon, Na Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the multi-agent Bayesian optimization (BO) problem, where multiple
agents maximize a black-box function via iterative queries. We focus on Entropy
Search (ES), a sample-efficient BO algorithm that selects queries to maximize
the mutual information about the maximum of the black-box function. One of the
main challenges of ES is that calculating the mutual information requires
computationally-costly approximation techniques. For multi-agent BO problems,
the computational cost of ES is exponential in the number of agents. To address
this challenge, we propose the Gaussian Max-value Entropy Search, a multi-agent
BO algorithm with favorable sample and computational efficiency. The key to our
idea is to use a normal distribution to approximate the function maximum and
calculate its mutual information accordingly. The resulting approximation
allows queries to be cast as the solution of a closed-form optimization problem
which, in turn, can be solved via a modified gradient ascent algorithm and
scaled to a large number of agents. We demonstrate the effectiveness of
Gaussian max-value Entropy Search through numerical experiments on standard
test functions and real-robot experiments on the source-seeking problem.
Results show that the proposed algorithm outperforms the multi-agent BO
baselines in the numerical experiments and can stably seek the source with a
limited number of noisy observations on real robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Pose Estimation from Ambiguous Pressure Recordings with
  Spatio-temporal Masked <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vandad Davoodnia, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance of vision-based pose estimators, they
generally fail to perform well under adverse vision conditions and often don't
satisfy the privacy demands of customers. As a result, researchers have begun
to study tactile sensing systems as an alternative. However, these systems
suffer from noisy and ambiguous recordings. To tackle this problem, we propose
a novel solution for pose estimation from ambiguous pressure data. Our method
comprises a spatio-temporal vision transformer with an encoder-decoder
architecture. Detailed experiments on two popular public datasets reveal that
our model outperforms existing solutions in the area. Moreover, we observe that
increasing the number of temporal crops in the early stages of the network
positively impacts the performance while pre-training the network in a
self-supervised setting using a masked auto-encoder approach also further
improves the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Clustering with OWA-based Linkages, the Lance-Williams
  Formula, and Dendrogram Inversions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Gagolewski, Anna Cena, Simon James, Gleb Beliakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agglomerative hierarchical clustering based on Ordered Weighted Averaging
(OWA) operators not only generalises the single, complete, and average
linkages, but also includes intercluster distances based on a few nearest or
farthest neighbours, trimmed and winsorised means of pairwise point
similarities, amongst many others. We explore the relationships between the
famous Lance-Williams update formula and the extended OWA-based linkages with
weights generated via infinite coefficient sequences. Furthermore, we provide
some conditions for the weight generators to guarantee the resulting
dendrograms to be free from unaesthetic inversions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A dual basis approach to multidimensional scaling: spectral analysis and
  graph regularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Lichtenberg, Abiy Tasissa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical multidimensional scaling (CMDS) is a technique that aims to embed a
set of objects in a Euclidean space given their pairwise Euclidean distance
matrix. The main part of CMDS is based on double centering a squared distance
matrix and employing a truncated eigendecomposition to recover the point
coordinates. A central result in CMDS connects the squared Euclidean matrix to
a Gram matrix derived from the set of points. In this paper, we study a dual
basis approach to classical multidimensional scaling. We give an explicit
formula for the dual basis and fully characterize the spectrum of an essential
matrix in the dual basis framework. We make connections to a related problem in
metric nearness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering with minimum spanning trees: How good can it be? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Gagolewski, Anna Cena, Maciej Bartoszuk, Łukasz Brzozowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimum spanning trees (MSTs) provide a convenient representation of datasets
in numerous pattern recognition activities. Moreover, they are relatively fast
to compute. In this paper, we quantify the extent to which they can be
meaningful in data clustering tasks. By identifying the upper bounds for the
agreement between the best (oracle) algorithm and the expert labels from a
large battery of benchmark data, we discover that MST methods can overall be
very competitive. Next, instead of proposing yet another algorithm that
performs well on a limited set of examples, we review, study, extend, and
generalise existing, the state-of-the-art MST-based partitioning schemes, which
leads to a few new and interesting approaches. It turns out that the Genie
method and the information-theoretic approaches often outperform the non-MST
algorithms such as k-means, Gaussian mixtures, spectral clustering, BIRCH, and
classical hierarchical agglomerative procedures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Weakly Supervised Sound Event Detection with Causal
  Intervention <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xin, Dongchao Yang, Fan Cui, Yujun Wang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing weakly supervised sound event detection (WSSED) work has not
explored both types of co-occurrences simultaneously, i.e., some sound events
often co-occur, and their occurrences are usually accompanied by specific
background sounds, so they would be inevitably entangled, causing
misclassification and biased localization results with only clip-level
supervision. To tackle this issue, we first establish a structural causal model
(SCM) to reveal that the context is the main cause of co-occurrence confounders
that mislead the model to learn spurious correlations between frames and
clip-level labels. Based on the causal analysis, we propose a causal
intervention (CI) method for WSSED to remove the negative impact of
co-occurrence confounders by iteratively accumulating every possible context of
each class and then re-projecting the contexts to the frame-level features for
making the event boundary clearer. Experiments show that our method effectively
improves the performance on multiple datasets and can generalize to various
baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards better traffic volume estimation: Tackling both underdetermined
  and non-equilibrium problems via a correlation adaptive graph convolution
  network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Nie, Guoyang Qin, Yunpeng Wang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic volume is an indispensable ingredient to provide fine-grained
information for traffic management and control. However, due to limited
deployment of traffic sensors, obtaining full-scale volume information is far
from easy. Existing works on this topic primarily focus on improving the
overall estimation accuracy of a particular method and ignore the underlying
challenges of volume estimation, thereby having inferior performances on some
critical tasks. This paper studies two key problems with regard to traffic
volume estimation: (1) underdetermined traffic flows caused by undetected
movements, and (2) non-equilibrium traffic flows arise from congestion
propagation. Here we demonstrate a graph-based deep learning method that can
offer a data-driven, model-free and correlation adaptive approach to tackle the
above issues and perform accurate network-wide traffic volume estimation.
Particularly, in order to quantify the dynamic and nonlinear relationships
between traffic speed and volume for the estimation of underdetermined flows, a
speed patternadaptive adjacent matrix based on graph attention is developed and
integrated into the graph convolution process, to capture non-local
correlations between sensors. To measure the impacts of non-equilibrium flows,
a temporal masked and clipped attention combined with a gated temporal
convolution layer is customized to capture time-asynchronous correlations
between upstream and downstream sensors. We then evaluate our model on a
real-world highway traffic volume dataset and compare it with several benchmark
models. It is demonstrated that the proposed model achieves high estimation
accuracy even under 20% sensor coverage rate and outperforms other baselines
significantly, especially on underdetermined and non-equilibrium flow
locations. Furthermore, comprehensive quantitative model analysis are also
carried out to justify the model designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Yuan, Songchi Zhou, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic health records (EHR) contain vast biomedical knowledge and are
rich resources for developing precise medicine systems. However, due to privacy
concerns, there are limited high-quality EHR data accessible to researchers
hence hindering the advancement of methodologies. Recent research has explored
using generative modelling methods to synthesize realistic EHR data, and most
proposed methods are based on the generative adversarial network (GAN) and its
variants for EHR synthesis. Although GAN-style methods achieved
state-of-the-art performance in generating high-quality EHR data, such methods
are hard to train and prone to mode collapse. Diffusion models are recently
proposed generative modelling methods and set cutting-edge performance in image
generation. The performance of diffusion models in realistic EHR synthesis is
rarely explored. In this work, we explore whether the superior performance of
diffusion models can translate to the domain of EHR synthesis and propose a
novel EHR synthesis method named EHRDiff. Through comprehensive experiments,
EHRDiff achieves new state-of-the-art performance for the quality of synthetic
EHR data and can better protect private information in real training EHRs in
the meanwhile.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GATOR: Graph-Aware <span class="highlight-title">Transformer</span> with Motion-Disentangled Regression for
  Human Mesh Recovery from a 2D Pose <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxuan You, Hong Liu, Xia Li, Wenhao Li, Ti Wang, Runwei Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human mesh recovery from a 2D pose plays an important role in various
applications. However, it is hard for existing methods to simultaneously
capture the multiple relations during the evolution from skeleton to mesh,
including joint-joint, joint-vertex and vertex-vertex relations, which often
leads to implausible results. To address this issue, we propose a novel
solution, called GATOR, that contains an encoder of Graph-Aware Transformer
(GAT) and a decoder with Motion-Disentangled Regression (MDR) to explore these
multiple relations. Specifically, GAT combines a GCN and a graph-aware
self-attention in parallel to capture physical and hidden joint-joint
relations. Furthermore, MDR models joint-vertex and vertex-vertex interactions
to explore joint and vertex relations. Based on the clustering characteristics
of vertex offset fields, MDR regresses the vertices by composing the predicted
base motions. Extensive experiments show that GATOR achieves state-of-the-art
performance on two challenging benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pacos: Modeling Users' Interpretable and Context-Dependent Choices in
  Preference Reversals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choice problems refer to selecting the best choices from several items, and
learning users' preferences in choice problems is of great significance in
understanding the decision making mechanisms and providing personalized
services. Existing works typically assume that people evaluate items
independently. In practice, however, users' preferences depend on the market in
which items are placed, which is known as context effects; and the order of
users' preferences for two items may even be reversed, which is referred to
preference reversals. In this work, we identify three factors contributing to
context effects: users' adaptive weights, the inter-item comparison, and
display positions. We propose a context-dependent preference model named Pacos
as a unified framework for addressing three factors simultaneously, and
consider two design methods including an additive method with high
interpretability and an ANN-based method with high accuracy. We study the
conditions for preference reversals to occur and provide an theoretical proof
of the effectiveness of Pacos in addressing preference reversals. Experimental
results show that the proposed method has better performance than prior works
in predicting users' choices, and has great interpretability to help understand
the cause of preference reversals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Real Time Recurrent Learning through combined activity and
  parameter sparsity <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Subramoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backpropagation through time (BPTT) is the standard algorithm for training
recurrent neural networks (RNNs), which requires separate simulation phases for
the forward and backward passes for inference and learning, respectively.
Moreover, BPTT requires storing the complete history of network states between
phases, with memory consumption growing proportional to the input sequence
length. This makes BPTT unsuited for online learning and presents a challenge
for implementation on low-resource real-time systems. Real-Time Recurrent
Learning (RTRL) allows online learning, and the growth of required memory is
independent of sequence length. However, RTRL suffers from exceptionally high
computational costs that grow proportional to the fourth power of the state
size, making RTRL computationally intractable for all but the smallest of
networks. In this work, we show that recurrent networks exhibiting high
activity sparsity can reduce the computational cost of RTRL. Moreover,
combining activity and parameter sparsity can lead to significant enough
savings in computational and memory costs to make RTRL practical. Unlike
previous work, this improvement in the efficiency of RTRL can be achieved
without using any approximations for the learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a workshop paper at ICLR 2023 Workshop on Sparsity in
  Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> One-Shot Learning for Automatic Segmentation of StyleGAN
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Manerikar, Avinash C. Kak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose in this paper a framework for automatic one-shot segmentation of
synthetic images generated using StyleGANs. As to the need for `one-shot
segmentation', we want the network to carry out a semantic segmentation of the
images on the fly, that is, as they are being produced at inference time. The
implementation of our framework is based on the observation that the
multi-scale hidden features produced by a GAN during image synthesis hold
useful semantic information that can be utilized for automatic segmentation.
Using these features, our proposed framework learns to segment synthetic images
using a novel self-supervised, contrastive clustering algorithm that projects
the hidden features in the generator onto a compact feature space for per-pixel
classification. This contrastive learner uses a swapped prediction loss for
image segmentation that is computed using pixel-wise cluster assignments for
the image and its transformed variants. Using the hidden features from an
already pre-trained GAN for clustering, this leads to a much faster learning of
the pixel-wise feature vectors for one-shot segmentation. We have tested our
implementation on a number of standard benchmarks (CelebA, LSUN, PASCAL-Part)
for object and part segmentation. The results of our experiments yield a
segmentation performance that not only outperforms the semi-supervised baseline
methods with an average wIoU margin of 1.02 % but also improves the inference
speeds by a peak factor of 4.5. Finally, we also show the results of using the
proposed framework in the implementation of BagGAN, a GAN-based framework for
the production of annotated synthetic baggage X-ray scans for threat detection.
This one-shot learning framework was trained and tested on the PIDRay baggage
screening benchmark for 5 different threat categories to yield a segmentation
performance which stands close to its baseline segmenter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusarium head blight detection, spikelet estimation, and severity
  assessment in wheat using 3D convolutional neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oumaima Hamila, Christopher J. Henry, Oscar I. Molina, Christopher P. Bidinosti, Maria Antonia Henriquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fusarium head blight (FHB) is one of the most significant diseases affecting
wheat and other small grain cereals worldwide. The development of resistant
varieties requires the laborious task of field and greenhouse phenotyping. The
applications considered in this work are the automated detection of FHB disease
symptoms expressed on a wheat plant, the automated estimation of the total
number of spikelets and the total number of infected spikelets on a wheat head,
and the automated assessment of the FHB severity in infected wheat. The data
used to generate the results are 3-dimensional (3D) multispectral point clouds
(PC), which are 3D collections of points - each associated with a red, green,
blue (RGB), and near-infrared (NIR) measurement. Over 300 wheat plant images
were collected using a multispectral 3D scanner, and the labelled UW-MRDC 3D
wheat dataset was created. The data was used to develop novel and efficient 3D
convolutional neural network (CNN) models for FHB detection, which achieved
100% accuracy. The influence of the multispectral information on performance
was evaluated, and our results showed the dominance of the RGB channels over
both the NIR and the NIR plus RGB channels combined. Furthermore, novel and
efficient 3D CNNs were created to estimate the total number of spikelets and
the total number of infected spikelets on a wheat head, and our best models
achieved mean absolute errors (MAE) of 1.13 and 1.56, respectively. Moreover,
3D CNN models for FHB severity estimation were created, and our best model
achieved 8.6 MAE. A linear regression analysis between the visual FHB severity
assessment and the FHB severity predicted by our 3D CNN was performed, and the
results showed a significant correlation between the two variables with a
0.0001 P-value and 0.94 R-squared.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring Efficiency of IoT Wireless Charging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengwei Yang, Amani Abusafia, Abdallah Lakhdari, Athman Bouguettaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowdsourcing wireless energy is a novel and convenient solution to charge
nearby IoT devices. Several applications have been proposed to enable
peer-to-peer wireless energy charging. However, none of them considered the
energy efficiency of the wireless transfer of energy. In this paper, we propose
an energy estimation framework that predicts the actual received energy. Our
framework uses two machine learning algorithms, namely XGBoost and Neural
Network, to estimate the received energy. The result shows that the Neural
Network model is better than XGBoost at predicting the received energy. We
train and evaluate our models by collecting a real wireless energy dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 4 figures. This is an accepted demo paper and it will appear
  in The 21st International Conference on Pervasive Computing and
  Communications (PerCom 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Unlikelihood of D-Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Feigenbaum, Huan Wang, Shelby Heinecke, Juan Carlos Niebles, Weiran Yao, Caiming Xiong, Devansh Arpit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery aims to recover a causal graph from data generated by it;
constraint based methods do so by searching for a d-separating conditioning set
of nodes in the graph via an oracle. In this paper, we provide analytic
evidence that on large graphs, d-separation is a rare phenomenon, even when
guaranteed to exist, unless the graph is extremely sparse. We then provide an
analytic average case analysis of the PC Algorithm for causal discovery, as
well as a variant of the SGS Algorithm we call UniformSGS. We consider a set
$V=\{v_1,\ldots,v_n\}$ of nodes, and generate a random DAG $G=(V,E)$ where
$(v_a, v_b) \in E$ with i.i.d. probability $p_1$ if $a<b$ and $0$ if $a > b$.
We provide upper bounds on the probability that a subset of $V-\{x,y\}$
d-separates $x$ and $y$, conditional on $x$ and $y$ being d-separable; our
upper bounds decay exponentially fast to $0$ as $|V| \rightarrow \infty$. For
the PC Algorithm, while it is known that its worst-case guarantees fail on
non-sparse graphs, we show that the same is true for the average case, and that
the sparsity requirement is quite demanding: for good performance, the density
must go to $0$ as $|V| \rightarrow \infty$ even in the average case. For
UniformSGS, while it is known that the running time is exponential for existing
edges, we show that in the average case, that is the expected running time for
most non-existing edges as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An analytic theory for the dynamics of wide quantum neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Liu, Khadijeh Najafi, Kunal Sharma, Francesco Tacchino, Liang Jiang, Antonio Mezzacapo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameterized quantum circuits can be used as quantum neural networks and
have the potential to outperform their classical counterparts when trained for
addressing learning problems. To date, much of the results on their performance
on practical problems are heuristic in nature. In particular, the convergence
rate for the training of quantum neural networks is not fully understood. Here,
we analyze the dynamics of gradient descent for the training error of a class
of variational quantum machine learning models. We define wide quantum neural
networks as parameterized quantum circuits in the limit of a large number of
qubits and variational parameters. We then find a simple analytic formula that
captures the average behavior of their loss function and discuss the
consequences of our findings. For example, for random quantum circuits, we
predict and characterize an exponential decay of the residual training error as
a function of the parameters of the system. We finally validate our analytic
results with numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, many figures. v2: adding learning supervised perspectives
  and new results, close to published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DDPNAS: Efficient Neural Architecture Search via Dynamic Distribution
  Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1905.13543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1905.13543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiawu Zheng, Chenyi Yang, Shaokun Zhang, Yan Wang, Baochang Zhang, Yongjian Wu, Yunsheng Wu, Ling Shao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has demonstrated state-of-the-art
performance on various computer vision tasks. Despite the superior performance
achieved, the efficiency and generality of existing methods are highly valued
due to their high computational complexity and low generality. In this paper,
we propose an efficient and unified NAS framework termed DDPNAS via dynamic
distribution pruning, facilitating a theoretical bound on accuracy and
efficiency. In particular, we first sample architectures from a joint
categorical distribution. Then the search space is dynamically pruned and its
distribution is updated every few epochs. With the proposed efficient network
generation method, we directly obtain the optimal neural architectures on given
constraints, which is practical for on-device models across diverse search
spaces and constraints. The architectures searched by our method achieve
remarkable top-1 accuracies, 97.56 and 77.2 on CIFAR-10 and ImageNet (mobile
settings), respectively, with the fastest search process, i.e., only 1.8 GPU
hours on a Tesla V100. Codes for searching and network generation are available
at: https://openi.pcl.ac.cn/PCL AutoML/XNAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A update version of this work. 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEJA VU: Continual Model Generalization For Unseen Domains <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao Wang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, deep learning models often run in non-stationary
environments where the target data distribution continually shifts over time.
There have been numerous domain adaptation (DA) methods in both online and
offline modes to improve cross-domain adaptation ability. However, these DA
methods typically only provide good performance after a long period of
adaptation, and perform poorly on new domains before and during adaptation - in
what we call the "Unfamiliar Period", especially when domain shifts happen
suddenly and significantly. On the other hand, domain generalization (DG)
methods have been proposed to improve the model generalization ability on
unadapted domains. However, existing DG works are ineffective for continually
changing domains due to severe catastrophic forgetting of learned knowledge. To
overcome these limitations of DA and DG in handling the Unfamiliar Period
during continual domain shift, we propose RaTP, a framework that focuses on
improving models' target domain generalization (TDG) capability, while also
achieving effective target domain adaptation (TDA) capability right after
training on certain domains and forgetting alleviation (FA) capability on past
domains. RaTP includes a training-free data augmentation module to prepare data
for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for
TDA, and a prototype contrastive alignment algorithm to align different domains
for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and
DomainNet demonstrate that RaTP significantly outperforms state-of-the-art
works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG,
Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multidimensional Interactive Fixed-Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a linear and additively separable model for
multidimensional panel data of three or more dimensions with unobserved
interactive fixed effects. Two approaches are considered to account for these
unobserved interactive fixed-effects when estimating coefficients on the
observed covariates. First, the model is embedded within the standard
two-dimensional panel framework and restrictions are derived under which the
factor structure methods in Bai (2009) lead to consistent estimation of model
parameters, but at potentially slow rates of convergence. The second approach
utilises popular machine learning techniques to develop group fixed-effects and
kernel weighted fixed-effects that are more robust to the multidimensional
nature of the problem and can achieve the parametric rate of consistency under
certain conditions. Theoretical results and simulations show the benefit of
standard two-dimensional panel methods when the structure of the interactive
fixed-effect term is known, but also highlight how the group fixed-effects and
kernel methods perform well without knowledge of this structure. The methods
are implemented to estimate the demand elasticity for beer under a handful of
models for demand.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Clustering Survival Machines with Interpretable Expert
  Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bojian Hou, Hongming Li, Zhicheng Jiao, Zhen Zhou, Hao Zheng, Yong Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional survival analysis methods are typically ineffective to
characterize heterogeneity in the population while such information can be used
to assist predictive modeling. In this study, we propose a hybrid survival
analysis method, referred to as deep clustering survival machines, that
combines the discriminative and generative mechanisms. Similar to the mixture
models, we assume that the timing information of survival data is generatively
described by a mixture of certain numbers of parametric distributions, i.e.,
expert distributions. We learn weights of the expert distributions for
individual instances according to their features discriminatively such that
each instance's survival information can be characterized by a weighted
combination of the learned constant expert distributions. This method also
facilitates interpretable subgrouping/clustering of all instances according to
their associated expert distributions. Extensive experiments on both real and
synthetic datasets have demonstrated that the method is capable of obtaining
promising clustering results and competitive time-to-event predicting
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating ODE-Based Neural Networks on Low-Cost FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.15465v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.15465v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirohisa Watanabe, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ODENet is a deep neural network architecture in which a stacking structure of
ResNet is implemented with an ordinary differential equation (ODE) solver. It
can reduce the number of parameters and strike a balance between accuracy and
performance by selecting a proper solver. It is also possible to improve the
accuracy while keeping the same number of parameters on resource-limited edge
devices. In this paper, using Euler method as an ODE solver, a part of ODENet
is implemented as a dedicated logic on a low-cost FPGA (Field-Programmable Gate
Array) board, such as PYNQ-Z2 board. As ODENet variants, reduced ODENets
(rODENets) each of which heavily uses a part of ODENet layers and
reduces/eliminates some layers differently are proposed and analyzed for
low-cost FPGA implementation. They are evaluated in terms of parameter size,
accuracy, execution time, and resource utilization on the FPGA. The results
show that an overall execution time of an rODENet variant is improved by up to
2.66 times compared to a pure software execution while keeping a comparable
accuracy to the original ODENet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RAW'21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Distributed Deep Reinforcement Learning by In-Network
  Experience Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Furukawa, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A computing cluster that interconnects multiple compute nodes is used to
accelerate distributed reinforcement learning based on DQN (Deep Q-Network). In
distributed reinforcement learning, Actor nodes acquire experiences by
interacting with a given environment and a Learner node optimizes their DQN
model. Since data transfer between Actor and Learner nodes increases depending
on the number of Actor nodes and their experience size, communication overhead
between them is one of major performance bottlenecks. In this paper, their
communication is accelerated by DPDK-based network optimizations, and
DPDK-based low-latency experience replay memory server is deployed between
Actor and Learner nodes interconnected with a 40GbE (40Gbit Ethernet) network.
Evaluation results show that, as a network optimization technique, kernel
bypassing by DPDK reduces network access latencies to a shared memory server by
32.7% to 58.9%. As another network optimization technique, an in-network
experience replay memory server between Actor and Learner nodes reduces access
latencies to the experience replay memory by 11.7% to 28.1% and communication
latencies for prioritized experience sampling by 21.9% to 29.1%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PDP'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APTx: better activation function than MISH, SWISH, and ReLU's variants
  used in deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06119v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06119v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation Functions introduce non-linearity in the deep neural networks.
This nonlinearity helps the neural networks learn faster and efficiently from
the dataset. In deep learning, many activation functions are developed and used
based on the type of problem statement. ReLU's variants, SWISH, and MISH are
goto activation functions. MISH function is considered having similar or even
better performance than SWISH, and much better than ReLU. In this paper, we
propose an activation function named APTx which behaves similar to MISH, but
requires lesser mathematical operations to compute. The lesser computational
requirements of APTx does speed up the model training, and thus also reduces
the hardware requirement for the deep learning model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication Size Reduction of Federated Learning using Neural ODE
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09478v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09478v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Hoshino, Hiroki Kawakami, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a machine learning approach in which data is not
aggregated on a server, but is trained at clients locally, in consideration of
security and privacy. ResNet is a classic but representative neural network
that succeeds in deepening the neural network by learning a residual function
that adds the inputs and outputs together. In federated learning, communication
is performed between the server and clients to exchange weight parameters.
Since ResNet has deep layers and a large number of parameters, the
communication size becomes large. In this paper, we use Neural ODE as a
lightweight model of ResNet to reduce communication size in federated learning.
In addition, we newly introduce a flexible federated learning using Neural ODE
models with different number of iterations, which correspond to ResNet models
with different depths. Evaluation results using CIFAR-10 dataset show that the
use of Neural ODE reduces communication size by up to 92.4% compared to ResNet.
We also show that the proposed flexible federated learning can merge models
with different iteration counts or depths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conjugate Natural Selection: Fisher-Rao Natural Gradient Descent
  Optimally Approximates Evolutionary Dynamics and Continuous Bayesian
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reilly Raab, Luca de Alfaro, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rather than refining individual candidate solutions for a general non-convex
optimization problem, by analogy to evolution, we consider minimizing the
average loss for a parametric distribution over hypotheses. In this setting, we
prove that Fisher-Rao natural gradient descent (FR-NGD) optimally approximates
the continuous-time replicator equation (an essential model of evolutionary
dynamics) by minimizing the mean-squared error for the relative fitness of
competing hypotheses. We term this finding "conjugate natural selection" and
demonstrate its utility by numerically solving an example non-convex
optimization problem over a continuous strategy space. Next, by developing
known connections between discrete-time replicator dynamics and Bayes's rule,
we show that when absolute fitness corresponds to the negative KL-divergence of
a hypothesis's predictions from actual observations, FR-NGD provides the
optimal approximation of continuous Bayesian inference. We use this result to
demonstrate a novel method for estimating the parameters of stochastic
processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Human-Level <span class="highlight-title">Prompt</span> Engineers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the "program," optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reusing Combinatorial Structure: Faster Iterative Projections over
  Submodular Base Polytopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.11943v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.11943v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Moondra, Hassan Mortagy, Swati Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization algorithms such as projected Newton's method, FISTA, mirror
descent, and its variants enjoy near-optimal regret bounds and convergence
rates, but suffer from a computational bottleneck of computing ``projections''
in potentially each iteration (e.g., $O(T^{1/2})$ regret of online mirror
descent). On the other hand, conditional gradient variants solve a linear
optimization in each iteration, but result in suboptimal rates (e.g.,
$O(T^{3/4})$ regret of online Frank-Wolfe). Motivated by this trade-off in
runtime v/s convergence rates, we consider iterative projections of close-by
points over widely-prevalent submodular base polytopes $B(f)$. We first give
necessary and sufficient conditions for when two close points project to the
same face of a polytope, and then show that points far away from the polytope
project onto its vertices with high probability. We next use this theory and
develop a toolkit to speed up the computation of iterative projections over
submodular polytopes using both discrete and continuous perspectives. We
subsequently adapt the away-step Frank-Wolfe algorithm to use this information
and enable early termination. For the special case of cardinality-based
submodular polytopes, we improve the runtime of computing certain Bregman
projections by a factor of $\Omega(n/\log(n))$. Our theoretical results show
orders of magnitude reduction in runtime in preliminary computational
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locally Regularized Neural Differential Equations: Some Black Boxes Were
  Meant to Remain Closed! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avik Pal, Alan Edelman, Chris Rackauckas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit layer deep learning techniques, like Neural Differential Equations,
have become an important modeling framework due to their ability to adapt to
new problems automatically. Training a neural differential equation is
effectively a search over a space of plausible dynamical systems. However,
controlling the computational cost for these models is difficult since it
relies on the number of steps the adaptive solver takes. Most prior works have
used higher-order methods to reduce prediction timings while greatly increasing
training time or reducing both training and prediction timings by relying on
specific training algorithms, which are harder to use as a drop-in replacement
due to strict requirements on automatic differentiation. In this manuscript, we
use internal cost heuristics of adaptive differential equation solvers at
stochastic time points to guide the training toward learning a dynamical system
that is easier to integrate. We "close the black-box" and allow the use of our
method with any adjoint technique for gradient calculations of the differential
equation solution. We perform experimental studies to compare our method to
global regularization to show that we attain similar performance numbers
without compromising the flexibility of implementation on ordinary differential
equations (ODEs) and stochastic differential equations (SDEs). We develop two
sampling strategies to trade off between performance and training time. Our
method reduces the number of function evaluations to 0.556-0.733x and
accelerates predictions by 1.3-2x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metrizing Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yves Rychener, Bahar Taskesen, Daniel Kuhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study supervised learning problems for predicting properties of
individuals who belong to one of two demographic groups, and we seek predictors
that are fair according to statistical parity. This means that the
distributions of the predictions within the two groups should be close with
respect to the Kolmogorov distance, and fairness is achieved by penalizing the
dissimilarity of these two distributions in the objective function of the
learning problem. In this paper, we showcase conceptual and computational
benefits of measuring unfairness with integral probability metrics (IPMs) other
than the Kolmogorov distance. Conceptually, we show that the generator of any
IPM can be interpreted as a family of utility functions and that unfairness
with respect to this IPM arises if individuals in the two demographic groups
have diverging expected utilities. We also prove that the
unfairness-regularized prediction loss admits unbiased gradient estimators if
unfairness is measured by the squared $\mathcal L^2$-distance or by a squared
maximum mean discrepancy. In this case, the fair learning problem is
susceptible to efficient stochastic gradient descent (SGD) algorithms.
Numerical experiments on real data show that these SGD algorithms outperform
state-of-the-art methods for fair learning in that they achieve superior
accuracy-unfairness trade-offs -- sometimes orders of magnitude faster.
Finally, we identify conditions under which statistical parity can improve
prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> POLICE: Provably Optimal Linear Constraint Enforcement for Deep Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Randall Balestriero, <span class="highlight-author">Yann LeCun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) outshine alternative function approximators in
many settings thanks to their modularity in composing any desired
differentiable operator. The formed parametrized functional is then tuned to
solve a task at hand from simple gradient descent. This modularity comes at the
cost of making strict enforcement of constraints on DNNs, e.g. from a priori
knowledge of the task, or from desired physical properties, an open challenge.
In this paper we propose the first provable affine constraint enforcement
method for DNNs that only requires minimal changes into a given DNN's
forward-pass, that is computationally friendly, and that leaves the
optimization of the DNN's parameter to be unconstrained, i.e. standard
gradient-based method can be employed. Our method does not require any sampling
and provably ensures that the DNN fulfills the affine constraint on a given
input space's region at any point during training, and testing. We coin this
method POLICE, standing for Provably Optimal LInear Constraint Enforcement.
Github: https://github.com/RandallBalestriero/POLICE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Estimates of Predictions via a General Bias-Variance
  Decomposition <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian G. Gruber, Florian Buettner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliably estimating the uncertainty of a prediction throughout the model
lifecycle is crucial in many safety-critical applications. The most common way
to measure this uncertainty is via the predicted confidence. While this tends
to work well for in-domain samples, these estimates are unreliable under domain
drift and restricted to classification. Alternatively, proper scores can be
used for most predictive tasks but a bias-variance decomposition for model
uncertainty does not exist in the current literature. In this work we introduce
a general bias-variance decomposition for proper scores, giving rise to the
Bregman Information as the variance term. We discover how exponential families
and the classification log-likelihood are special cases and provide novel
formulations. Surprisingly, we can express the classification case purely in
the logit space. We showcase the practical relevance of this decomposition on
several downstream tasks, including model ensembles and confidence regions.
Further, we demonstrate how different approximations of the instance-level
Bregman Information allow reliable out-of-distribution detection for all
degrees of domain drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-based Causal Bayesian Optimization <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Sussex, Anastasiia Makarova, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How should we intervene on an unknown structural equation model to maximize a
downstream variable of interest? This setting, also known as causal Bayesian
optimization (CBO), has important applications in medicine, ecology, and
manufacturing. Standard Bayesian optimization algorithms fail to effectively
leverage the underlying causal structure. Existing CBO approaches assume
noiseless measurements and do not come with guarantees. We propose the
model-based causal Bayesian optimization algorithm (MCBO) that learns a full
system model instead of only modeling intervention-reward pairs. MCBO
propagates epistemic uncertainty about the causal mechanisms through the graph
and trades off exploration and exploitation via the optimism principle. We
bound its cumulative regret, and obtain the first non-asymptotic bounds for
CBO. Unlike in standard Bayesian optimization, our acquisition function cannot
be evaluated in closed form, so we show how the reparameterization trick can be
used to apply gradient-based optimizers. The resulting practical implementation
of MCBO compares favorably with state-of-the-art approaches empirically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures, accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximal Objectives in the Multi-armed Bandit with Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.06853v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.06853v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eren Ozbay, Vijay Kamble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In several applications of the stochastic multi-armed bandit problem, the
traditional objective of maximizing the expected total reward can be
inappropriate. In this paper, motivated by certain operational concerns in
online platforms, we consider a new objective in the classical setup. Given $K$
arms, instead of maximizing the expected total reward from $T$ pulls (the
traditional "sum" objective), we consider the vector of total rewards earned
from each of the $K$ arms at the end of $T$ pulls and aim to maximize the
expected highest total reward across arms (the "max" objective). For this
objective, we show that any policy must incur an instance-dependent asymptotic
regret of $\Omega(\log T)$ (with a higher instance-dependent constant compared
to the traditional objective) and a worst-case regret of
$\Omega(K^{1/3}T^{2/3})$. We then design an adaptive explore-then-commit policy
featuring exploration based on appropriately tuned confidence bounds on the
mean reward and an adaptive stopping criterion, which adapts to the problem
difficulty and achieves these bounds (up to logarithmic factors). We then
generalize our algorithmic insights to the problem of maximizing the expected
value of the average total reward of the top $m$ arms with the highest total
rewards. Our numerical experiments demonstrate the efficacy of our policies
compared to several natural alternatives in practical parameter regimes. We
discuss applications of these new objectives to the problem of grooming an
adequate supply of value-providing market participants (workers/sellers/service
providers) in online platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>92 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximate Regions of Attraction in Learning with Decision-Dependent
  Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.00055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.00055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Dong, Heling Zhang, Lillian J. Ratliff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data-driven methods are deployed in real-world settings, the processes
that generate the observed data will often react to the decisions of the
learner. For example, a data source may have some incentive for the algorithm
to provide a particular label (e.g. approve a bank loan), and manipulate their
features accordingly. Work in strategic classification and decision-dependent
distributions seeks to characterize the closed-loop behavior of deploying
learning algorithms by explicitly considering the effect of the classifier on
the underlying data distribution. More recently, works in performative
prediction seek to classify the closed-loop behavior by considering general
properties of the mapping from classifier to data distribution, rather than an
explicit form. Building on this notion, we analyze repeated risk minimization
as the perturbed trajectories of the gradient flows of performative risk
minimization. We consider the case where there may be multiple local minimizers
of performative risk, motivated by situations where the initial conditions may
have significant impact on the long-term behavior of the system. We provide
sufficient conditions to characterize the region of attraction for the various
equilibria in this settings. Additionally, we introduce the notion of
performative alignment, which provides a geometric condition on the convergence
of repeated risk minimization to performative risk minimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Contrastive Approach to Online Change Point Detection <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Puchkin, Valeriia Shcherbakova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We suggest a novel procedure for online change point detection. Our approach
expands an idea of maximizing a discrepancy measure between points from
pre-change and post-change distributions. This leads to a flexible procedure
suitable for both parametric and nonparametric scenarios. We prove
non-asymptotic bounds on the average running length of the procedure and its
expected detection delay. The efficiency of the algorithm is illustrated with
numerical experiments on synthetic and real-world data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at AISTATS 2023; 28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DORA: Exploring outlier representations in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-Robert Müller, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) draw their power from the representations they
learn. However, while being incredibly effective in learning complex
abstractions, they are susceptible to learning malicious concepts, due to the
spurious correlations inherent in the training data. So far, existing methods
for uncovering such artifactual behavior in trained models focus on finding
artifacts in the input data, which requires both availability of a data set and
human supervision. In this paper, we introduce DORA (Data-agnOstic
Representation Analysis): the first data-agnostic framework for the analysis of
the representation space of DNNs. We propose a novel distance measure between
representations that utilizes self-explaining capabilities within the network
itself without access to any data and quantitatively validate its alignment
with human-defined semantic distances. We further demonstrate that this metric
could be utilized for the detection of anomalous representations, which may
bear a risk of learning unintended spurious concepts deviating from the desired
decision-making policy. Finally, we demonstrate the practical utility of DORA
by analyzing and identifying artifactual representations in widely popular
Computer Vision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Best of Many Worlds Guarantees for Online Learning with Knapsacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Celli, Matteo Castiglioni, Christian Kroer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study online learning problems in which a decision maker wants to maximize
their expected reward without violating a finite set of $m$ resource
constraints. By casting the learning process over a suitably defined space of
strategy mixtures, we recover strong duality on a Lagrangian relaxation of the
underlying optimization problem, even for general settings with non-convex
reward and resource-consumption functions. Then, we provide the first
best-of-many-worlds type framework for this setting, with no-regret guarantees
under stochastic, adversarial, and non-stationary inputs. Our framework yields
the same regret guarantees of prior work in the stochastic case. On the other
hand, when budgets grow at least linearly in the time horizon, it allows us to
provide a constant competitive ratio in the adversarial case, which improves
over the best known upper bound bound of $O(\log m \log T)$. Moreover, our
framework allows the decision maker to handle non-convex reward and cost
functions. We provide two game-theoretic applications of our framework to give
further evidence of its flexibility. In doing so, we show that it can be
employed to implement budget-pacing mechanisms in repeated first-price
auctions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-powered Course Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ermis Soumalias, Behnoosh Zamanlooy, Jakob Weissteiner, Sven Seuken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a machine learning-powered course allocation mechanism.
Concretely, we extend the state-of-the-art Course Match mechanism with a
machine learning-based preference elicitation module. In an iterative,
asynchronous manner, this module generates pairwise comparison queries that are
tailored to each individual student. Regarding incentives, our machine
learning-powered course match (MLCM) mechanism retains the attractive
strategyproofness in the large property of Course Match. Regarding welfare, we
perform computational experiments using a simulator that was fitted to
real-world data. Our results show that, compared to Course Match, MLCM
increases average student utility by 4%-9% and minimum student utility by
10%-21%, even with only ten comparison queries. Finally, we highlight the
practicability of MLCM and the ease of piloting it for universities currently
using Course Match.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GradMA: A Gradient-Memory-based Accelerated Federated Learning with
  Alleviated Catastrophic Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Xiang Li, Yunshi Lan, Ming Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a de facto machine learning area and
received rapid increasing research interests from the community. However,
catastrophic forgetting caused by data heterogeneity and partial participation
poses distinctive challenges for FL, which are detrimental to the performance.
To tackle the problems, we propose a new FL approach (namely GradMA), which
takes inspiration from continual learning to simultaneously correct the
server-side and worker-side update directions as well as take full advantage of
server's rich computing and memory resources. Furthermore, we elaborate a
memory reduction strategy to enable GradMA to accommodate FL with a large scale
of workers. We then analyze convergence of GradMA theoretically under the
smooth non-convex setting and show that its convergence rate achieves a linear
speed up w.r.t the increasing number of sampled active workers. At last, our
extensive experiments on various image classification tasks show that GradMA
achieves significant performance gains in accuracy and communication efficiency
compared to SOTA baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1906.07801v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1906.07801v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Grünwald, Rianne de Heide, Wouter Koolen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop the theory of hypothesis testing based on the e-value, a notion of
evidence that, unlike the p-value, allows for effortlessly combining results
from several studies in the common scenario where the decision to perform a new
study may depend on previous outcomes. Tests based on e-values are safe, i.e.
they preserve Type-I error guarantees, under such optional continuation. We
define growth-rate optimality (GRO) as an analogue of power in an optional
continuation context, and we show how to construct GRO e-variables for general
testing problems with composite null and alternative, emphasizing models with
nuisance parameters. GRO e-values take the form of Bayes factors with special
priors. We illustrate the theory using several classic examples including a
one-sample safe t-test and the 2 x 2 contingency table. Sharing Fisherian,
Neymanian and Jeffreys-Bayesian interpretations, e-values may provide a
methodology acceptable to adherents of all three schools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as discussion paper to the Journal of the Royal Statistical
  Society series B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seq2Seq Surrogates of Epidemic Models to Facilitate Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Charles, Timothy M. Wolock, Peter Winskill, Azra Ghani, Samir Bhatt, Seth Flaxman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Epidemic models are powerful tools in understanding infectious disease.
However, as they increase in size and complexity, they can quickly become
computationally intractable. Recent progress in modelling methodology has shown
that surrogate models can be used to emulate complex epidemic models with a
high-dimensional parameter space. We show that deep sequence-to-sequence
(seq2seq) models can serve as accurate surrogates for complex epidemic models
with sequence based model parameters, effectively replicating seasonal and
long-term transmission dynamics. Once trained, our surrogate can predict
scenarios a several thousand times faster than the original model, making them
ideal for policy exploration. We demonstrate that replacing a traditional
epidemic model with a learned simulator facilitates robust Bayesian inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skew Class-balanced Re-weighting for Unbiased Scene Graph <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An unbiased scene graph generation (SGG) algorithm referred to as Skew
Class-balanced Re-weighting (SCR) is proposed for considering the unbiased
predicate prediction caused by the long-tailed distribution. The prior works
focus mainly on alleviating the deteriorating performances of the minority
predicate predictions, showing drastic dropping recall scores, i.e., losing the
majority predicate performances. It has not yet correctly analyzed the
trade-off between majority and minority predicate performances in the limited
SGG datasets. In this paper, to alleviate the issue, the Skew Class-balanced
Re-weighting (SCR) loss function is considered for the unbiased SGG models.
Leveraged by the skewness of biased predicate predictions, the SCR estimates
the target predicate weight coefficient and then re-weights more to the biased
predicates for better trading-off between the majority predicates and the
minority ones. Extensive experiments conducted on the standard Visual Genome
dataset and Open Image V4 \& V6 show the performances and generality of the SCR
with the traditional SGG models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-One Laws of Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Adam-Day, Theodor Mihai Iliant, İsmail İlkan Ceylan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are de facto standard deep learning
architectures for machine learning on graphs. This has led to a large body of
work analyzing the capabilities and limitations of these models, particularly
pertaining to their representation and extrapolation capacity. We offer a novel
theoretical perspective on the representation and extrapolation capacity of
GNNs, by answering the question: how do GNNs behave as the number of graph
nodes become very large? Under mild assumptions, we show that when we draw
graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability
that such graphs are mapped to a particular output by a class of GNN
classifiers tends to either zero or to one. This class includes the popular
graph convolutional network architecture. The result establishes 'zero-one
laws' for these GNNs, and analogously to other convergence laws, entails
theoretical limitations on their capacity. We empirically verify our results,
observing that the theoretical asymptotic limits are evident already on
relatively small graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + references + 9 pages appendices, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A novel notion of barycenter for probability distributions based on
  optimal weak mass transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13380v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13380v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elsa Cazelles, Felipe Tobar, Joaquín Fontbona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce weak barycenters of a family of probability distributions, based
on the recently developed notion of optimal weak transport of mass by Gozlanet
al. (2017) and Backhoff-Veraguas et al. (2020). We provide a theoretical
analysis of this object and discuss its interpretation in the light of convex
ordering between probability measures. In particular, we show that, rather than
averaging the input distributions in a geometric way (as the Wasserstein
barycenter based on classic optimal transport does) weak barycenters extract
common geometric information shared by all the input distributions, encoded as
a latent random variable that underlies all of them. We also provide an
iterative algorithm to compute a weak barycenter for a finite family of input
distributions, and a stochastic algorithm that computes them for arbitrary
populations of laws. The latter approach is particularly well suited for the
streaming setting, i.e., when distributions are observed sequentially. The
notion of weak barycenter and our approaches to compute it are illustrated on
synthetic examples, validated on 2D real-world data and compared to standard
Wasserstein barycenters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SHINE: SHaring the INverse Estimate from the forward pass for bi-level
  optimization and implicit models <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.00553v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.00553v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, Thomas Moreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, implicit deep learning has emerged as a method to increase
the effective depth of deep neural networks. While their training is
memory-efficient, they are still significantly slower to train than their
explicit counterparts. In Deep Equilibrium Models (DEQs), the training is
performed as a bi-level problem, and its computational complexity is partially
driven by the iterative inversion of a huge Jacobian matrix. In this paper, we
propose a novel strategy to tackle this computational bottleneck from which
many bi-level problems suffer. The main idea is to use the quasi-Newton
matrices from the forward pass to efficiently approximate the inverse Jacobian
matrix in the direction needed for the gradient computation. We provide a
theorem that motivates using our method with the original forward algorithms.
In addition, by modifying these forward algorithms, we further provide
theoretical guarantees that our method asymptotically estimates the true
implicit gradient. We empirically study this approach and the recent
Jacobian-Free method in different settings, ranging from hyperparameter
optimization to large Multiscale DEQs (MDEQs) applied to CIFAR and ImageNet.
Both methods reduce significantly the computational cost of the backward pass.
While SHINE has a clear advantage on hyperparameter optimization problems, both
methods attain similar computational performances for larger scale problems
such as MDEQs at the cost of a limited performance drop compared to the
original models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a spotlight to ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Proximity-Aware Tasks for Embodied Social Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Cancelli, Tommaso Campari, Luciano Serafini, Angel X. Chang, Lamberto Ballan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning how to navigate among humans in an occluded and spatially
constrained indoor environment, is a key ability required to embodied agent to
be integrated into our society. In this paper, we propose an end-to-end
architecture that exploits Proximity-Aware Tasks (referred as to Risk and
Proximity Compass) to inject into a reinforcement learning navigation policy
the ability to infer common-sense social behaviors. To this end, our tasks
exploit the notion of immediate and future dangers of collision. Furthermore,
we propose an evaluation protocol specifically designed for the Social
Navigation Task in simulated environments. This is done to capture fine-grained
features and characteristics of the policy by analyzing the minimal unit of
human-robot spatial interaction, called Encounter. We validate our approach on
Gibson4+ and Habitat-Matterport3D datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EiX-GNN : Concept-level eigencentrality explainer for graph neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03491v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03491v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Raison, Pascal Bourdon, David Helbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, deep prediction models, especially graph neural networks, have a
majorplace in critical applications. In such context, those models need to be
highlyinterpretable or being explainable by humans, and at the societal scope,
this understandingmay also be feasible for humans that do not have a strong
prior knowledgein models and contexts that need to be explained. In the
literature, explainingis a human knowledge transfer process regarding a
phenomenon between an explainerand an explainee. We propose EiX-GNN
(Eigencentrality eXplainer forGraph Neural Networks) a new powerful method for
explaining graph neural networksthat encodes computationally this social
explainer-to-explainee dependenceunderlying in the explanation process. To
handle this dependency, we introducethe notion of explainee concept
assimibility which allows explainer to adapt itsexplanation to explainee
background or expectation. We lead a qualitative studyto illustrate our
explainee concept assimibility notion on real-world data as wellas a
qualitative study that compares, according to objective metrics established
inthe literature, fairness and compactness of our method with respect to
performingstate-of-the-art methods. It turns out that our method achieves
strong results inboth aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Relationship between Architecture and Adversarially Robust
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishan Liu, Shiyu Tang, Siyuan Liang, Ruihao Gong, Boxi Wu, Xianglong Liu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training has been demonstrated to be one of the most effective
remedies for defending adversarial examples, yet it often suffers from the huge
robustness generalization gap on unseen testing adversaries, deemed as the
adversarially robust generalization problem. Despite the preliminary
understandings devoted to adversarially robust generalization, little is known
from the architectural perspective. To bridge the gap, this paper for the first
time systematically investigated the relationship between adversarially robust
generalization and architectural design. Inparticular, we comprehensively
evaluated 20 most representative adversarially trained architectures on
ImageNette and CIFAR-10 datasets towards multiple `p-norm adversarial attacks.
Based on the extensive experiments, we found that, under aligned settings,
Vision Transformers (e.g., PVT, CoAtNet) often yield better adversarially
robust generalization while CNNs tend to overfit on specific attacks and fail
to generalize on multiple adversaries. To better understand the nature behind
it, we conduct theoretical analysis via the lens of Rademacher complexity. We
revealed the fact that the higher weight sparsity contributes significantly
towards the better adversarially robust generalization of Transformers, which
can be often achieved by the specially-designed attention blocks. We hope our
paper could help to better understand the mechanism for designing robust DNNs.
Our model weights can be found at http://robust.art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Reason for No Supervision: Improved Generalization in Supervised
  Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari, Diane Larlus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of training a deep neural network on a given
classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the
training task as well as at other (future) transfer tasks. These two seemingly
contradictory properties impose a trade-off between improving the model's
generalization and maintaining its performance on the original task. Models
trained with self-supervised learning tend to generalize better than their
supervised counterparts for transfer learning; yet, they still lag behind
supervised models on IN1K. In this paper, we propose a supervised learning
setup that leverages the best of both worlds. We extensively analyze supervised
training using multi-scale crops for data augmentation and an expendable
projector head, and reveal that the design of the projector allows us to
control the trade-off between performance on the training task and
transferability. We further replace the last layer of class weights with class
prototypes computed on the fly using a memory bank and derive two models: t-ReX
that achieves a new state of the art for transfer learning and outperforms top
methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly
optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code
and pretrained models: https://europe.naverlabs.com/t-rex
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The CMA Evolution Strategy: A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1604.00772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1604.00772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaus Hansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands
for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized,
method for real-parameter (continuous domain) optimization of non-linear,
non-convex functions. We try to motivate and derive the algorithm from
intuitive concepts and from requirements of non-linear, non-convex search in
continuous domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ArXiv e-prints, arXiv:1604.00772, 2016, pp.1-39</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multivariate Probabilistic Forecasting of Intraday Electricity Prices
  using Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13826v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13826v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eike Cramer, Dirk Witthaut, Alexander Mitsos, Manuel Dahmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electricity is traded on various markets with different time horizons and
regulations. Short-term intraday trading becomes increasingly important due to
the higher penetration of renewables. In Germany, the intraday electricity
price typically fluctuates around the day-ahead price of the European Power
EXchange (EPEX) spot markets in a distinct hourly pattern. This work proposes a
probabilistic modeling approach that models the intraday price difference to
the day-ahead contracts. The model captures the emerging hourly pattern by
considering the four 15 min intervals in each day-ahead price interval as a
four-dimensional joint probability distribution. The resulting nontrivial,
multivariate price difference distribution is learned using a normalizing flow,
i.e., a deep generative model that combines conditional multivariate density
estimation and probabilistic regression. Furthermore, this work discusses the
influence of different external impact factors based on literature insights and
impact analysis using explainable artificial intelligence (XAI). The
normalizing flow is compared to an informed selection of historical data and
probabilistic forecasts using a Gaussian copula and a Gaussian regression
model. Among the different models, the normalizing flow identifies the trends
with the highest accuracy and has the narrowest prediction intervals. Both the
XAI analysis and the empirical experiments highlight that the immediate history
of the price difference realization and the increments of the day-ahead price
have the most substantial impact on the price difference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>manuscript (20 pages, 11 figures, 5 tables), supporting information
  (8 pages, 5 figures, 4 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Spiking Neural Networks towards Deep Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Hu, Lei Deng, Yujie Wu, Man Yao, Guoqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid progress of neuromorphic computing, inadequate capacity and
insufficient representation power of spiking neural networks (SNNs) severely
restrict their application scope in practice. Residual learning and shortcuts
have been evidenced as an important approach for training deep neural networks,
but rarely did previous work assess their applicability to the characteristics
of spike-based communication and spatiotemporal dynamics. In this paper, we
first identify that this negligence leads to impeded information flow and the
accompanying degradation problem in previous residual SNNs. To address this
issue, we propose a novel SNN-oriented residual architecture termed MS-ResNet,
which establishes membrane-based shortcut pathways, and further prove that the
gradient norm equality can be achieved in MS-ResNet by introducing block
dynamical isometry theory, which ensures the network can be well-behaved in a
depth-insensitive way. Thus we are able to significantly extend the depth of
directly trained SNNs, e.g., up to 482 layers on CIFAR-10 and 104 layers on
ImageNet, without observing any slight degradation problem. To validate the
effectiveness of MS-ResNet, experiments on both frame-based and neuromorphic
datasets are conducted. MS-ResNet104 achieves a superior result of 76.02%
accuracy on ImageNet, which is the highest to our best knowledge in the domain
of directly trained SNNs. Great energy efficiency is also observed, with an
average of only one spike per neuron needed to classify an input sample. We
believe our powerful and scalable models will provide a strong support for
further exploration of SNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving and <span class="highlight-title">Loss</span>less Distributed Estimation of
  High-Dimensional Generalized Additive Mixed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schalk, Bernd Bischl, David Rügamer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various privacy-preserving frameworks that respect the individual's privacy
in the analysis of data have been developed in recent years. However, available
model classes such as simple statistics or generalized linear models lack the
flexibility required for a good approximation of the underlying data-generating
process in practice. In this paper, we propose an algorithm for a distributed,
privacy-preserving, and lossless estimation of generalized additive mixed
models (GAMM) using component-wise gradient boosting (CWB). Making use of CWB
allows us to reframe the GAMM estimation as a distributed fitting of base
learners using the $L_2$-loss. In order to account for the heterogeneity of
different data location sites, we propose a distributed version of a row-wise
tensor product that allows the computation of site-specific (smooth) effects.
Our adaption of CWB preserves all the important properties of the original
algorithm, such as an unbiased feature selection and the feasibility to fit
models in high-dimensional feature spaces, and yields equivalent model
estimates as CWB on pooled data. Next to a derivation of the equivalence of
both algorithms, we also showcase the efficacy of our algorithm on a
distributed heart disease data set and compare it with state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Sample Complexity of Two-Layer Networks: Lipschitz vs.
  Element-Wise Lipschitz Activation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Daniely, Elad Granot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the sample complexity of bounded two-layer neural networks
using different activation functions.
  In particular, we consider the class
  $$ \mathcal{H} = \left\{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ
W\textbf{b} + \textbf{b} \rangle : \textbf{b}\in\mathbb{R}^d, W \in
\mathbb{R}^{\mathcal{T}\times d}, \textbf{v} \in
\mathbb{R}^{\mathcal{T}}\right\} $$
  where the spectral norm of $W$ and $\textbf{v}$ is bounded by $O(1)$, the
Frobenius norm of $W$ is bounded from its initialization by $R > 0$, and
$\sigma$ is a Lipschitz activation function.
  We prove that if $\sigma$ is element-wise, then the sample complexity of
$\mathcal{H}$ has only logarithmic dependency in width and that this complexity
is tight, up to logarithmic factors.
  We further show that the element-wise property of $\sigma$ is essential for a
logarithmic dependency bound in width, in the sense that there exist
non-element-wise activation functions whose sample complexity is linear in
width, for widths that can be up to exponential in the input dimension.
  For the upper bound, we use the recent approach for norm-based bounds named
Approximate Description Length (ADL) by arXiv:1910.05697.
  We further develop new techniques and tools for this approach that will
hopefully inspire future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages with additional 15 pages of supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pistol: Pupil Invisible Supportive Tool to extract Pupil, Iris, Eye
  Opening, Eye Movements, Pupil and Iris Gaze Vector, and 2D as well as 3D Gaze 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wolfgang Fuhl, Daniel Weber, Shahram Eivazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a feature extraction and gaze estimation software, named
\textit{Pistol} that can be used with Pupil Invisible projects and other eye
trackers in the future. In offline mode, our software extracts multiple
features from the eye including, the pupil and iris ellipse, eye aperture,
pupil vector, iris vector, eye movement types from pupil and iris velocities,
marker detection, marker distance, 2D gaze estimation for the pupil center,
iris center, pupil vector, and iris vector using Levenberg Marquart fitting and
neural networks. The gaze signal is computed in 2D for each eye and each
feature separately and for both eyes in 3D also for each feature separately. We
hope this software helps other researchers to extract state-of-the-art features
for their research out of their recordings.
  Link:
https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FPISTOL&mode=list
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Security in Industry: A Quantitative <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Battista Biggio, Katharina Krombholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the large body of academic work on machine learning security, little
is known about the occurrence of attacks on machine learning systems in the
wild. In this paper, we report on a quantitative study with 139 industrial
practitioners. We analyze attack occurrence and concern and evaluate
statistical hypotheses on factors influencing threat perception and exposure.
Our results shed light on real-world attacks on deployed machine learning. On
the organizational level, while we find no predictors for threat exposure in
our sample, the amount of implement defenses depends on exposure to threats or
expected likelihood to become a target. We also provide a detailed analysis of
practitioners' replies on the relevance of individual machine learning attacks,
unveiling complex concerns like unreliable decision making, business
information leakage, and bias introduction into models. Finally, we find that
on the individual level, prior knowledge about machine learning security
influences threat perception. Our work paves the way for more research about
adversarial machine learning in practice, but yields also insights for
regulation and auditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TIFS, version with more detailed appendix containing more
  detailed statistical results. 17 pages, 6 tables and 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You Only Need End-to-End Training for Long-Tailed Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.05958v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.05958v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization gap on the long-tailed data sets is largely owing to most
categories only occupying a few training samples. Decoupled training achieves
better performance by training backbone and classifier separately. What causes
the poorer performance of end-to-end model training (e.g., logits margin-based
methods)? In this work, we identify a key factor that affects the learning of
the classifier: the channel-correlated features with low entropy before
inputting into the classifier. From the perspective of information theory, we
analyze why cross-entropy loss tends to produce highly correlated features on
the imbalanced data. In addition, we theoretically analyze and prove its
impacts on the gradients of classifier weights, the condition number of
Hessian, and logits margin-based approach. Therefore, we firstly propose to use
Channel Whitening to decorrelate ("scatter") the classifier's inputs for
decoupling the weight update and reshaping the skewed decision boundary, which
achieves satisfactory results combined with logits margin-based method.
However, when the number of minor classes are large, batch imbalance and more
participation in training cause over-fitting of the major classes. We also
propose two novel modules, Block-based Relatively Balanced Batch Sampler (B3RS)
and Batch Embedded Training (BET) to solve the above problems, which makes the
end-to-end training achieve even better performance than decoupled training.
Experimental results on the long-tailed classification benchmarks, CIFAR-LT and
ImageNet-LT, demonstrate the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a draft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Training of CNNs and <span class="highlight-title">Transformer</span>s with Coresets: A
  Stability Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Animesh Gupta, Irtiza Hasan, Dilip K. Prasad, Deepak K. Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coreset selection is among the most effective ways to reduce the training
time of CNNs, however, only limited is known on how the resultant models will
behave under variations of the coreset size, and choice of datasets and models.
Moreover, given the recent paradigm shift towards transformer-based models, it
is still an open question how coreset selection would impact their performance.
There are several similar intriguing questions that need to be answered for a
wide acceptance of coreset selection methods, and this paper attempts to answer
some of these. We present a systematic benchmarking setup and perform a
rigorous comparison of different coreset selection methods on CNNs and
transformers. Our investigation reveals that under certain circumstances,
random selection of subsets is more robust and stable when compared with the
SOTA selection methods. We demonstrate that the conventional concept of uniform
subset sampling across the various classes of the data is not the appropriate
choice. Rather samples should be adaptively chosen based on the complexity of
the data distribution for each class. Transformers are generally pretrained on
large datasets, and we show that for certain target datasets, it helps to keep
their performance stable at even very small coreset sizes. We further show that
when no pretraining is done or when the pretrained transformer models are used
with non-natural images (e.g. medical data), CNNs tend to generalize better
than transformers at even very small coreset sizes. Lastly, we demonstrate that
in the absence of the right pretraining, CNNs are better at learning the
semantic coherence between spatially distant objects within an image, and these
tend to outperform transformers at almost all choices of the coreset size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolutionary Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Bai, Ran Cheng, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) is a machine learning approach that trains agents
to maximize cumulative rewards through interactions with environments. The
integration of RL with deep learning has recently resulted in impressive
achievements in a wide range of challenging tasks, including board games,
arcade games, and robot control. Despite these successes, there remain several
crucial challenges, including brittle convergence properties caused by
sensitive hyperparameters, difficulties in temporal credit assignment with long
time horizons and sparse rewards, a lack of diverse exploration, especially in
continuous search space scenarios, difficulties in credit assignment in
multi-agent reinforcement learning, and conflicting objectives for rewards.
Evolutionary computation (EC), which maintains a population of learning agents,
has demonstrated promising performance in addressing these limitations. This
article presents a comprehensive survey of state-of-the-art methods for
integrating EC into RL, referred to as evolutionary reinforcement learning
(EvoRL). We categorize EvoRL methods according to key research fields in RL,
including hyperparameter optimization, policy search, exploration, reward
shaping, meta-RL, and multi-objective RL. We then discuss future research
directions in terms of efficient methods, benchmarks, and scalable platforms.
This survey serves as a resource for researchers and practitioners interested
in the field of EvoRL, highlighting the important challenges and opportunities
for future research. With the help of this survey, researchers and
practitioners can develop more efficient methods and tailored benchmarks for
EvoRL, further advancing this promising cross-disciplinary research field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Wang, Lu Chen, Bo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of 3D scene geometry decomposition and
manipulation from 2D views. By leveraging the recent implicit neural
representation techniques, particularly the appealing neural radiance fields,
we introduce an object field component to learn unique codes for all individual
objects in 3D space only from 2D supervision. The key to this component is a
series of carefully designed loss functions to enable every 3D point,
especially in non-occupied space, to be effectively optimized even without 3D
labels. In addition, we introduce an inverse query algorithm to freely
manipulate any specified 3D object shape in the learned scene representation.
Notably, our manipulation algorithm can explicitly tackle key issues such as
object collisions and visual occlusions. Our method, called DM-NeRF, is among
the first to simultaneously reconstruct, decompose, manipulate and render
complex 3D scenes in a single pipeline. Extensive experiments on three datasets
clearly show that our method can accurately decompose all 3D objects from 2D
views, allowing any interested object to be freely manipulated in 3D space such
as translation, rotation, size adjustment, and deformation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Our data and code are available at:
  https://github.com/vLAR-group/DM-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GFlowCausal: Generative Flow Networks for Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Li, Yinchuan Li, Shengyu Zhu, Yunfeng Shao, Jianye Hao, Yan Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery aims to uncover causal structure among a set of variables.
Score-based approaches mainly focus on searching for the best Directed Acyclic
Graph (DAG) based on a predefined score function. However, most of them are not
applicable on a large scale due to the limited searchability. Inspired by the
active learning in generative flow networks, we propose a novel approach to
learning a DAG from observational data called GFlowCausal. It converts the
graph search problem to a generation problem, in which direct edges are added
gradually. GFlowCausal aims to learn the best policy to generate high-reward
DAGs by sequential actions with probabilities proportional to predefined
rewards. We propose a plug-and-play module based on transitive closure to
ensure efficient sampling. Theoretical analysis shows that this module could
guarantee acyclicity properties effectively and the consistency between final
states and fully-connected graphs. We conduct extensive experiments on both
synthetic and real datasets, and results show the proposed approach to be
superior and also performs well in a large-scale setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Optimization of Energy Consumption and Completion Time in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhou, Jun Zhao, Huimei Han, Claude Guet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is an intriguing distributed machine learning
approach due to its privacy-preserving characteristics. To balance the
trade-off between energy and execution latency, and thus accommodate different
demands and application scenarios, we formulate an optimization problem to
minimize a weighted sum of total energy consumption and completion time through
two weight parameters. The optimization variables include bandwidth,
transmission power and CPU frequency of each device in the FL system, where all
devices are linked to a base station and train a global model collaboratively.
Through decomposing the non-convex optimization problem into two subproblems,
we devise a resource allocation algorithm to determine the bandwidth
allocation, transmission power, and CPU frequency for each participating
device. We further present the convergence analysis and computational
complexity of the proposed algorithm. Numerical results show that our proposed
algorithm not only has better performance at different weight parameters (i.e.,
different demands) but also outperforms the state of the art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper appears in the Proceedings of IEEE International
  Conference on Distributed Computing Systems (ICDCS) 2022. Please feel free to
  contact us for questions or remarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A numerical approximation method for the Fisher-Rao distance between
  multivariate normal distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08175v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08175v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple method to approximate Rao's distance between multivariate
normal distributions based on discretizing curves joining normal distributions
and approximating Rao distances between successive nearby normal distributions
on the curves by the square root of Jeffreys divergence. We consider
experimentally the linear interpolation curves in the ordinary, natural and
expectation parameterizations of the normal distributions, and compare these
curves with a curve derived from the Calvo and Oller's isometric embedding of
the Fisher-Rao $d$-variate normal manifold into the cone of $(d+1)\times (d+1)$
symmetric positive-definite matrices [Journal of multivariate analysis 35.2
(1990): 223-242]. We report on our experiments and assess the quality of our
approximation technique by comparing the numerical approximations with lower
and upper bounds. Finally, we present some information-geometric properties of
the Calvo and Oller's isometric embedding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 16 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit
  Detection & Emotional Reaction Intensity Estimation Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan Cowen, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part
of the respective ABAW Workshop which will be held in conjunction with IEEE
Computer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW
Competition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR
2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at
automatically analyzing affect. For this year's Competition, we feature two
corpora: i) an extended version of the Aff-Wild2 database and ii) the
Hume-Reaction dataset. The former database is an audiovisual one of around 600
videos of around 3M frames and is annotated with respect to:a) two continuous
affect dimensions -valence (how positive/negative a person is) and arousal (how
active/passive a person is)-; b) basic expressions (e.g. happiness, sadness,
neutral state); and c) atomic facial muscle actions (i.e., action units). The
latter dataset is an audiovisual one in which reactions of individuals to
emotional stimuli have been annotated with respect to seven emotional
expression intensities. Thus the 5th ABAW Competition encompasses four
Challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression
Classification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction
Intensity Estimation. In this paper, we present these Challenges, along with
their corpora, we outline the evaluation metrics, we present the baseline
systems and illustrate their obtained performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.10659</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Contention-Based Spectrum Access and Adaptive Modulation using
  Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.11723v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.11723v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Doshi, Jeffrey G. Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of unlicensed spectrum for cellular systems to mitigate spectrum
scarcity has led to the development of intelligent adaptive approaches to
spectrum access that improve upon traditional carrier sensing and
listen-before-talk methods. We study decentralized contention-based medium
access for base stations (BSs) of a single Radio Access Technology (RAT)
operating on unlicensed shared spectrum. We devise a distributed deep
reinforcement learning-based algorithm for both contention and adaptive
modulation, modelled on a two state Markov decision process, that attempts to
maximize a network-wide downlink throughput objective. Empirically, we find the
(proportional fairness) reward accumulated by a policy gradient approach to be
significantly higher than even a genie-aided adaptive energy detection
threshold. Our approaches are further validated by improved sum and peak
throughput. The scalability of our approach to large networks is demonstrated
via an improved cumulative reward earned on both indoor and outdoor layouts
with a large number of BSs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures. Published in Asilomar 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Task Recommendations with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multi-task Learning (MTL) has yielded immense success in
Recommender System (RS) applications. However, current MTL-based recommendation
models tend to disregard the session-wise patterns of user-item interactions
because they are predominantly constructed based on item-wise datasets.
Moreover, balancing multiple objectives has always been a challenge in this
field, which is typically avoided via linear estimations in existing works. To
address these issues, in this paper, we propose a Reinforcement Learning (RL)
enhanced MTL framework, namely RMTL, to combine the losses of different
recommendation tasks using dynamic weights. To be specific, the RMTL structure
can address the two aforementioned issues by (i) constructing an MTL
environment from session-wise interactions and (ii) training multi-task
actor-critic network structure, which is compatible with most existing
MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL
loss function using the weights generated by critic networks. Experiments on
two real-world public datasets demonstrate the effectiveness of RMTL with a
higher AUC against state-of-the-art MTL-based recommendation models.
Additionally, we evaluate and validate RMTL's compatibility and transferability
across various MTL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RawNet: Fast End-to-End Neural Vocoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1904.05351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1904.05351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchao He, Yujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network-based vocoders have recently demonstrated the powerful ability
to synthesize high-quality speech. These models usually generate samples by
conditioning on spectral features, such as Mel-spectrogram and fundamental
frequency, which is crucial to speech synthesis. However, the feature
extraction procession tends to depend heavily on human knowledge resulting in a
less expressive description of the origin audio. In this work, we proposed
RawNet, a complete end-to-end neural vocoder following the auto-encoder
structure for speaker-dependent and -independent speech synthesis. It
automatically learns to extract features and recover audio using neural
networks, which include a coder network to capture a higher representation of
the input audio and an autoregressive voder network to restore the audio in a
sample-by-sample manner. The coder and voder are jointly trained directly on
the raw waveform without any human-designed features. The experimental results
show that RawNet achieves a better speech quality using a simplified model
architecture and obtains a faster speech generation speed at the inference
stage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagnosing Model Performance Under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiffany Tianhui Cai, Hongseok Namkoong, Steve Yadlowsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction models can perform poorly when deployed to target distributions
different from the training distribution. To understand these operational
failure modes, we develop a method, called DIstribution Shift DEcomposition
(DISDE), to attribute a drop in performance to different types of distribution
shifts. Our approach decomposes the performance drop into terms for 1) an
increase in harder but frequently seen examples from training, 2) changes in
the relationship between features and outcomes, and 3) poor performance on
examples infrequent or unseen during training. These terms are defined by
fixing a distribution on $X$ while varying the conditional distribution of $Y
\mid X$ between training and target, or by fixing the conditional distribution
of $Y \mid X$ while varying the distribution on $X$. In order to do this, we
define a hypothetical distribution on $X$ consisting of values common in both
training and target, over which it is easy to compare $Y \mid X$ and thus
predictive performance. We estimate performance on this hypothetical
distribution via reweighting methods. Empirically, we show how our method can
1) inform potential modeling improvements across distribution shifts for
employment prediction on tabular census data, and 2) help to explain why
certain domain adaptation methods fail to improve model performance for
satellite image classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reproducible and Portable Big Data Analytics in the Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09762v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09762v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Pei Guo, Xingyan Li, Aryya Gangopadhyay, Carl E. Busart, Jade Freeman, Jianwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud computing has become a major approach to help reproduce computational
experiments. Yet there are still two main difficulties in reproducing batch
based big data analytics (including descriptive and predictive analytics) in
the cloud. The first is how to automate end-to-end scalable execution of
analytics including distributed environment provisioning, analytics pipeline
description, parallel execution, and resource termination. The second is that
an application developed for one cloud is difficult to be reproduced in another
cloud, a.k.a. vendor lock-in problem. To tackle these problems, we leverage
serverless computing and containerization techniques for automated scalable
execution and reproducibility, and utilize the adapter design pattern to enable
application portability and reproducibility across different clouds. We propose
and develop an open-source toolkit that supports 1) fully automated end-to-end
execution and reproduction via a single command, 2) automated data and
configuration storage for each execution, 3) flexible client modes based on
user preferences, 4) execution history query, and 5) simple reproduction of
existing executions in the same environment or a different environment. We did
extensive experiments on both AWS and Azure using four big data analytics
applications that run on virtual CPU/GPU clusters. The experiments show our
toolkit can achieve good execution performance, scalability, and efficient
reproducibility for cloud-based big data analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by journal IEEE Transactions on Cloud Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoSyn: Detecting Implicit Hate Speech in Online <span class="highlight-title">Conversation</span>s Using a
  Context Synergized Hyperbolic Network <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tremendous growth of social media users interacting in online
conversations has also led to significant growth in hate speech. Most of the
prior works focus on detecting explicit hate speech, which is overt and
leverages hateful phrases, with very little work focusing on detecting hate
speech that is implicit or denotes hatred through indirect or coded language.
In this paper, we present CoSyn, a user- and conversational-context synergized
network for detecting implicit hate speech in online conversation trees. CoSyn
first models the user's personal historical and social context using a novel
hyperbolic Fourier attention mechanism and hyperbolic graph convolution
network. Next, we jointly model the user's personal context and the
conversational context using a novel context interaction mechanism in the
hyperbolic space that clearly captures the interplay between the two and makes
independent assessments on the amounts of information to be retrieved from both
contexts. CoSyn performs all operations in the hyperbolic space to account for
the scale-free dynamics of social media. We demonstrate the effectiveness of
CoSyn both qualitatively and quantitatively on an open-source hate speech
dataset with Twitter conversations and show that CoSyn outperforms all our
baselines in detecting implicit hate speech with absolute improvements in the
range of 8.15% - 19.50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReAct: Synergizing <span class="highlight-title">Reasoning</span> and Acting in Language Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3 is the ICLR camera ready version with some typos fixed. Project
  site with code: https://react-lm.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QVRF: A Quantization-error-aware Variable Rate Framework for Learned
  Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kedeng Tong, Yaojun Wu, Yue Li, Kai Zhang, Li Zhang, Xin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression has exhibited promising compression performance,
but variable bitrates over a wide range remain a challenge. State-of-the-art
variable rate methods compromise the loss of model performance and require
numerous additional parameters. In this paper, we present a
Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a
univariate quantization regulator a to achieve wide-range variable rates within
a single model. Specifically, QVRF defines a quantization regulator vector
coupled with predefined Lagrange multipliers to control quantization error of
all latent representation for discrete variable rates. Additionally, the
reparameterization method makes QVRF compatible with a round quantizer.
Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods
equipped with QVRF can achieve wide-range continuous variable rates within a
single model without significant performance degradation. Furthermore, QVRF
outperforms contemporary variable-rate methods in rate-distortion performance
with minimal additional parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler
  and Multiple Choice Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-and-language understanding has a variety of applications in the
industry, such as video question answering, text-video retrieval and
multi-label classification. Existing video-and-language understanding methods
generally adopt heavy multi-modal encoders and feature fusion modules, which
consume large amounts of GPU memory. Especially, they have difficulty dealing
with dense video frames or long text that are prevalent in industrial
applications. In this paper, we propose MuLTI, a highly accurate and
memory-efficient video-and-language understanding model that achieves efficient
and effective feature fusion through feature sampling and attention modules.
Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we
introduce an attention-based adapter to the encoders, which finetunes the
shallow features to improve the model's performance with low GPU memory
consumption. Finally, to further improve the model's performance, we introduce
a new pretraining task named Multiple Choice Modeling to bridge the task gap
between pretraining and downstream tasks and enhance the model's ability to
align the video and the text. Benefiting from the efficient feature fusion
module, the attention-based adapter and the new pretraining task, MuLTI
achieves state-of-the-art performance on multiple datasets. Implementation and
pretrained models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Video Compression with Diverse Contexts <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Li, Bin Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any video codecs, the coding efficiency highly relies on whether the
current signal to be encoded can find the relevant contexts from the previous
reconstructed signals. Traditional codec has verified more contexts bring
substantial coding gain, but in a time-consuming manner. However, for the
emerging neural video codec (NVC), its contexts are still limited, leading to
low compression ratio. To boost NVC, this paper proposes increasing the context
diversity in both temporal and spatial dimensions. First, we guide the model to
learn hierarchical quality patterns across frames, which enriches long-term and
yet high-quality temporal contexts. Furthermore, to tap the potential of
optical flow-based coding framework, we introduce a group-based offset
diversity where the cross-group interaction is proposed for better context
mining. In addition, this paper also adopts a quadtree-based partition to
increase spatial context diversity when encoding the latent representation in
parallel. Experiments show that our codec obtains 23.5% bitrate saving over
previous SOTA NVC. Better yet, our codec has surpassed the under-developing
next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in
terms of PSNR. The codes are at https://github.com/microsoft/DCVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Audio-Visual Masked Autoencoder <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first extend the recent Masked Auto-Encoder (MAE) model
from a single modality to audio-visual multi-modalities. Subsequently, we
propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining
contrastive learning and masked data modeling, two major self-supervised
learning frameworks, to learn a joint and coordinated audio-visual
representation. Our experiments show that the contrastive audio-visual
correspondence learning objective not only enables the model to perform
audio-visual retrieval tasks, but also helps the model learn a better joint
representation. As a result, our fully self-supervised pretrained CAV-MAE
achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the
previous best supervised pretrained model on AudioSet in the audio-visual event
classification task. Code and pretrained models are at
https://github.com/yuangongnd/cav-mae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained
  models are at https://github.com/yuangongnd/cav-mae</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-09T00:00:00Z">2023-03-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning with Large Language Models for Code <span class="highlight-title">Generation</span> <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language model-based code generation pipelines typically use
beam search or sampling algorithms during the decoding process. Although the
programs they generate achieve high token-matching-based scores, they often
fail to compile or generate incorrect outputs. The main reason is that
conventional Transformer decoding algorithms may not be the best choice for
code generation. In this work, we propose a novel Transformer decoding
algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning
algorithm to do lookahead search and guide the Transformer to generate better
programs. Specifically, instead of simply optimizing the likelihood of the
generated sequences, the Transformer makes use of a planner to generate
candidate programs and test them on public test cases. The Transformer can
therefore make more informed decisions and generate tokens that will eventually
lead to higher-quality programs. We also design a mechanism that shares
information between the Transformer and the planner to make our algorithm
computationally efficient. We empirically evaluate our framework with several
large language models as backbones on public coding challenge benchmarks,
showing that 1) it can generate programs that consistently achieve higher
performance compared with competing baseline methods; 2) it enables
controllable code generation, such as concise codes and highly-commented codes
by optimizing modified objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Project page:https://codeaimcts.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Persona</span>lisation within bounds: A risk taxonomy and policy framework for
  the alignment of large language models with <span class="highlight-title">persona</span>lised feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are used to generate content for a wide range of
tasks, and are set to reach a growing audience in coming years due to
integration in product interfaces like ChatGPT or search engines like Bing.
This intensifies the need to ensure that models are aligned with human
preferences and do not produce unsafe, inaccurate or toxic outputs. While
alignment techniques like reinforcement learning with human feedback (RLHF) and
red-teaming can mitigate some safety concerns and improve model capabilities,
it is unlikely that an aggregate fine-tuning process can adequately represent
the full range of users' preferences and values. Different people may
legitimately disagree on their preferences for language and conversational
norms, as well as on values or ideologies which guide their communication.
Personalising LLMs through micro-level preference learning processes may result
in models that are better aligned with each user. However, there are several
normative challenges in defining the bounds of a societally-acceptable and safe
degree of personalisation. In this paper, we ask how, and in what ways, LLMs
should be personalised. First, we review literature on current paradigms for
aligning LLMs with human feedback, and identify issues including (i) a lack of
clarity regarding what alignment means; (ii) a tendency of technology providers
to prescribe definitions of inherently subjective preferences and values; and
(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in
who we are really aligning to. Second, we present a taxonomy of benefits and
risks associated with personalised LLMs, for individuals and society at large.
Finally, we propose a three-tiered policy framework that allows users to
experience the benefits of personalised alignment, while restraining unsafe and
undesirable LLM-behaviours within (supra-)national and organisational bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing <span class="highlight-title">ChatGPT</span> Through Students' Eyes: An Analysis of TikTok Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna-Carolina Haensch, Sarah Ball, Markus Herklotz, Frauke Kreuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced large language models like ChatGPT have gained considerable
attention recently, including among students. However, while the debate on
ChatGPT in academia is making waves, more understanding is needed among
lecturers and teachers on how students use and perceive ChatGPT. To address
this gap, we analyzed the content on ChatGPT available on TikTok in February
2023. TikTok is a rapidly growing social media platform popular among
individuals under 30. Specifically, we analyzed the content of the 100 most
popular videos in English tagged with #chatgpt, which collectively garnered
over 250 million views. Most of the videos we studied promoted the use of
ChatGPT for tasks like writing essays or code. In addition, many videos
discussed AI detectors, with a focus on how other tools can help to transform
ChatGPT output to fool these detectors. This also mirrors the discussion among
educators on how to treat ChatGPT as lecturers and teachers in teaching and
grading. What is, however, missing from the analyzed clips on TikTok are videos
that discuss ChatGPT producing content that is nonsensical or unfaithful to the
training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replacement as a Self-supervision for Fine-grained Vision-language
  <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisai Zhang, Qingcai Chen, Zhijian Chen, Yunpeng Han, Zhonghua Li, Zhao Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained supervision based on object annotations has been widely used for
vision and language pre-training (VLP). However, in real-world application
scenarios, aligned multi-modal data is usually in the image-caption format,
which only provides coarse-grained supervision. It is cost-expensive to collect
object annotations and build object annotation pre-extractor for different
scenarios. In this paper, we propose a fine-grained self-supervision signal
without object annotations from a replacement perspective. First, we propose a
homonym sentence rewriting (HSR) algorithm to provide token-level supervision.
The algorithm replaces a verb/noun/adjective/quantifier word of the caption
with its homonyms from WordNet. Correspondingly, we propose a replacement
vision-language modeling (RVLM) framework to exploit the token-level
supervision. Two replaced modeling tasks, i.e., replaced language contrastive
(RLC) and replaced language modeling (RLM), are proposed to learn the
fine-grained alignment. Extensive experiments on several downstream tasks
demonstrate the superior performance of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup
  for Visual Speech Translation and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xize Cheng, Linjun Li, Tao Jin, Rongjie Huang, Wang Lin, Zehan Wang, Huangdai Liu, Ye Wang, Aoxiong Yin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-media communications facilitate global interaction among people.
However, despite researchers exploring cross-lingual translation techniques
such as machine translation and audio speech translation to overcome language
barriers, there is still a shortage of cross-lingual studies on visual speech.
This lack of research is mainly due to the absence of datasets containing
visual speech and translated text pairs. In this paper, we present
\textbf{AVMuST-TED}, the first dataset for \textbf{A}udio-\textbf{V}isual
\textbf{Mu}ltilingual \textbf{S}peech \textbf{T}ranslation, derived from
\textbf{TED} talks. Nonetheless, visual speech is not as distinguishable as
audio speech, making it difficult to develop a mapping from source speech
phonemes to the target language text. To address this issue, we propose
MixSpeech, a cross-modality self-learning framework that utilizes audio speech
to regularize the training of visual speech tasks. To further minimize the
cross-modality gap and its impact on knowledge transfer, we suggest adopting
mixed speech, which is created by interpolating audio and visual streams, along
with a curriculum learning strategy to adjust the mixing ratio as needed.
MixSpeech enhances speech translation in noisy environments, improving BLEU
scores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves
state-of-the-art performance in lip reading on CMLR (11.1\%), LRS2 (25.5\%),
and LRS3 (28.0\%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Exgc/AVMuST-TED</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Stashing Quantization for Efficient <span class="highlight-title">Transformer</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guo Yang, Daniel Lo, Robert Mullins, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive performance on a
range of Natural Language Processing (NLP) tasks. Unfortunately, the immense
amount of computations and memory accesses required for LLM training makes them
prohibitively expensive in terms of hardware cost, and thus challenging to
deploy in use cases such as on-device learning. In this paper, motivated by the
observation that LLM training is memory-bound, we propose a novel dynamic
quantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a
special focus on reducing the memory operations, but also enjoys the other
benefits of low precision training, such as the reduced arithmetic cost. We
conduct a thorough study on two translation tasks (trained-from-scratch) and
three classification tasks (fine-tuning). DSQ reduces the amount of arithmetic
operations by $20.95\times$ and the number of DRAM operations by $2.55\times$
on IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in
on-device learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAM: An Integrated Activation-Coupled Model of Sentence Processing and
  Eye Movements in Reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian M. Rabe, Dario Paape, Daniela Mertzen, Shravan Vasishth, Ralf Engbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models of eye-movement control during reading, developed largely within
psychology, usually focus on visual, attentional, and motor processes but
neglect post-lexical language processing; by contrast, models of sentence
comprehension processes, developed largely within psycholinguistics, generally
focus only on post-lexical language processes. We present a model that combines
these two research threads, by integrating eye-movement control and sentence
processing. Developing such an integrated model is extremely challenging and
computationally demanding, but such an integration is an important step toward
complete mathematical models of natural language comprehension in reading. We
combine the SWIFT model of eye-movement control (Engbert et al., Psychological
Review, 112, 2005, pp. 777-813) with key components of the Lewis and Vasishth
sentence processing model (Lewis and Vasishth, Cognitive Science, 29, 2005, pp.
375-419). This integration becomes possible, for the first time, due in part to
recent advances in successful parameter identification in dynamical models,
which allows us to investigate profile log-likelihoods for individual model
parameters. We present a fully implemented proof-of-concept model demonstrating
how such an integrated model can be achieved; our approach includes Bayesian
model inference with Markov Chain Monte Carlo (MCMC) sampling as a key
computational tool. The integrated model, SEAM, can successfully reproduce eye
movement patterns that arise due to similarity-based interference in reading.
To our knowledge, this is the first-ever integration of a complete process
model of eye-movement control with linguistic dependency completion processes
in sentence comprehension. In future work, this proof of concept model will
need to be evaluated using a comprehensive set of benchmark data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry of Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loe Feijs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present a fresh perspective on language, combining ideas
from various sources, but mixed in a new synthesis. As in the minimalist
program, the question is whether we can formulate an elegant formalism, a
universal grammar or a mechanism which explains significant aspects of the
human faculty of language, which in turn can be considered a natural
disposition for the evolution and deployment of the diverse human languages. We
describe such a mechanism, which differs from existing logical and grammatical
approaches by its geometric nature. Our main contribution is to explore the
assumption that sentence recognition takes place by forming chains of tokens
representing words, followed by matching these chains with pre-existing chains
representing grammatical word orders. The aligned chains of tokens give rise to
two- and three-dimensional complexes. The resulting model gives an alternative
presentation for subtle rules, traditionally formalized using categorial
grammar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $π$-augmented pregroups and applications to linguistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Boboc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We enrich pregroups with a mapping which allows us to locally apply precyclic
permutations to designated substrings. We prove a normalisation theorem for
such algebraic structures and briefly formalise some known applications of
pregroups to the analysis of clitic pronouns in certain natural languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can a Frozen <span class="highlight-title">Pretrain</span>ed Language Model be used for Zero-shot Neural
  Retrieval on Entity-centric Questions? <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasuto Hoshi, Daisuke Miyashita, Yasuhiro Morioka, Youyang Ng, Osamu Torii, Jun Deguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural document retrievers, including dense passage retrieval (DPR), have
outperformed classical lexical-matching retrievers, such as BM25, when
fine-tuned and tested on specific question-answering datasets. However, it has
been shown that the existing dense retrievers do not generalize well not only
out of domain but even in domain such as Wikipedia, especially when a named
entity in a question is a dominant clue for retrieval. In this paper, we
propose an approach toward in-domain generalization using the embeddings
generated by the frozen language model trained with the entities in the domain.
By not fine-tuning, we explore the possibility that the rich knowledge
contained in a pretrained language model can be used for retrieval tasks. The
proposed method outperforms conventional DPRs on entity-centric questions in
Wikipedia domain and achieves almost comparable performance to BM25 and
state-of-the-art SPAR model. We also show that the contextualized keys lead to
strong improvements compared to BM25 when the entity names consist of common
words. Our results demonstrate the feasibility of the zero-shot retrieval
method for entity-centric questions of Wikipedia domain, where DPR has
struggled to perform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Workshop on Knowledge Augmented Methods for Natural
  Language Processing, in conjunction with AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESCL: Equivariant Self-Contrastive Learning for Sentence Representations <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Yixuan Liu, Xue Han, Chao Deng, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous contrastive learning methods for sentence representations often
focus on insensitive transformations to produce positive pairs, but neglect the
role of sensitive transformations that are harmful to semantic representations.
Therefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to
make full use of sensitive transformations, which encourages the learned
representations to be sensitive to certain types of transformations with an
additional equivariant learning task. Meanwhile, in order to improve
practicability and generality, ESCL simplifies the implementations of
traditional equivariant contrastive methods to share model parameters from the
perspective of multi-task learning. We evaluate our ESCL on semantic textual
similarity tasks. The proposed method achieves better results while using fewer
learning parameters compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Video Retrieval by Adaptive Margin <span class="chip">SIGIR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng He, Qi Wang, Zhifan Feng, Wenbin Jiang, Yajuan Lv, Yong zhu, Xiao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video retrieval is becoming increasingly important owing to the rapid
emergence of videos on the Internet. The dominant paradigm for video retrieval
learns video-text representations by pushing the distance between the
similarity of positive pairs and that of negative pairs apart from a fixed
margin. However, negative pairs used for training are sampled randomly, which
indicates that the semantics between negative pairs may be related or even
equivalent, while most methods still enforce dissimilar representations to
decrease their similarity. This phenomenon leads to inaccurate supervision and
poor performance in learning video-text representations.
  While most video retrieval methods overlook that phenomenon, we propose an
adaptive margin changed with the distance between positive and negative pairs
to solve the aforementioned issue. First, we design the calculation framework
of the adaptive margin, including the method of distance measurement and the
function between the distance and the margin. Then, we explore a novel
implementation called "Cross-Modal Generalized Self-Distillation" (CMGSD),
which can be built on the top of most video retrieval models with few
modifications. Notably, CMGSD adds few computational overheads at train time
and adds no computational overhead at test time. Experimental results on three
widely used datasets demonstrate that the proposed method can yield
significantly better performance than the corresponding backbone model, and it
outperforms state-of-the-art methods by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang, Bin Ji, Shasha Li, Jun Ma, Long Peng, Jie Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many studies incorporate external knowledge into character-level
feature based models to improve the performance of Chinese relation extraction.
However, these methods tend to ignore the internal information of the Chinese
character and cannot filter out the noisy information of external knowledge. To
address these issues, we propose a mixture-of-view-experts framework (MoVE) to
dynamically learn multi-view features for Chinese relation extraction. With
both the internal and external knowledge of Chinese characters, our framework
can better capture the semantic information of Chinese characters. To
demonstrate the effectiveness of the proposed framework, we conduct extensive
experiments on three real-world datasets in distinct domains. Experimental
results show consistent and significant superiority and robustness of our
proposed framework. Our code and dataset will be released at:
https://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by PAKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the relevance of traditional genres: a network analysis of
  fiction readers' preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taom Sakal, Stephen Proulx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how well traditional fiction genres like Fantasy, Thriller,
and Literature represent readers' preferences. Using user data from Goodreads
we construct a book network where two books are strongly linked if the same
people tend to read or enjoy them both. We then partition this network into
communities of similar books and assign each a list of subjects from The Open
Library to serve as a proxy for traditional genres. Our analysis reveals that
the network communities correspond to existing combinations of traditional
genres, but that the exact communities differ depending on whether we consider
books that people read or books that people enjoy.
  In addition, we apply principal component analysis to the data and find that
the variance in the book communities is best explained by two factors: the
maturity/childishness and realism/fantastical nature of the books. We propose
using this maturity-realism plane as a coarse classification tool for stories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary materials at https://github.com/taomsakal/book-networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Legibility of Visual Text Perturbations <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dev Seth, Rickard Stureborg, Danish Pruthi, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many adversarial attacks in NLP perturb inputs to produce visually similar
strings ('ergo' $\rightarrow$ '$\epsilon$rgo') which are legible to humans but
degrade model performance. Although preserving legibility is a necessary
condition for text perturbation, little work has been done to systematically
characterize it; instead, legibility is typically loosely enforced via
intuitions around the nature and extent of perturbations. Particularly, it is
unclear to what extent can inputs be perturbed while preserving legibility, or
how to quantify the legibility of a perturbed string. In this work, we address
this gap by learning models that predict the legibility of a perturbed string,
and rank candidate perturbations based on their legibility. To do so, we
collect and release \dataset, a human-annotated dataset comprising the
legibility of visually perturbed text. Using this dataset, we build both text-
and vision-based models which achieve up to $0.91$ F1 score in predicting
whether an input is legible, and an accuracy of $0.86$ in predicting which of
two given perturbations is more legible. Additionally, we discover that legible
perturbations from the \dataset dataset are more effective at lowering the
performance of NLP models than best-known attack strategies, suggesting that
current models may be vulnerable to a broad range of perturbations beyond what
is captured by existing visual attacks. Data, code, and models are available at
https://github.com/dvsth/learning-legibility-2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures. Long paper at EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for
  Document Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated
remarkable results in various natural language processing (NLP) tasks with
in-context learning, which involves inference based on a few demonstration
examples. Despite their successes in NLP tasks, no investigation has been
conducted to assess the ability of LLMs to perform document information
extraction (DIE) using in-context learning. Applying LLMs to DIE poses two
challenges: the modality and task gap. To this end, we propose a simple but
effective in-context learning framework called ICL-D3IE, which enables LLMs to
perform DIE with different types of demonstration examples. Specifically, we
extract the most difficult and distinct segments from hard training documents
as hard demonstrations for benefiting all test instances. We design
demonstrations describing relationships that enable LLMs to understand
positional relationships. We introduce formatting demonstrations for easy
answer extraction. Additionally, the framework improves diverse demonstrations
by updating them iteratively. Our experiments on three widely used benchmark
datasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to
achieve superior performance when compared to previous pre-trained methods
fine-tuned with full training in both the in-distribution (ID) setting and in
the out-of-distribution (OOD) setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Language agnostic WER Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satarupa Guha, Rahul Ambavat, Ankur Gupta, Manish Gupta, Rupeshkumar Mehta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word error rate (WER) is a standard metric for the evaluation of Automated
Speech Recognition (ASR) systems. However, WER fails to provide a fair
evaluation of human perceived quality in presence of spelling variations,
abbreviations, or compound words arising out of agglutination. Multiple
spelling variations might be acceptable based on locale/geography, alternative
abbreviations, borrowed words, and transliteration of code-mixed words from a
foreign language to the target language script. Similarly, in case of
agglutination, often times the agglutinated, as well as the split forms, are
acceptable. Previous work handled this problem by using manually identified
normalization pairs and applying them to both the transcription and the
hypothesis before computing WER. In this paper, we propose an automatic WER
normalization system consisting of two modules: spelling normalization and
segmentation normalization. The proposed system is unsupervised and language
agnostic, and therefore scalable. Experiments with ASR on 35K utterances across
four languages yielded an average WER reduction of 13.28%. Human judgements of
these automatically identified normalization pairs show that our WER-normalized
evaluation is highly consistent with the perceived quality of ASR output.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Coarse-to-Fine Contrastive Learning for <span class="highlight-title">Conversation</span> Intent
  Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caiyuan Chu, Ya Li, Yifan Liu, Jia-Chen Gu, Quan Liu, Yongxin Ge, Guoping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent recognition is critical for task-oriented dialogue systems. However,
for emerging domains and new services, it is difficult to accurately identify
the key intent of a conversation due to time-consuming data annotation and
comparatively poor model transferability. Therefore, the automatic induction of
dialogue intention is very important for intelligent dialogue systems. This
paper presents our solution to Track 2 of Intent Induction from Conversations
for Task-Oriented Dialogue at the Eleventh Dialogue System Technology Challenge
(DSTC11). The essence of intention clustering lies in distinguishing the
representation of different dialogue utterances. The key to automatic intention
induction is that, for any given set of new data, the sentence representation
obtained by the model can be well distinguished from different labels.
Therefore, we propose a multi-stage coarse-to-fine contrastive learning model
training scheme including unsupervised contrastive learning pre-training,
supervised contrastive learning pre-training, and fine-tuning with joint
contrastive learning and clustering to obtain a better dialogue utterance
representation model for the clustering task. In the released DSTC11 Track 2
evaluation results, our proposed system ranked first on both of the two
subtasks of this Track.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ranked 1st on Track 2 at DSTC 11, Accepted by DSTC 11 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's Get <span class="highlight-title">Persona</span>l: <span class="highlight-title">Persona</span>l Questions Improve SocialBot Performance in
  the Alexa Prize 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin K. Bowden, Marilyn Walker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been an increased focus on creating conversational open-domain
dialogue systems in the spoken dialogue community. Unlike traditional dialogue
systems, these conversational systems cannot assume any specific information
need or domain restrictions, i.e., the only inherent goal is to converse with
the user on an unknown set of topics. While massive improvements in Natural
Language Understanding (NLU) and the growth of available knowledge resources
can partially support a robust conversation, these conversations generally lack
the rapport between two humans that know each other. We developed a robust
open-domain conversational system, Athena, that real Amazon Echo users access
and evaluate at scale in the context of the Alexa Prize competition. We
experiment with methods intended to increase intimacy between Athena and the
user by heuristically developing a rule-based user model that personalizes both
the current and subsequent conversations and evaluating specific personal
opinion question strategies in A/B studies. Our results show a statistically
significant positive impact on perceived conversation quality and length when
employing these strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Won Best Paper at IWSDS '23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open World Classification with Adaptive Negative Samples <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Bai, Guoyin Wang, Jiwei Li, Sunghyun Park, Sungjin Lee, Puyang Xu, Ricardo Henao, Lawrence Carin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open world classification is a task in natural language processing with key
practical relevance and impact. Since the open or {\em unknown} category data
only manifests in the inference phase, finding a model with a suitable decision
boundary accommodating for the identification of known classes and
discrimination of the open category is challenging. The performance of existing
models is limited by the lack of effective open category data during the
training stage or the lack of a good mechanism to learn appropriate decision
boundaries. We propose an approach based on \underline{a}daptive
\underline{n}egative \underline{s}amples (ANS) designed to generate effective
synthetic open category samples in the training stage and without requiring any
prior knowledge or external datasets. Empirically, we find a significant
advantage in using auxiliary one-versus-rest binary classifiers, which
effectively utilize the generated negative samples and avoid the complex
threshold-seeking stage in previous works. Extensive experiments on three
benchmark datasets show that ANS achieves significant improvements over
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2021 (Main Track, Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Robustness of Text Vectorizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Catellier, Samuel Vaiter, Damien Garreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental issue in natural language processing is the robustness of the
models with respect to changes in the input. One critical step in this process
is the embedding of documents, which transforms sequences of words or tokens
into vector representations. Our work formally proves that popular embedding
schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec),
exhibit robustness in the H\"older or Lipschitz sense with respect to the
Hamming distance. We provide quantitative bounds for these schemes and
demonstrate how the constants involved are affected by the length of the
document. These findings are exemplified through a series of numerical
examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeamAttack: Generating High-quality Textual Adversarial Examples through
  Beam Search and Mixed Semantic Spaces <span class="chip">PAKDD2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Zhu, Qingyang Zhao, Yuren Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing models based on neural networks are vulnerable to
adversarial examples. These adversarial examples are imperceptible to human
readers but can mislead models to make the wrong predictions. In a black-box
setting, attacker can fool the model without knowing model's parameters and
architecture. Previous works on word-level attacks widely use single semantic
space and greedy search as a search strategy. However, these methods fail to
balance the attack success rate, quality of adversarial examples and time
consumption. In this paper, we propose BeamAttack, a textual attack algorithm
that makes use of mixed semantic spaces and improved beam search to craft
high-quality adversarial examples. Extensive experiments demonstrate that
BeamAttack can improve attack success rate while saving numerous queries and
time, e.g., improving at most 7\% attack success rate than greedy search when
attacking the examples from MR dataset. Compared with heuristic search,
BeamAttack can save at most 85\% model queries and achieve a competitive attack
success rate. The adversarial examples crafted by BeamAttack are highly
transferable and can effectively improve model's robustness during adversarial
training. Code is available at
https://github.com/zhuhai-ustc/beamattack/tree/master
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PAKDD2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Feasibility of <span class="highlight-title">ChatGPT</span> for Event Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Gao, Huan Zhao, Changlong Yu, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction is a fundamental task in natural language processing that
involves identifying and extracting information about events mentioned in text.
However, it is a challenging task due to the lack of annotated data, which is
expensive and time-consuming to obtain. The emergence of large language models
(LLMs) such as ChatGPT provides an opportunity to solve language tasks with
simple prompts without the need for task-specific datasets and fine-tuning.
While ChatGPT has demonstrated impressive results in tasks like machine
translation, text summarization, and question answering, it presents challenges
when used for complex tasks like event extraction. Unlike other tasks, event
extraction requires the model to be provided with a complex set of instructions
defining all event types and their schemas. To explore the feasibility of
ChatGPT for event extraction and the challenges it poses, we conducted a series
of experiments. Our results show that ChatGPT has, on average, only 51.04% of
the performance of a task-specific model such as EEQA in long-tail and complex
scenarios. Our usability testing experiments indicate that ChatGPT is not
robust enough, and continuous refinement of the prompt does not lead to stable
performance improvements, which can result in a poor user experience. Besides,
ChatGPT is highly sensitive to different prompt styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the state-of-the-art of hybrid language models
architectures and strategies for "complex" question-answering (QA, CQA, CPS).
Large Language Models (LLM) are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, methods,
sensitive data protection, explainability, human approval and versatile
feedback... We identify key elements augmenting LLM to solve complex questions
or problems. We extend findings from the robust community edited research
papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and
challenges of LLM in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like
ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential
as well as the equally strong limitations of language models in complex QA.
Hybridizing these models with different components could allow to overcome
these different limits and go much further. We discuss some challenges
associated with complex QA, including domain adaptation, decomposition and
efficient multi-step QA, long form and non-factoid QA, safety and
multi-sensitivity data protection, multimodal search, hallucinations,
explainability and truthfulness, temproal reasoning. Therefore, we analyze
current solutions and promising research trends, using elements such as: hybrid
LLM architectures, active human reinforcement learning supervised with AI,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, iterated decomposition and others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELODIN: Naming Concepts in Embedding Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mello, Filipe Calegario, Geber Ramalho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements, the field of text-to-image synthesis still
suffers from lack of fine-grained control. Using only text, it remains
challenging to deal with issues such as concept coherence and concept
contamination. We propose a method to enhance control by generating specific
concepts that can be reused throughout multiple images, effectively expanding
natural language with new words that can be combined much like a painter's
palette. Unlike previous contributions, our method does not copy visuals from
input data and can generate concepts through text alone. We perform a set of
comparisons that finds our method to be a significant improvement over
text-only prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added quantitative data, fixed formatting issues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearly Mapping from Image to Text Space <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extent to which text-only language models (LMs) learn to represent
features of the non-linguistic world is an open question. Prior work has shown
that pretrained LMs can be taught to caption images when a vision model's
parameters are optimized to encode images in the language space. We test a
stronger hypothesis: that the conceptual representations learned by frozen
text-only models and vision-only models are similar enough that this can be
achieved with a linear map. We show that the image representations from vision
models can be transferred as continuous prompts to frozen LMs by training only
a single linear projection. Using these to prompt the LM achieves competitive
performance on captioning and visual question answering tasks compared to
models that tune both the image encoder and text decoder (such as the MAGMA
model). We compare three image encoders with increasing amounts of linguistic
supervision seen during pretraining: BEIT (no linguistic information),
NF-ResNET (lexical category information), and CLIP (full natural language
descriptions). We find that all three encoders perform equally well at
transferring visual property information to the language model (e.g., whether
an animal is large or small), but that image encoders pretrained with
linguistic supervision more saliently encode category information (e.g.,
distinguishing hippo vs. elephant) and thus perform significantly better on
benchmark language-and-vision tasks. Our results indicate that LMs encode
conceptual information structurally similarly to vision-based models, even
those that are solely trained on images. Code is available here:
https://github.com/jmerullo/limber
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedKLIP: Medical <span class="highlight-title">Knowledge</span> Enhanced Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider enhancing medical visual-language pre-training
(VLP) with domain-specific knowledge, by exploiting the paired image-text
reports from the radiological daily practice. In particular, we make the
following contributions: First, unlike existing works that directly process the
raw reports, we adopt a novel triplet extraction module to extract the
medical-related information, avoiding unnecessary complexity from language
grammar and enhancing the supervision signals; Second, we propose a novel
triplet encoding module with entity translation by querying a knowledge base,
to exploit the rich domain knowledge in medical field, and implicitly build
relationships between medical entities in the language embedding space; Third,
we propose to use a Transformer-based fusion model for spatially aligning the
entity description with visual signals at the image patch level, enabling the
ability for medical diagnosis; Fourth, we conduct thorough experiments to
validate the effectiveness of our architecture, and benchmark on numerous
public benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,
COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning
settings, our model has demonstrated strong performance compared with the
former methods on disease classification and grounding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic
  Forgetting in Automatic Speech Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vander Eeckt, Hugo Van hamme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting a trained Automatic Speech Recognition (ASR) model to new tasks
results in catastrophic forgetting of old tasks, limiting the model's ability
to learn continually and to be extended to new speakers, dialects, languages,
etc. Focusing on End-to-End ASR, in this paper, we propose a simple yet
effective method to overcome catastrophic forgetting: weight averaging. By
simply taking the average of the previous and the adapted model, our method
achieves high performance on both the old and new tasks. It can be further
improved by introducing a knowledge distillation loss during the adaptation. We
illustrate the effectiveness of our method on both monolingual and multilingual
ASR. In both cases, our method strongly outperforms all baselines, even in its
simplest form.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023. 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning for Monolingual End-to-End Automatic Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09427v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09427v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vander Eeckt, Hugo Van hamme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Automatic Speech Recognition (ASR) models to new domains results in
a deterioration of performance on the original domain(s), a phenomenon called
Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to
new accents, dialects, topics, etc. without suffering from CF, making them
unable to be continually enhanced without storing all past data. Fortunately,
Continual Learning (CL) methods, which aim to enable continual adaptation while
overcoming CF, can be used. In this paper, we implement an extensive number of
CL methods for End-to-End ASR and test and compare their ability to extend a
monolingual Hybrid CTC-Transformer model across four new tasks. We find that
the best performing CL method closes the gap between the fine-tuned model
(lower bound) and the model trained jointly on all tasks (upper bound) by more
than 40%, while requiring access to only 0.6% of the original data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EUSIPCO 2022. 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Paraphrasing Techniques for Maritime QA system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Van Nguyen, Shirui Pan, Weiqing Wang, Reza Haffari, Yuan-Fang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been an increasing interest in incorporating Artificial
Intelligence (AI) into Defence and military systems to complement and augment
human intelligence and capabilities. However, much work still needs to be done
toward achieving an effective human-machine partnership. This work is aimed at
enhancing human-machine communications by developing a capability for
automatically translating human natural language into a machine-understandable
language (e.g., SQL queries). Techniques toward achieving this goal typically
involve building a semantic parser trained on a very large amount of
high-quality manually-annotated data. However, in many real-world Defence
scenarios, it is not feasible to obtain such a large amount of training data.
To the best of our knowledge, there are few works trying to explore the
possibility of training a semantic parser with limited manually-paraphrased
data, in other words, zero-shot. In this paper, we investigate how to exploit
paraphrasing methods for the automated generation of large-scale training
datasets (in the form of paraphrased utterances and their corresponding logical
forms in SQL format) and present our experimental results using real-world data
in the maritime domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViLPAct: A Benchmark for Compositional Generalization on Multimodal
  Human Activities <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05556v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05556v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Yaqing Liao, Yuecheng Lei, Lizhen Qu, Gerard de Melo, Xiaojun Chang, Yazhou Ren, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ViLPAct, a novel vision-language benchmark for human activity
planning. It is designed for a task where embodied AI agents can reason and
forecast future actions of humans based on video clips about their initial
activities and intents in text. The dataset consists of 2.9k videos from
\charades extended with intents via crowdsourcing, a multi-choice question test
set, and four strong baselines. One of the baselines implements a neurosymbolic
approach based on a multi-modal knowledge base (MKB), while the other ones are
deep generative models adapted from recent state-of-the-art (SOTA) methods.
According to our extensive experiments, the key challenges are compositional
generalization and effective use of information from both modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Robustness of <span class="highlight-title">Prompt</span>-based Semantic Parsing with Large <span class="highlight-title">Pre-train</span>ed
  Language Model: An Empirical Study on Codex <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12868v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12868v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic parsing is a technique aimed at constructing a structured
representation of the meaning of a natural-language question. Recent
advancements in few-shot language models trained on code have demonstrated
superior performance in generating these representations compared to
traditional unimodal language models, which are trained on downstream tasks.
Despite these advancements, existing fine-tuned neural semantic parsers are
susceptible to adversarial attacks on natural-language inputs. While it has
been established that the robustness of smaller semantic parsers can be
enhanced through adversarial training, this approach is not feasible for large
language models in real-world scenarios, as it requires both substantial
computational resources and expensive human annotation on in-domain semantic
parsing data. This paper presents the first empirical study on the adversarial
robustness of a large prompt-based language model of code, \codex. Our results
demonstrate that the state-of-the-art (SOTA) code-language models are
vulnerable to carefully crafted adversarial examples. To address this
challenge, we propose methods for improving robustness without the need for
significant amounts of labeled data or heavy computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL2023 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Event Extraction with Memory-based <span class="highlight-title">Loss</span> Prediction
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirong Shen, Zhen Li, Guilin Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction (EE) plays an important role in many industrial application
scenarios, and high-quality EE methods require a large amount of manual
annotation data to train supervised learning models. However, the cost of
obtaining annotation data is very high, especially for annotation of domain
events, which requires the participation of experts from corresponding domain.
So we introduce active learning (AL) technology to reduce the cost of event
annotation. But the existing AL methods have two main problems, which make them
not well used for event extraction. Firstly, the existing pool-based selection
strategies have limitations in terms of computational cost and sample validity.
Secondly, the existing evaluation of sample importance lacks the use of local
sample information. In this paper, we present a novel deep AL method for EE. We
propose a batch-based selection strategy and a Memory-Based Loss Prediction
model (MBLP) to select unlabeled samples efficiently. During the selection
process, we use an internal-external sample loss ranking method to evaluate the
sample importance by using local information. Finally, we propose a delayed
training strategy to train the MBLP model. Extensive experiments are performed
on three domain datasets, and our method outperforms other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenges in Explanation Quality Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Schuff, Heike Adel, Peng Qi, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While much research focused on producing explanations, it is still unclear
how the produced explanations' quality can be evaluated in a meaningful way.
Today's predominant approach is to quantify explanations using proxy scores
which compare explanations to (human-annotated) gold explanations. This
approach assumes that explanations which reach higher proxy scores will also
provide a greater benefit to human users. In this paper, we present problems of
this approach. Concretely, we (i) formulate desired characteristics of
explanation quality, (ii) describe how current evaluation practices violate
them, and (iii) support our argumentation with initial evidence from a
crowdsourcing case study in which we investigate the explanation quality of
state-of-the-art explainable question answering systems. We find that proxy
scores correlate poorly with human quality ratings and, additionally, become
less expressive the more often they are used (i.e. following Goodhart's law).
Finally, we propose guidelines to enable a meaningful evaluation of
explanations to drive the development of systems that provide tangible benefits
to human users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Challenging Benchmark for <span class="highlight-title">Low-Resource</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Wang, Chang Ma, Qingxiu Dong, Lingpeng Kong, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With promising yet saturated results in high-resource settings, low-resource
datasets have gradually become popular benchmarks for evaluating the learning
ability of advanced neural networks (e.g., BigBench, superGLUE). Some models
even surpass humans according to benchmark test results. However, we find that
there exists a set of hard examples in low-resource settings that challenge
neural networks but are not well evaluated, which causes over-estimated
performance. We first give a theoretical analysis on which factors bring the
difficulty of low-resource learning. It then motivate us to propose a
challenging benchmark hardBench to better evaluate the learning ability, which
covers 11 datasets, including 3 computer vision (CV) datasets and 8 natural
language process (NLP) datasets. Experiments on a wide range of models show
that neural networks, even pre-trained language models, have sharp performance
drops on our benchmark, demonstrating the effectiveness on evaluating the
weaknesses of neural networks. On NLP tasks, we surprisingly find that despite
better results on traditional low-resource benchmarks, pre-trained networks,
does not show performance improvements on our benchmarks. These results
demonstrate that there are still a large robustness gap between existing models
and human-level performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flat Multi-modal Interaction <span class="highlight-title">Transformer</span> for Named Entity Recognition <span class="chip">COLING 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Lu, Dixiang Zhang, Pingjian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal named entity recognition (MNER) aims at identifying entity spans
and recognizing their categories in social media posts with the aid of images.
However, in dominant MNER approaches, the interaction of different modalities
is usually carried out through the alternation of self-attention and
cross-attention or over-reliance on the gating machine, which results in
imprecise and biased correspondence between fine-grained semantic units of text
and image. To address this issue, we propose a Flat Multi-modal Interaction
Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in
sentences and general domain words to obtain visual cues. Then, we transform
the fine-grained semantic representation of the vision and text into a unified
lattice structure and design a novel relative position encoding to match
different modalities in Transformer. Meanwhile, we propose to leverage entity
boundary detection as an auxiliary task to alleviate visual bias. Experiments
show that our methods achieve the new state-of-the-art performance on two
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2022, oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Risks of Stealing the Decoding Algorithms of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask-guided <span class="highlight-title">BERT</span> for Few Shot Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiong Liao, Zhengliang Liu, Haixing Dai, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Yuzhong Chen, Xi Jiang, Wei Liu, Dajiang Zhu, Tianming Liu, Sheng Li, Xiang Li, Hongmin Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models have achieved significant success in
various domains. However, the data-intensive nature of the transformer
architecture requires much labeled data, which is challenging in low-resource
scenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the
difficulty of training robust models on small amounts of samples, which
frequently leads to overfitting. Here we present Mask-BERT, a simple and
modular framework to help BERT-based architectures tackle FSL. The proposed
approach fundamentally differs from existing FSL strategies such as prompt
tuning and meta-learning. The core idea is to selectively apply masks on text
inputs and filter out irrelevant information, which guides the model to focus
on discriminative tokens that influence prediction results. In addition, to
make the text representations from different categories more separable and the
text representations from the same category more compact, we introduce a
contrastive learning loss function. Experimental results on public-domain
benchmark datasets demonstrate the effectiveness of Mask-BERT.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic neutrality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milo Phillips-Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias infects the algorithms that wield increasing control over our lives.
Predictive policing systems overestimate crime in communities of color; hiring
algorithms dock qualified female candidates; and facial recognition software
struggles to recognize dark-skinned faces. Algorithmic bias has received
significant attention. Algorithmic neutrality, in contrast, has been largely
neglected. Algorithmic neutrality is my topic. I take up three questions. What
is algorithmic neutrality? Is algorithmic neutrality possible? When we have an
eye to algorithmic neutrality, what can we learn about algorithmic bias? To
answer these questions in concrete terms, I work with a case study: search
engines. Drawing on work about neutrality in science, I say that a search
engine is neutral only if certain values, like political ideologies or the
financial interests of the search engine operator, play no role in how the
search engine ranks pages. Search neutrality, I argue, is impossible. Its
impossibility seems to threaten the significance of search bias: if no search
engine is neutral, then every search engine is biased. To defuse this threat, I
distinguish two forms of bias, failing-on-its-own-terms bias and other-values
bias. This distinction allows us to make sense of search bias, and capture its
normative complexion, despite the impossibility of neutrality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Recommendation Systems with User <span class="highlight-title">Persona</span>lity Inferred from
  Product <span class="highlight-title">Review</span>s <span class="chip">WSDM'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Lu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personality is a psychological factor that reflects people's preferences,
which in turn influences their decision-making. We hypothesize that accurate
modeling of users' personalities improves recommendation systems' performance.
However, acquiring such personality profiles is both sensitive and expensive.
We address this problem by introducing a novel method to automatically extract
personality profiles from public product review text. We then design and assess
three context-aware recommendation architectures that leverage the profiles to
test our hypothesis.
  Experiments on our two newly contributed personality datasets --
Amazon-beauty and Amazon-music -- validate our hypothesis, showing performance
boosts of 3--28%.Our analysis uncovers that varying personality types
contribute differently to recommendation performance: open and extroverted
personalities are most helpful in music recommendation, while a conscientious
personality is most helpful in beauty product recommendation. The dataset is
available at https://github.com/XinyuanLu00/IRS-WSDM2023-personality-dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IRS@WSDM'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization analysis of an unfolding network for analysis-based
  Compressed Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Kouni, Yannis Panagakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unfolding networks have shown promising results in the Compressed Sensing
(CS) field. Yet, the investigation of their generalization ability is still in
its infancy. In this paper, we perform generalization analysis of a
state-of-the-art ADMM-based unfolding network, which jointly learns a decoder
for CS and a sparsifying redundant analysis operator. To this end, we first
impose a structural constraint on the learnable sparsifier, which parametrizes
the network's hypothesis class. For the latter, we estimate its Rademacher
complexity. With this estimate in hand, we deliver generalization error bounds
for the examined network. Finally, the validity of our theory is assessed and
numerical comparisons to a state-of-the-art unfolding network are made, on
synthetic and real-world datasets. Our experimental results demonstrate that
our proposed framework complies with our theoretical findings and outperforms
the baseline, consistently for all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Robustness of <span class="highlight-title">Conversation</span>al Recommender Systems by
  Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Montazeralghaem, James Allan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRSs) are improving rapidly, according to
the standard recommendation accuracy metrics. However, it is essential to make
sure that these systems are robust in interacting with users including regular
and malicious users who want to attack the system by feeding the system
modified input data. In this paper, we propose an adversarial evaluation scheme
including four scenarios in two categories and automatically generate
adversarial examples to evaluate the robustness of these systems in the face of
different input data. By executing these adversarial examples we can compare
the ability of different conversational recommender systems to satisfy the
user's preferences. We evaluate three CRSs by the proposed adversarial examples
on two datasets. Our results show that none of these systems are robust and
reliable to the adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Projection Bias in Intertemporal Choices: A Prospect Theory
  Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users often face bundle promotions when purchasing, where they have to select
between two options: buy the single item at full price, or buy the bundle at a
discount. In this scenario, users' preferences are usually influenced by the
projection bias, that is, users often believe that their future preferences are
similar to their current preferences, causing them to make irrational and
short-sighted decisions. It is of great significance to analyze the effect of
the projection bias on users' preferences, and this study may help understand
users' decision-making process and provide bundling and pricing strategies for
sellers. Prior works typically use a linear bias model for qualitative
analysis, and they cannot quantitatively calculate users' nonlinear and
personalized bias. In this work, we propose Pobe, a projection bias-embedded
preference model to accurately predict users' choices. The proposed Pobe
introduces the prospect theory to analyze users' irrational decisions, and
utilizes the weight function to handle users' nonlinear and personalized bias.
Based on the proposed Pobe, we also study the impact of items' correlations or
discount prices on users' choices, and provide four bundling strategies.
Experimental results show that the proposed method can achieve better
performance than prior works, especially when only small data is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the state-of-the-art of hybrid language models
architectures and strategies for "complex" question-answering (QA, CQA, CPS).
Large Language Models (LLM) are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, methods,
sensitive data protection, explainability, human approval and versatile
feedback... We identify key elements augmenting LLM to solve complex questions
or problems. We extend findings from the robust community edited research
papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and
challenges of LLM in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like
ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential
as well as the equally strong limitations of language models in complex QA.
Hybridizing these models with different components could allow to overcome
these different limits and go much further. We discuss some challenges
associated with complex QA, including domain adaptation, decomposition and
efficient multi-step QA, long form and non-factoid QA, safety and
multi-sensitivity data protection, multimodal search, hallucinations,
explainability and truthfulness, temproal reasoning. Therefore, we analyze
current solutions and promising research trends, using elements such as: hybrid
LLM architectures, active human reinforcement learning supervised with AI,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, iterated decomposition and others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Next Basket Recommendation Reality Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Sami Jullien, Mozhdeh Ariannezhad, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of a next basket recommendation (NBR) system is to recommend items
for the next basket for a user, based on the sequence of their prior baskets.
Recently, a number of methods with complex modules have been proposed that
claim state-of-the-art performance. They rarely look into the predicted basket
and just provide intuitive reasons for the observed improvements, e.g., better
representation, capturing intentions or relations, etc. We provide a novel
angle on the evaluation of next basket recommendation methods, centered on the
distinction between repetition and exploration: the next basket is typically
composed of previously consumed items (i.e., repeat items) and new items (i.e,
explore items). We propose a set of metrics that measure the repeat/explore
ratio and performance of NBR models. Using these new metrics, we analyze
state-of-the-art NBR models. The results of our analysis help to clarify the
extent of the actual progress achieved by existing NBR methods as well as the
underlying reasons for the improvements. Overall, our work sheds light on the
evaluation problem of NBR and provides useful insights into the model design
for this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Federated Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Sun, Yonghui Xu, Yong Liu, Wei He, Lanju Kong, Fangzhao Wu, Yali Jiang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has recently been applied to recommendation systems to
protect user privacy. In federated learning settings, recommendation systems
can train recommendation models only collecting the intermediate parameters
instead of the real user data, which greatly enhances the user privacy. Beside,
federated recommendation systems enable to collaborate with other data
platforms to improve recommended model performance while meeting the regulation
and privacy constraints. However, federated recommendation systems faces many
new challenges such as privacy, security, heterogeneity and communication
costs. While significant research has been conducted in these areas, gaps in
the surveying literature still exist. In this survey, we-(1) summarize some
common privacy mechanisms used in federated recommendation systems and discuss
the advantages and limitations of each mechanism; (2) review some robust
aggregation strategies and several novel attacks against security; (3)
summarize some approaches to address heterogeneity and communication costs
problems; (4)introduce some open source platforms that can be used to build
federated recommendation systems; (5) present some prospective research
directions in the future. This survey can guide researchers and practitioners
understand the research progress in these areas.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for
  Geometry-Agnostic System Identification <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to system identification (estimating the physical
parameters of an object) from videos assume known object geometries. This
precludes their applicability in a vast majority of scenes where object
geometries are complex or unknown. In this work, we aim to identify parameters
characterizing a physical system from a set of multi-view videos without any
assumption on object geometry or topology. To this end, we propose "Physics
Augmented Continuum Neural Radiance Fields" (PAC-NeRF), to estimate both the
unknown geometry and physical parameters of highly dynamic objects from
multi-view videos. We design PAC-NeRF to only ever produce physically plausible
states by enforcing the neural radiance field to follow the conservation laws
of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian
representation of the neural radiance field, i.e., we use the Eulerian grid
representation for NeRF density and color fields, while advecting the neural
radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian
representation seamlessly blends efficient neural rendering with the material
point method (MPM) for robust differentiable physics simulation. We validate
the effectiveness of our proposed framework on geometry and physical parameter
estimation over a vast range of materials, including elastic bodies,
plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate
significant performance gain on most tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 Spotlight. Project page:
  https://sites.google.com/view/PAC-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning with Large Language Models for Code <span class="highlight-title">Generation</span> <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language model-based code generation pipelines typically use
beam search or sampling algorithms during the decoding process. Although the
programs they generate achieve high token-matching-based scores, they often
fail to compile or generate incorrect outputs. The main reason is that
conventional Transformer decoding algorithms may not be the best choice for
code generation. In this work, we propose a novel Transformer decoding
algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning
algorithm to do lookahead search and guide the Transformer to generate better
programs. Specifically, instead of simply optimizing the likelihood of the
generated sequences, the Transformer makes use of a planner to generate
candidate programs and test them on public test cases. The Transformer can
therefore make more informed decisions and generate tokens that will eventually
lead to higher-quality programs. We also design a mechanism that shares
information between the Transformer and the planner to make our algorithm
computationally efficient. We empirically evaluate our framework with several
large language models as backbones on public coding challenge benchmarks,
showing that 1) it can generate programs that consistently achieve higher
performance compared with competing baseline methods; 2) it enables
controllable code generation, such as concise codes and highly-commented codes
by optimizing modified objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Project page:https://codeaimcts.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling up GANs for Text-to-Image Synthesis <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of text-to-image synthesis has taken the world by storm
and captured the general public's imagination. From a technical standpoint, it
also marked a drastic change in the favored architecture to design generative
image models. GANs used to be the de facto choice, with techniques like
StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new
standard for large-scale generative models overnight. This rapid shift raises a
fundamental question: can we scale up GANs to benefit from large datasets like
LAION? We find that na\"Ively increasing the capacity of the StyleGAN
architecture quickly becomes unstable. We introduce GigaGAN, a new GAN
architecture that far exceeds this limit, demonstrating GANs as a viable option
for text-to-image synthesis. GigaGAN offers three major advantages. First, it
is orders of magnitude faster at inference time, taking only 0.13 seconds to
synthesize a 512px image. Second, it can synthesize high-resolution images, for
example, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various
latent space editing applications such as latent interpolation, style mixing,
and vector arithmetic operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project webpage at https://mingukkang.github.io/GigaGAN/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TANGOS: Regularizing Tabular Neural Networks through Gradient
  Orthogonalization and Specialization <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Jeffares, Tennison Liu, Jonathan Crabbé, Fergus Imrie, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their success with unstructured data, deep neural networks are not
yet a panacea for structured tabular data. In the tabular domain, their
efficiency crucially relies on various forms of regularization to prevent
overfitting and provide strong generalization performance. Existing
regularization techniques include broad modelling decisions such as choice of
architecture, loss functions, and optimization methods. In this work, we
introduce Tabular Neural Gradient Orthogonalization and Specialization
(TANGOS), a novel framework for regularization in the tabular setting built on
latent unit attributions. The gradient attribution of an activation with
respect to a given input feature suggests how the neuron attends to that
feature, and is often employed to interpret the predictions of deep networks.
In TANGOS, we take a different approach and incorporate neuron attributions
directly into training to encourage orthogonalization and specialization of
latent attributions in a fully-connected network. Our regularizer encourages
neurons to focus on sparse, non-overlapping input features and results in a set
of diverse and specialized latent units. In the tabular domain, we demonstrate
that our approach can lead to improved out-of-sample generalization
performance, outperforming other popular regularization methods. We provide
insight into why our regularizer is effective and demonstrate that TANGOS can
be applied jointly with existing methods to achieve even greater generalization
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Learning Representations
  (ICLR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computable Phenotypes to Characterize Changing Patient Brain Dysfunction
  in the Intensive Care Unit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanfang Ren, Tyler J. Loftus, Ziyuan Guan, Rayon Uddin, Benjamin Shickel, Carolina B. Maciel, Katharina Busl, Parisa Rashidi, Azra Bihorac, Tezcan Ozrazgat-Baslanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the United States, more than 5 million patients are admitted annually to
ICUs, with ICU mortality of 10%-29% and costs over $82 billion. Acute brain
dysfunction status, delirium, is often underdiagnosed or undervalued. This
study's objective was to develop automated computable phenotypes for acute
brain dysfunction states and describe transitions among brain dysfunction
states to illustrate the clinical trajectories of ICU patients. We created two
single-center, longitudinal EHR datasets for 48,817 adult patients admitted to
an ICU at UFH Gainesville (GNV) and Jacksonville (JAX). We developed algorithms
to quantify acute brain dysfunction status including coma, delirium, normal, or
death at 12-hour intervals of each ICU admission and to identify acute brain
dysfunction phenotypes using continuous acute brain dysfunction status and
k-means clustering approach. There were 49,770 admissions for 37,835 patients
in UFH GNV dataset and 18,472 admissions for 10,982 patients in UFH JAX
dataset. In total, 18% of patients had coma as the worst brain dysfunction
status; every 12 hours, around 4%-7% would transit to delirium, 22%-25% would
recover, 3%-4% would expire, and 67%-68% would remain in a coma in the ICU.
Additionally, 7% of patients had delirium as the worst brain dysfunction
status; around 6%-7% would transit to coma, 40%-42% would be no delirium, 1%
would expire, and 51%-52% would remain delirium in the ICU. There were three
phenotypes: persistent coma/delirium, persistently normal, and transition from
coma/delirium to normal almost exclusively in first 48 hours after ICU
admission. We developed phenotyping scoring algorithms that determined acute
brain dysfunction status every 12 hours while admitted to the ICU. This
approach may be useful in developing prognostic and decision-support tools to
aid patients and clinicians in decision-making on resource use and escalation
of care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 3 tables, 1 eTable</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-world Instance Segmentation: Top-down Learning with Bottom-up
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Kalluri, Weiyao Wang, Heng Wang, Manmohan Chandraker, Lorenzo Torresani, Du Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many top-down architectures for instance segmentation achieve significant
success when trained and tested on pre-defined closed-world taxonomy. However,
when deployed in the open world, they exhibit notable bias towards seen classes
and suffer from significant performance drop. In this work, we propose a novel
approach for open world instance segmentation called bottom-Up and top-Down
Open-world Segmentation (UDOS) that combines classical bottom-up segmentation
algorithms within a top-down learning framework. UDOS first predicts parts of
objects using a top-down network trained with weak supervision from bottom-up
segmentations. The bottom-up segmentations are class-agnostic and do not
overfit to specific taxonomies. The part-masks are then fed into affinity-based
grouping and refinement modules to predict robust instance-level segmentations.
UDOS enjoys both the speed and efficiency from the top-down architectures and
the generalization ability to unseen categories from bottom-up supervision. We
validate the strengths of UDOS on multiple cross-category as well as
cross-dataset transfer tasks from 5 challenging datasets including MS-COCO,
LVIS, ADE20k, UVO and OpenImages, achieving significant improvements over
state-of-the-art across the board. Our code and models are available on our
project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://tarun005.github.io/UDOS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDSketch: Integrated Planning Domain Programming and Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Mao, Tomás Lozano-Pérez, Joshua B. Tenenbaum, Leslie Pack Kaelbling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a model learning and online planning approach towards
building flexible and general robots. Specifically, we investigate how to
exploit the locality and sparsity structures in the underlying environmental
transition model to improve model generalization, data-efficiency, and
runtime-efficiency. We present a new domain definition language, named
PDSketch. It allows users to flexibly define high-level structures in the
transition models, such as object and feature dependencies, in a way similar to
how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden
dimensions of a convolutional neural network. The details of the transition
model will be filled in by trainable neural networks. Based on the defined
structures and learned parameters, PDSketch automatically generates
domain-independent planning heuristics without additional training. The derived
heuristics accelerate the performance-time planning for novel goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022. Project page: https://pdsketch.csail.mit.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mark My Words: Dangers of Watermarked Images in ImageNet <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Bykov, Klaus-Robert Müller, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The utilization of pre-trained networks, especially those trained on
ImageNet, has become a common practice in Computer Vision. However, prior
research has indicated that a significant number of images in the ImageNet
dataset contain watermarks, making pre-trained networks susceptible to learning
artifacts such as watermark patterns within their latent spaces. In this paper,
we aim to assess the extent to which popular pre-trained architectures display
such behavior and to determine which classes are most affected. Additionally,
we examine the impact of watermarks on the extracted features. Contrary to the
popular belief that the Chinese logographic watermarks impact the "carton"
class only, our analysis reveals that a variety of ImageNet classes, such as
"monitor", "broom", "apron" and "safe" rely on spurious correlations. Finally,
we propose a simple approach to mitigate this issue in fine-tuned networks by
ignoring the encodings from the feature-extractor layer of ImageNet pre-trained
networks that are most susceptible to watermark imprints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, Accepted to the ICLR 2023 TrustML-(un)Limited
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Stationary Markov Processes with Contrastive Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludvig Bergenstråhle, Jens Lagergren, Joakim Lundeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new optimization algorithm, termed \emph{contrastive
adjustment}, for learning Markov transition kernels whose stationary
distribution matches the data distribution. Contrastive adjustment is not
restricted to a particular family of transition distributions and can be used
to model data in both continuous and discrete state spaces. Inspired by recent
work on noise-annealed sampling, we propose a particular transition operator,
the \emph{noise kernel}, that can trade mixing speed for sample fidelity. We
show that contrastive adjustment is highly valuable in human-computer design
processes, as the stationarity of the learned Markov chain enables local
exploration of the data manifold and makes it possible to iteratively refine
outputs by human feedback. We compare the performance of noise kernels trained
with contrastive adjustment to current state-of-the-art generative models and
demonstrate promising results on a variety of image synthesis tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse and Local Networks for Hypergraph <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xiao, Leslie Pack Kaelbling, Jiajun Wu, Jiayuan Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning about the relationships between entities from input facts (e.g.,
whether Ari is a grandparent of Charlie) generally requires explicit
consideration of other entities that are not mentioned in the query (e.g., the
parents of Charlie). In this paper, we present an approach for learning to
solve problems of this kind in large, real-world domains, using sparse and
local hypergraph neural networks (SpaLoc). SpaLoc is motivated by two
observations from traditional logic-based reasoning: relational inferences
usually apply locally (i.e., involve only a small number of individuals), and
relations are usually sparse (i.e., only hold for a small percentage of tuples
in a domain). We exploit these properties to make learning and inference
efficient in very large domains by (1) using a sparse tensor representation for
hypergraph neural networks, (2) applying a sparsification loss during training
to encourage sparse representations, and (3) subsampling based on a novel
information sufficiency-based sampling process during training. SpaLoc achieves
state-of-the-art performance on several real-world, large-scale knowledge graph
reasoning benchmarks, and is the first framework for applying hypergraph neural
networks on real-world knowledge graphs with more than 10k nodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Learning on Graphs Conference (LoG) 2022. Project page:
  https://spaloc.csail.mit.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Expressiveness and Generalization of Hypergraph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhezheng Luo, Jiayuan Mao, Joshua B. Tenenbaum, Leslie Pack Kaelbling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This extended abstract describes a framework for analyzing the
expressiveness, learning, and (structural) generalization of hypergraph neural
networks (HyperGNNs). Specifically, we focus on how HyperGNNs can learn from
finite datasets and generalize structurally to graph reasoning problems of
arbitrary input sizes. Our first contribution is a fine-grained analysis of the
expressiveness of HyperGNNs, that is, the set of functions that they can
realize. Our result is a hierarchy of problems they can solve, defined in terms
of various hyperparameters such as depths and edge arities. Next, we analyze
the learning properties of these neural networks, especially focusing on how
they can be trained on a finite set of small graphs and generalize to larger
graphs, which we term structural generalization. Our theoretical results are
further supported by the empirical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Learning on Graphs Conference (LoG) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Rational Subgoals from Demonstrations and Instructions <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhezheng Luo, Jiayuan Mao, Jiajun Wu, Tomás Lozano-Pérez, Joshua B. Tenenbaum, Leslie Pack Kaelbling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for learning useful subgoals that support efficient
long-term planning to achieve novel goals. At the core of our framework is a
collection of rational subgoals (RSGs), which are essentially binary
classifiers over the environmental states. RSGs can be learned from
weakly-annotated data, in the form of unsegmented demonstration trajectories,
paired with abstract task descriptions, which are composed of terms initially
unknown to the agent (e.g., collect-wood then craft-boat then go-across-river).
Our framework also discovers dependencies between RSGs, e.g., the task
collect-wood is a helpful subgoal for the task craft-boat. Given a goal
description, the learned subgoals and the derived dependencies facilitate
off-the-shelf planning algorithms, such as A* and RRT, by setting helpful
subgoals as waypoints to the planner, which significantly improves
performance-time efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023. First two authors contributed equally. Project page:
  https://rsg.csail.mit.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Testable Learning of Halfspaces with Adversarial Label Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Sihan Liu, Nikos Zarifis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give the first polynomial-time algorithm for the testable learning of
halfspaces in the presence of adversarial label noise under the Gaussian
distribution. In the recently introduced testable learning model, one is
required to produce a tester-learner such that if the data passes the tester,
then one can trust the output of the robust learner on the data. Our
tester-learner runs in time $\poly(d/\eps)$ and outputs a halfspace with
misclassification error $O(\opt)+\eps$, where $\opt$ is the 0-1 error of the
best fitting halfspace. At a technical level, our algorithm employs an
iterative soft localization technique enhanced with appropriate testers to
ensure that the data distribution is sufficiently similar to a Gaussian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cal-QL: Calibrated Offline RL <span class="highlight-title">Pre-Train</span>ing for Efficient Online
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A compelling use case of offline reinforcement learning (RL) is to obtain a
policy initialization from existing datasets, which allows efficient
fine-tuning with limited amounts of active online interaction. However, several
existing offline RL methods tend to exhibit poor online fine-tuning
performance. On the other hand, online RL methods can learn effectively through
online interaction, but struggle to incorporate offline data, which can make
them very slow in settings where exploration is challenging or pre-training is
necessary. In this paper, we devise an approach for learning an effective
initialization from offline data that also enables fast online fine-tuning
capabilities. Our approach, calibrated Q-learning (Cal-QL) accomplishes this by
learning a conservative value function initialization that underestimates the
value of the learned policy from offline data, while also being calibrated, in
the sense that the learned Q-values are at a reasonable scale. We refer to this
property as calibration, and define it formally as providing a lower bound on
the true value function of the learned policy and an upper bound on the value
of some other (suboptimal) reference policy, which may simply be the behavior
policy. We show that offline RL algorithms that learn such calibrated value
functions lead to effective online fine-tuning, enabling us to take the
benefits of offline initializations in online fine-tuning. In practice, Cal-QL
can be implemented on top of existing conservative methods for offline RL
within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art
methods on 10/11 fine-tuning benchmark tasks that we study in this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aengus Lynch, Gbètondji J-S Dovonon, Jean Kaddour, Ricardo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of spurious correlations (SCs) arises when a classifier relies on
non-predictive features that happen to be correlated with the labels in the
training data. For example, a classifier may misclassify dog breeds based on
the background of dog images. This happens when the backgrounds are correlated
with other breeds in the training data, leading to misclassifications during
test time. Previous SC benchmark datasets suffer from varying issues, e.g.,
over-saturation or only containing one-to-one (O2O) SCs, but no many-to-many
(M2M) SCs arising between groups of spurious attributes and classes. In this
paper, we present Spawrious-{O2O, M2M}-{Easy, Medium, Hard}, an image
classification benchmark suite containing spurious correlations among different
dog breeds and background locations. To create this dataset, we employ a
text-to-image model to generate photo-realistic images, and an image captioning
model to filter out unsuitable ones. The resulting dataset is of high quality,
containing approximately 152,000 images. Our experimental results demonstrate
that state-of-the-art group robustness methods struggle with Spawrious, most
notably on the Hard-splits with $<60\%$ accuracy. By examining model
misclassifications, we detect reliances on spurious backgrounds, demonstrating
that our dataset provides a significant challenge to drive future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resolving quantitative MRI model degeneracy with machine learning via
  training data distribution design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Guerreri, Sean Epstein, Hojjat Azadbakht, Hui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative MRI (qMRI) aims to map tissue properties non-invasively via
models that relate these unknown quantities to measured MRI signals. Estimating
these unknowns, which has traditionally required model fitting - an often
iterative procedure, can now be done with one-shot machine learning (ML)
approaches. Such parameter estimation may be complicated by intrinsic qMRI
signal model degeneracy: different combinations of tissue properties produce
the same signal. Despite their many advantages, it remains unclear whether ML
approaches can resolve this issue. Growing empirical evidence appears to
suggest ML approaches remain susceptible to model degeneracy. Here we
demonstrate under the right circumstances ML can address this issue. Inspired
by recent works on the impact of training data distributions on ML-based
parameter estimation, we propose to resolve model degeneracy by designing
training data distributions. We put forward a classification of model
degeneracies and identify one particular kind of degeneracies amenable to the
proposed attack. The strategy is demonstrated successfully using the Revised
NODDI model with standard multi-shell diffusion MRI data as an exemplar. Our
results illustrate the importance of training set design which has the
potential to allow accurate estimation of tissue properties with ML.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beware of Instantaneous Dependence in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengmao Zhu, Yuren Liu, Honglong Tian, Yang Yu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Playing an important role in Model-Based Reinforcement Learning (MBRL),
environment models aim to predict future states based on the past. Existing
works usually ignore instantaneous dependence in the state, that is, assuming
that the future state variables are conditionally independent given the past
states. However, instantaneous dependence is prevalent in many RL environments.
For instance, in the stock market, instantaneous dependence can exist between
two stocks because the fluctuation of one stock can quickly affect the other
and the resolution of price change is lower than that of the effect. In this
paper, we prove that with few exceptions, ignoring instantaneous dependence can
result in suboptimal policy learning in MBRL. To address the suboptimality
problem, we propose a simple plug-and-play method to enable existing MBRL
algorithms to take instantaneous dependence into account. Through experiments
on two benchmarks, we (1) confirm the existence of instantaneous dependence
with visualization; (2) validate our theoretical findings that ignoring
instantaneous dependence leads to suboptimal policy; (3) verify that our method
effectively enables reinforcement learning with instantaneous dependence and
improves policy performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Power and Interference Control for VLC-Based UDN: A Reinforcement
  Learning Approach <span class="chip">ISWC'2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Tang, Sicong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible light communication (VLC) has been widely applied as a promising
solution for modern short range communication. When it comes to the deployment
of LED arrays in VLC networks, the emerging ultra-dense network (UDN)
technology can be adopted to expand the VLC network's capacity. However, the
problem of inter-cell interference (ICI) mitigation and efficient power control
in the VLC-based UDN is still a critical challenge. To this end, a
reinforcement learning (RL) based VLC UDN architecture is devised in this
paper. The deployment of the cells is optimized via spatial reuse to mitigate
ICI. An RL-based algorithm is proposed to dynamically optimize the policy of
power and interference control, maximizing the system utility in the
complicated and dynamic environment. Simulation results demonstrate the
superiority of the proposed scheme, it increase the system utility and
achievable data rate while reducing the energy consumption and ICI, which
outperforms the benchmark scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by and to appear in Proc. ACM
  UbiComp/ISWC'2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Collaborative Heterogeneous Bandits in Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Lee, Laura Schmid, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-agent multi-armed bandit problem has been studied extensively due
to its ubiquity in many real-life applications, such as online recommendation
systems and wireless networking. We consider the setting where agents should
minimize their group regret while collaborating over a given graph via some
communication protocol and where each agent is given a different set of arms.
Previous literature on this problem only considered one of the two desired
features separately: agents with the same arm set communicate over a general
graph, or agents with different arm sets communicate over a fully connected
graph. In this work, we introduce a more general problem setting that
encompasses all the desired features. For this novel setting, we first provide
a rigorous regret analysis for the standard flooding protocol combined with the
UCB policy. Then, to mitigate the issue of high communication costs incurred by
flooding, we propose a new protocol called Flooding with Absorption (FWA). We
provide a theoretical analysis of the regret bound and intuitions on the
advantages of using FWA over flooding. Lastly, we verify empirically that using
FWA leads to significantly lower communication costs despite minimal regret
performance loss compared to flooding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures; submitted to MobiHoc 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Splines for Non-Linear Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Macaluso, Luca Clissa, Stefano Lodi, Claudio Sartori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Computing offers a new paradigm for efficient computing and many AI
applications could benefit from its potential boost in performance. However,
the main limitation is the constraint to linear operations that hampers the
representation of complex relationships in data. In this work, we propose an
efficient implementation of quantum splines for non-linear approximation. In
particular, we first discuss possible parametrisations, and select the most
convenient for exploiting the HHL algorithm to obtain the estimates of spline
coefficients. Then, we investigate QSpline performance as an evaluation routine
for some of the most popular activation functions adopted in ML. Finally, a
detailed comparison with classical alternatives to the HHL is also presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Regression with Infinite-Width Neural Networks on Millions of
  Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Adlam, Jaehoon Lee, Shreyas Padhy, Zachary Nado, Jasper Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural kernels have drastically increased performance on diverse and
nonstandard data modalities but require significantly more compute, which
previously limited their application to smaller datasets. In this work, we
address this by massively parallelizing their computation across many GPUs. We
combine this with a distributed, preconditioned conjugate gradients algorithm
to enable kernel regression at a large scale (i.e. up to five million
examples). Using this approach, we study scaling laws of several neural kernels
across many orders of magnitude for the CIFAR-5m dataset. Using data
augmentation to expand the original CIFAR-10 training dataset by a factor of
20, we obtain a test accuracy of 91.2\% (SotA for a pure kernel method).
Moreover, we explore neural kernels on other data modalities, obtaining results
on protein and small molecule prediction tasks that are competitive with SotA
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast kernel methods for Data Quality Monitoring as a goodness-of-fit
  test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaia Grosso, Nicolò Lai, Marco Letizia, Jacopo Pazzini, Marco Rando, Lorenzo Rosasco, Andrea Wulzer, Marco Zanetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We here propose a machine learning approach for monitoring particle detectors
in real-time. The goal is to assess the compatibility of incoming experimental
data with a reference dataset, characterising the data behaviour under normal
circumstances, via a likelihood-ratio hypothesis test. The model is based on a
modern implementation of kernel methods, nonparametric algorithms that can
learn any continuous function given enough data. The resulting approach is
efficient and agnostic to the type of anomaly that may be present in the data.
Our study demonstrates the effectiveness of this strategy on multivariate data
from drift tube chamber muon detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Greener yet Powerful: Taming Large Code <span class="highlight-title">Generation</span> Models with
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokai Wei, Sujan Gonugondla, Wasi Ahmad, Shiqi Wang, Baishakhi Ray, Haifeng Qian, Xiaopeng Li, Varun Kumar, Zijian Wang, Yuchen Tian, Qing Sun, Ben Athiwaratkun, Mingyue Shang, Murali Krishna Ramanathan, Parminder Bhatia, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ML-powered code generation aims to assist developers to write code in a more
productive manner, by intelligently generating code blocks based on natural
language prompts. Recently, large pretrained deep learning models have
substantially pushed the boundary of code generation and achieved impressive
performance. Despite their great power, the huge number of model parameters
poses a significant threat to adapting them in a regular software development
environment, where a developer might use a standard laptop or mid-size server
to develop her code. Such large models incur significant resource usage (in
terms of memory, latency, and dollars) as well as carbon footprint.
  Model compression is a promising approach to address these challenges.
Several techniques are proposed to compress large pretrained models typically
used for vision or textual data. Out of many available compression techniques,
we identified that quantization is mostly applicable for code generation task
as it does not require significant retraining cost. As quantization represents
model parameters with lower-bit integer (e.g., int8), the model size and
runtime latency would both benefit from such int representation. We extensively
study the impact of quantized model on code generation tasks across different
dimension: (i) resource usage and carbon footprint, (ii) accuracy, and (iii)
robustness. To this end, through systematic experiments we find a recipe of
quantization technique that could run even a $6$B model in a regular laptop
without significant accuracy or robustness degradation. We further found the
recipe is readily applicable to code summarization task as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC-JeDi: Diffusion for Particle Cloud <span class="highlight-title">Generation</span> in High Energy Physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Leigh, Debajyoti Sengupta, Guillaume Quétant, John Andrew Raine, Knut Zoch, Tobias Golling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new method to efficiently generate jets in High
Energy Physics called PC-JeDi. This method utilises score-based diffusion
models in conjunction with transformers which are well suited to the task of
generating jets as particle clouds due to their permutation equivariance.
PC-JeDi achieves competitive performance with current state-of-the-art methods
across several metrics that evaluate the quality of the generated jets.
Although slower than other models, due to the large number of forward passes
required by diffusion models, it is still substantially faster than traditional
detailed simulation. Furthermore, PC-JeDi uses conditional generation to
produce jets with a desired mass and transverse momentum for two different
particles, top quarks and gluons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 25 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-dependent Generalization Bounds via Variable-Size Compressibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Sefidgaran, Abdellatif Zaidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we establish novel data-dependent upper bounds on the
generalization error through the lens of a "variable-size compressibility"
framework that we introduce newly here. In this framework, the generalization
error of an algorithm is linked to a variable-size 'compression rate' of its
input data. This is shown to yield bounds that depend on the empirical measure
of the given input data at hand, rather than its unknown distribution. Our new
generalization bounds that we establish are tail bounds, tail bounds on the
expectation, and in-expectations bounds. Moreover, it is shown that our
framework also allows to derive general bounds on any function of the input
data and output hypothesis random variables. In particular, these general
bounds are shown to subsume and possibly improve over several existing
PAC-Bayes and data-dependent intrinsic dimension-based bounds that are
recovered as special cases, thus unveiling a unifying character of our
approach. For instance, a new data-dependent intrinsic dimension based bounds
is established, which connects the generalization error to the optimization
trajectories and reveals various interesting connections with rate-distortion
dimension of process, R\'enyi information dimension of process, and metric mean
dimension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Penalized Deep Partially Linear Cox Models with Application to CT Scans
  of Lung Cancer Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Sun, Jian Kang, Chinmay Haridas, Nicholas R. Mayne, Alexandra L. Potter, Chi-Fu Jeffrey Yang, David C. Christiani, Yi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung cancer is a leading cause of cancer mortality globally, highlighting the
importance of understanding its mortality risks to design effective
patient-centered therapies. The National Lung Screening Trial (NLST) was a
nationwide study aimed at investigating risk factors for lung cancer. The study
employed computed tomography texture analysis (CTTA), which provides objective
measurements of texture patterns on CT scans, to quantify the mortality risks
of lung cancer patients. Partially linear Cox models are becoming a popular
tool for modeling survival outcomes, as they effectively handle both
established risk factors (such as age and other clinical factors) and new risk
factors (such as image features) in a single framework. The challenge in
identifying the texture features that impact cancer survival is due to their
sensitivity to factors such as scanner type, segmentation, and organ motion. To
overcome this challenge, we propose a novel Penalized Deep Partially Linear Cox
Model (Penalized DPLC), which incorporates the SCAD penalty to select
significant texture features and employs a deep neural network to estimate the
nonparametric component of the model accurately. We prove the convergence and
asymptotic properties of the estimator and compare it to other methods through
extensive simulation studies, evaluating its performance in risk prediction and
feature selection. The proposed method is applied to the NLST study dataset to
uncover the effects of key clinical and imaging risk factors on patients'
survival. Our findings provide valuable insights into the relationship between
these factors and survival outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Calibrator Ensemble for Model Calibration under Distribution
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Zou, Weijian Deng, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model calibration usually requires optimizing some parameters (e.g.,
temperature) w.r.t an objective function (e.g., negative log-likelihood). In
this paper, we report a plain, important but often neglected fact that the
objective function is influenced by calibration set difficulty, i.e., the ratio
of the number of incorrectly classified samples to that of correctly classified
samples. If a test set has a drastically different difficulty level from the
calibration set, the optimal calibration parameters of the two datasets would
be different. In other words, a calibrator optimal on the calibration set would
be suboptimal on the OOD test set and thus has degraded performance. With this
knowledge, we propose a simple and effective method named adaptive calibrator
ensemble (ACE) to calibrate OOD datasets whose difficulty is usually higher
than the calibration set. Specifically, two calibration functions are trained,
one for in-distribution data (low difficulty), and the other for severely OOD
data (high difficulty). To achieve desirable calibration on a new OOD dataset,
ACE uses an adaptive weighting method that strikes a balance between the two
extreme functions. When plugged in, ACE generally improves the performance of a
few state-of-the-art calibration schemes on a series of OOD benchmarks.
Importantly, such improvement does not come at the cost of the in-distribution
calibration accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoolPINNs: A Physics-informed Neural Network Modeling of Active Cooling
  in Vascular Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        N. V. Jagtap, M. K. Mudunuru, K. B. Nakshatrala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging technologies like hypersonic aircraft, space exploration vehicles,
and batteries avail fluid circulation in embedded microvasculatures for
efficient thermal regulation. Modeling is vital during these engineered
systems' design and operational phases. However, many challenges exist in
developing a modeling framework. What is lacking is an accurate framework that
(i) captures sharp jumps in the thermal flux across complex vasculature
layouts, (ii) deals with oblique derivatives (involving tangential and normal
components), (iii) handles nonlinearity because of radiative heat transfer,
(iv) provides a high-speed forecast for real-time monitoring, and (v)
facilitates robust inverse modeling. This paper addresses these challenges by
availing the power of physics-informed neural networks (PINNs). We develop a
fast, reliable, and accurate Scientific Machine Learning (SciML) framework for
vascular-based thermal regulation -- called CoolPINNs: a PINNs-based modeling
framework for active cooling. The proposed mesh-less framework elegantly
overcomes all the mentioned challenges. The significance of the reported
research is multi-fold. First, the framework is valuable for real-time
monitoring of thermal regulatory systems because of rapid forecasting. Second,
researchers can address complex thermoregulation designs inasmuch as the
approach is mesh-less. Finally, the framework facilitates systematic parameter
identification and inverse modeling studies, perhaps the current framework's
most significant utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Stashing Quantization for Efficient <span class="highlight-title">Transformer</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guo Yang, Daniel Lo, Robert Mullins, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive performance on a
range of Natural Language Processing (NLP) tasks. Unfortunately, the immense
amount of computations and memory accesses required for LLM training makes them
prohibitively expensive in terms of hardware cost, and thus challenging to
deploy in use cases such as on-device learning. In this paper, motivated by the
observation that LLM training is memory-bound, we propose a novel dynamic
quantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a
special focus on reducing the memory operations, but also enjoys the other
benefits of low precision training, such as the reduced arithmetic cost. We
conduct a thorough study on two translation tasks (trained-from-scratch) and
three classification tasks (fine-tuning). DSQ reduces the amount of arithmetic
operations by $20.95\times$ and the number of DRAM operations by $2.55\times$
on IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in
on-device learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast post-process Bayesian inference with Sparse Variational Bayesian
  Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkun Li, Grégoire Clarté, Luigi Acerbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sparse Variational Bayesian Monte Carlo (SVBMC), a method for
fast "post-process" Bayesian inference for models with black-box and
potentially noisy likelihoods. SVBMC reuses all existing target density
evaluations -- for example, from previous optimizations or partial Markov Chain
Monte Carlo runs -- to build a sparse Gaussian process (GP) surrogate model of
the log posterior density. Uncertain regions of the surrogate are then refined
via active learning as needed. Our work builds on the Variational Bayesian
Monte Carlo (VBMC) framework for sample-efficient inference, with several novel
contributions. First, we make VBMC scalable to a large number of pre-existing
evaluations via sparse GP regression, deriving novel Bayesian quadrature
formulae and acquisition functions for active learning with sparse GPs. Second,
we introduce noise shaping, a general technique to induce the sparse GP
approximation to focus on high posterior density regions. Third, we prove
theoretical results in support of the SVBMC refinement procedure. We validate
our method on a variety of challenging synthetic scenarios and real-world
applications. We find that SVBMC consistently builds good posterior
approximations by post-processing of existing model evaluations from different
sources, often requiring only a small number of additional density evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Certified Training and Robustness Verification of Neural ODEs <span class="chip">ICLR23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa Zeqiri, Mark Niklas Müller, Marc Fischer, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Ordinary Differential Equations (NODEs) are a novel neural
architecture, built around initial value problems with learned dynamics which
are solved during inference. Thought to be inherently more robust against
adversarial perturbations, they were recently shown to be vulnerable to strong
adversarial attacks, highlighting the need for formal guarantees. However,
despite significant progress in robustness verification for standard
feed-forward architectures, the verification of high dimensional NODEs remains
an open problem. In this work, we address this challenge and propose GAINS, an
analysis framework for NODEs combining three key ideas: (i) a novel class of
ODE solvers, based on variable but discrete time steps, (ii) an efficient graph
representation of solver trajectories, and (iii) a novel abstraction algorithm
operating on this graph representation. Together, these advances enable the
efficient analysis and certified training of high-dimensional NODEs, by
reducing the runtime from an intractable $O(\exp(d)+\exp(T))$ to ${O}(d+T^2
\log^2T)$ in the dimensionality $d$ and integration time $T$. In an extensive
evaluation on computer vision (MNIST and FMNIST) and time-series forecasting
(PHYSIO-NET) problems, we demonstrate the effectiveness of both our certified
training and verification methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-Aware Group Discrimination with Adaptive-View Graph Encoder: A
  Fast Graph Contrastive Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenshuo Zhang, Yun Zhu, Haizhou Shi, Siliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Albeit having gained significant progress lately, large-scale graph
representation learning remains expensive to train and deploy for two main
reasons: (i) the repetitive computation of multi-hop message passing and
non-linearity in graph neural networks (GNNs); (ii) the computational cost of
complex pairwise contrastive learning loss. Two main contributions are made in
this paper targeting this twofold challenge: we first propose an adaptive-view
graph neural encoder (AVGE) with a limited number of message passing to
accelerate the forward pass computation, and then we propose a structure-aware
group discrimination (SAGD) loss in our framework which avoids inefficient
pairwise loss computing in most common GCL and improves the performance of the
simple group discrimination. By the framework proposed, we manage to bring down
the training and inference cost on various large-scale datasets by a
significant margin (250x faster inference time) without loss of the
downstream-task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Contrast Maximization for Learning Sequential, Low-latency,
  Event-based Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Paredes-Vallés, Kirk Y. W. Scheper, Christophe De Wagter, Guido C. H. E. de Croon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras have recently gained significant traction since they open up
new avenues for low-latency and low-power solutions to complex computer vision
problems. To unlock these solutions, it is necessary to develop algorithms that
can leverage the unique nature of event data. However, the current
state-of-the-art is still highly influenced by the frame-based literature, and
usually fails to deliver on these promises. In this work, we take this into
consideration and propose a novel self-supervised learning pipeline for the
sequential estimation of event-based optical flow that allows for the scaling
of the models to high inference frequencies. At its core, we have a
continuously-running stateful neural model that is trained using a novel
formulation of contrast maximization that makes it robust to nonlinearities and
varying statistics in the input events. Results across multiple datasets
confirm the effectiveness of our method, which establishes a new state of the
art in terms of accuracy for approaches trained or optimized without ground
truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedREP: A Byzantine-Robust, Communication-Efficient and
  Privacy-Preserving Framework for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Rui Yang, Kun Wang, Wu-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has recently become a hot research topic, in which
Byzantine robustness, communication efficiency and privacy preservation are
three important aspects. However, the tension among these three aspects makes
it hard to simultaneously take all of them into account. In view of this
challenge, we theoretically analyze the conditions that a communication
compression method should satisfy to be compatible with existing
Byzantine-robust methods and privacy-preserving methods. Motivated by the
analysis results, we propose a novel communication compression method called
consensus sparsification (ConSpar). To the best of our knowledge, ConSpar is
the first communication compression method that is designed to be compatible
with both Byzantine-robust methods and privacy-preserving methods. Based on
ConSpar, we further propose a novel FL framework called FedREP, which is
Byzantine-robust, communication-efficient and privacy-preserving. We
theoretically prove the Byzantine robustness and the convergence of FedREP.
Empirical results show that FedREP can significantly outperform
communication-efficient privacy-preserving baselines. Furthermore, compared
with Byzantine-robust communication-efficient baselines, FedREP can achieve
comparable accuracy with the extra advantage of privacy preservation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time scheduling of renewable power systems through planning-based
  reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohuai Liu, Jinbo Liu, Weirui Ye, Nan Yang, Guanglun Zhang, Haiwang Zhong, Chongqing Kang, Qirong Jiang, Xuri Song, Fangchun Di, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing renewable energy sources have posed significant challenges to
traditional power scheduling. It is difficult for operators to obtain accurate
day-ahead forecasts of renewable generation, thereby requiring the future
scheduling system to make real-time scheduling decisions aligning with
ultra-short-term forecasts. Restricted by the computation speed, traditional
optimization-based methods can not solve this problem. Recent developments in
reinforcement learning (RL) have demonstrated the potential to solve this
challenge. However, the existing RL methods are inadequate in terms of
constraint complexity, algorithm performance, and environment fidelity. We are
the first to propose a systematic solution based on the state-of-the-art
reinforcement learning algorithm and the real power grid environment. The
proposed approach enables planning and finer time resolution adjustments of
power generators, including unit commitment and economic dispatch, thus
increasing the grid's ability to admit more renewable energy. The well-trained
scheduling agent significantly reduces renewable curtailment and load shedding,
which are issues arising from traditional scheduling's reliance on inaccurate
day-ahead forecasts. High-frequency control decisions exploit the existing
units' flexibility, reducing the power grid's dependence on hardware
transformations and saving investment and operating costs, as demonstrated in
experimental results. This research exhibits the potential of reinforcement
learning in promoting low-carbon and intelligent power systems and represents a
solid step toward sustainable electricity generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mastering Strategy Card Game (Hearthstone) with Improved Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changnan Xiao, Yongxin Zhang, Xuefeng Huang, Qinhan Huang, Jie Chen, Peng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategy card game is a well-known genre that is demanding on the intelligent
game-play and can be an ideal test-bench for AI. Previous work combines an
end-to-end policy function and an optimistic smooth fictitious play, which
shows promising performances on the strategy card game Legend of Code and
Magic. In this work, we apply such algorithms to Hearthstone, a famous
commercial game that is more complicated in game rules and mechanisms. We
further propose several improved techniques and consequently achieve
significant progress. For a machine-vs-human test we invite a Hearthstone
streamer whose best rank was top 10 of the official league in China region that
is estimated to be of millions of players. Our models defeat the human player
in all Best-of-5 tournaments of full games (including both deck building and
battle), showing a strong capability of decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOATS: Goal Sampling Adaptation for Scooping with Curriculum
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaru Niu, Shiyu Jin, Zeqing Zhang, Jiacheng Zhu, Ding Zhao, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we first formulate the problem of goal-conditioned robotic
water scooping with reinforcement learning. This task is challenging due to the
complex dynamics of fluid and multi-modal goal-reaching. The policy is required
to achieve both position goals and water amount goals, which leads to a large
convoluted goal state space. To address these challenges, we introduce Goal
Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning
method that can learn an effective and generalizable policy for robot scooping
tasks. Specifically, we use a goal-factorized reward formulation and
interpolate position goal distributions and amount goal distributions to create
curriculum through the learning process. As a result, our proposed method can
outperform the baselines in simulation and achieves 5.46% and 8.71% amount
errors on bowl scooping and bucket scooping tasks, respectively, under 1000
variations of initial water states in the tank and a large goal state space.
Besides being effective in simulation environments, our method can efficiently
generalize to noisy real-robot water-scooping scenarios with different physical
configurations and unseen settings, demonstrating superior efficacy and
generalizability. The videos of this work are available on our project page:
https://sites.google.com/view/goatscooping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for History-Aware Hyperparameter Optimisation in
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Marcelo Parra-Ullauri, Chen Zhen, Antonio García-Domínguez, Nelly Bencomo, Changgang Zheng, Juan Boubeta-Puig, Guadalupe Ortiz, Shufan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Reinforcement Learning (RL) system depends on a set of initial conditions
(hyperparameters) that affect the system's performance. However, defining a
good choice of hyperparameters is a challenging problem.
  Hyperparameter tuning often requires manual or automated searches to find
optimal values. Nonetheless, a noticeable limitation is the high cost of
algorithm evaluation for complex models, making the tuning process
computationally expensive and time-consuming.
  In this paper, we propose a framework based on integrating complex event
processing and temporal models, to alleviate these trade-offs. Through this
combination, it is possible to gain insights about a running RL system
efficiently and unobtrusively based on data stream monitoring and to create
abstract representations that allow reasoning about the historical behaviour of
the RL system. The obtained knowledge is exploited to provide feedback to the
RL system for optimising its hyperparameters while making effective use of
parallel resources.
  We introduce a novel history-aware epsilon-greedy logic for hyperparameter
optimisation that instead of using static hyperparameters that are kept fixed
for the whole training, adjusts the hyperparameters at runtime based on the
analysis of the agent's performance over time windows in a single agent's
lifetime. We tested the proposed approach in a 5G mobile communications case
study that uses DQN, a variant of RL, for its decision-making. Our experiments
demonstrated the effects of hyperparameter tuning using history on training
stability and reward values. The encouraging results show that the proposed
history-aware framework significantly improved performance compared to
traditional hyperparameter tuning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inversion dynamics of class manifolds in deep learning reveals tradeoffs
  underlying generalisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Ciceri, Lorenzo Cassani, Pierre Pizzochero, Matteo Osella, Pietro Rotondo, Marco Gherardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve near-zero training error in a classification problem, the layers
of a deep network have to disentangle the manifolds of data points with
different labels, to facilitate the discrimination. However, excessive class
separation can bring to overfitting since good generalisation requires learning
invariant features, which involve some level of entanglement. We report on
numerical experiments showing how the optimisation dynamics finds
representations that balance these opposing tendencies with a non-monotonic
trend. After a fast segregation phase, a slower rearrangement (conserved across
data sets and architectures) increases the class entanglement. The training
error at the inversion is remarkably stable under subsampling, and across
network initialisations and optimisers, which characterises it as a property
solely of the data structure and (very weakly) of the architecture. The
inversion is the manifestation of tradeoffs elicited by well-defined and
maximally stable elements of the training set, coined "stragglers",
particularly influential for generalisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary
  Dropouts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Agarwal, Deepak Gupta, Alexander Horsch, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world applications based on online learning produce streaming data
that is haphazard in nature, i.e., contains missing features, features becoming
obsolete in time, the appearance of new features at later points in time and a
lack of clarity on the total number of input features. These challenges make it
hard to build a learnable system for such applications, and almost no work
exists in deep learning that addresses this issue. In this paper, we present
Aux-Drop, an auxiliary dropout regularization strategy for online learning that
handles the haphazard input features in an effective manner. Aux-Drop adapts
the conventional dropout regularization scheme for the haphazard input feature
space ensuring that the final output is minimally impacted by the chaotic
appearance of such features. It helps to prevent the co-adaptation of
especially the auxiliary and base features, as well as reduces the strong
dependence of the output on any of the auxiliary inputs of the model. This
helps in better learning for scenarios where certain features disappear in time
or when new features are to be modeled. The efficacy of Aux-Drop has been
demonstrated through extensive numerical experiments on SOTA benchmarking
datasets that include Italy Power Demand, HIGGS, SUSY and multiple UCI
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Data Subset Selection For Efficient Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murad Tukan, Samson Zhou, Alaa Maalouf, Daniela Rus, Vladimir Braverman, Dan Feldman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radial basis function neural networks (\emph{RBFNN}) are {well-known} for
their capability to approximate any continuous function on a closed bounded set
with arbitrary precision given enough hidden neurons. In this paper, we
introduce the first algorithm to construct coresets for \emph{RBFNNs}, i.e.,
small weighted subsets that approximate the loss of the input data on any
radial basis function network and thus approximate any function defined by an
\emph{RBFNN} on the larger input data. In particular, we construct coresets for
radial basis and Laplacian loss functions. We then use our coresets to obtain a
provable data subset selection algorithm for training deep neural networks.
Since our coresets approximate every function, they also approximate the
gradient of each weight in a neural network, which is a particular function on
the input. We then perform empirical evaluations on function approximation and
dataset subset selection on popular network architectures and data sets,
demonstrating the efficacy and accuracy of our coreset construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESCL: Equivariant Self-Contrastive Learning for Sentence Representations <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Yixuan Liu, Xue Han, Chao Deng, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous contrastive learning methods for sentence representations often
focus on insensitive transformations to produce positive pairs, but neglect the
role of sensitive transformations that are harmful to semantic representations.
Therefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to
make full use of sensitive transformations, which encourages the learned
representations to be sensitive to certain types of transformations with an
additional equivariant learning task. Meanwhile, in order to improve
practicability and generality, ESCL simplifies the implementations of
traditional equivariant contrastive methods to share model parameters from the
perspective of multi-task learning. We evaluate our ESCL on semantic textual
similarity tasks. The proposed method achieves better results while using fewer
learning parameters compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The joint node degree distribution in the Erdős-Rényi network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshra Alarfaj, Charles Taylor, Leonid Bogachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Erd\H{o}s-R\'enyi random graph is the simplest model for node degree
distribution, and it is one of the most widely studied. In this model, pairs of
$n$ vertices are selected and connected uniformly at random with probability
$p$, consequently, the degrees for a given vertex follow the binomial
distribution. If the number of vertices is large, the binomial can be
approximated by Normal using the Central Limit Theorem, which is often allowed
when $\min (np, n(1-p)) > 5$. This is true for every node independently.
However, due to the fact that the degrees of nodes in a graph are not
independent, we aim in this paper to test whether the degrees of per node
collectively in the Erd\H{o}s-R\'enyi graph have a multivariate normal
distribution MVN. A chi square goodness of fit test for the hypothesis that
binomial is a distribution for the whole set of nodes is rejected because of
the dependence between degrees. Before testing MVN we show that the covariance
and correlation between the degrees of any pair of nodes in the graph are
$p(1-p)$ and $1/(n-1)$, respectively. We test MVN considering two assumptions:
independent and dependent degrees, and we obtain our results based on the
percentages of rejected statistics of chi square, the $p$-values of Anderson
Darling test, and a CDF comparison. We always achieve a good fit of
multivariate normal distribution with large values of $n$ and $p$, and very
poor fit when $n$ or $p$ are very small. The approximation seems valid when $np
\geq 10$. We also compare the maximum likelihood estimate of $p$ in MVN
distribution where we assume independence and dependence. The estimators are
assessed using bias, variance and mean square error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropic Wasserstein Component Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Collas, Titouan Vayer, Rémi Flamary, Arnaud Breloy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimension reduction (DR) methods provide systematic approaches for analyzing
high-dimensional data. A key requirement for DR is to incorporate global
dependencies among original and embedded samples while preserving clusters in
the embedding space. To achieve this, we combine the principles of optimal
transport (OT) and principal component analysis (PCA). Our method seeks the
best linear subspace that minimizes reconstruction error using entropic OT,
which naturally encodes the neighborhood information of the samples. From an
algorithmic standpoint, we propose an efficient block-majorization-minimization
solver over the Stiefel manifold. Our experimental results demonstrate that our
approach can effectively preserve high-dimensional clusters, leading to more
interpretable and effective embeddings. Python code of the algorithms and
experiments is available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLCA: Slow Learner with Classifier Alignment for Continual Learning on a
  <span class="highlight-title">Pre-train</span>ed Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmentation method for cerebral blood vessels from MRA using hysteresis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgia Kenyon, Stephan Lau, Michael A. Chappell, Mark Jenkinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of cerebral blood vessels from Magnetic Resonance Imaging (MRI)
is an open problem that could be solved with deep learning (DL). However,
annotated data for training is often scarce. Due to the absence of open-source
tools, we aim to develop a classical segmentation method that generates vessel
ground truth from Magnetic Resonance Angiography for DL training of
segmentation across a variety of modalities. The method combines size-specific
Hessian filters, hysteresis thresholding and connected component correction.
The optimal choice of processing steps was evaluated with a blinded scoring by
a clinician using 24 3D images. The results show that all method steps are
necessary to produce the highest (14.2/15) vessel segmentation quality score.
Omitting the connected component correction caused the largest quality loss.
The method, which is available on GitHub, can be used to train DL models for
vessel segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleDiff: Attribute Comparison Between Unlabeled <span class="highlight-title">Dataset</span>s in Latent
  Disentangled Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keisuke Kawano, Takuro Kutsuna, Ryoko Tokuhisa, Akihiro Nakamura, Yasushi Esaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One major challenge in machine learning applications is coping with
mismatches between the datasets used in the development and those obtained in
real-world applications. These mismatches may lead to inaccurate predictions
and errors, resulting in poor product quality and unreliable systems. In this
study, we propose StyleDiff to inform developers of the differences between the
two datasets for the steady development of machine learning systems. Using
disentangled image spaces obtained from recently proposed generative models,
StyleDiff compares the two datasets by focusing on attributes in the images and
provides an easy-to-understand analysis of the differences between the
datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the
size of the datasets and $d$ is the number of attributes, enabling the
application to large datasets. We demonstrate that StyleDiff accurately detects
differences between datasets and presents them in an understandable format
using, for example, driving scenes datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Stochastic Gradient Riemannian Langevin Dynamics in
  Non-Diagonal Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Arto Klami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural network inference is often carried out using stochastic
gradient sampling methods. For best performance the methods should use a
Riemannian metric that improves posterior exploration by accounting for the
local curvature, but the existing methods resort to simple diagonal metrics to
remain computationally efficient. This loses some of the gains. We propose two
non-diagonal metrics that can be used in stochastic samplers to improve
convergence and exploration but that have only a minor computational overhead
over diagonal metrics. We show that for neural networks with complex
posteriors, caused e.g. by use of sparsity-inducing priors, using these metrics
provides clear improvements. For some other choices the posterior is
sufficiently easy also for the simpler metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Sparse Linear Algebra Through Automatic Format Selection and
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christodoulos Stylianou, Michele Weiland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse matrices are an integral part of scientific simulations. As hardware
evolves new sparse matrix storage formats are proposed aiming to exploit
optimizations specific to the new hardware. In the era of heterogeneous
computing, users often are required to use multiple formats for their
applications to remain optimal across the different available hardware,
resulting in larger development times and maintenance overhead. A potential
solution to this problem is the use of a lightweight auto-tuner driven by
Machine Learning (ML) that would select for the user an optimal format from a
pool of available formats that will match the characteristics of the sparsity
pattern, target hardware and operation to execute.
  In this paper, we introduce Morpheus-Oracle, a library that provides a
lightweight ML auto-tuner capable of accurately predicting the optimal format
across multiple backends, targeting the major HPC architectures aiming to
eliminate any format selection input by the end-user. From more than 2000
real-life matrices, we achieve an average classification accuracy and balanced
accuracy of 92.63% and 80.22% respectively across the available systems. The
adoption of the auto-tuner results in average speedup of 1.1x on CPUs and 1.5x
to 8x on NVIDIA and AMD GPUs, with maximum speedups reaching up to 7x and 1000x
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward Informed Dreamer for Task Generalization in Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu, Jialian Li, Dong Yan, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A long-standing goal of reinforcement learning is that algorithms can learn
on training tasks and generalize well on unseen tasks like humans, where
different tasks share similar dynamic with different reward functions. A
general challenge is that it is nontrivial to quantitatively measure the
similarities between these different tasks, which is vital for analyzing the
task distribution and further designing algorithms with stronger
generalization. To address this, we present a novel metric named Task
Distribution Relevance (TDR) via optimal Q functions to capture the relevance
of the task distribution quantitatively. In the case of tasks with a high TDR,
i.e., the tasks differ significantly, we demonstrate that the Markovian
policies cannot distinguish them, yielding poor performance accordingly. Based
on this observation, we propose a framework of Reward Informed Dreamer (RID)
with reward-informed world models, which captures invariant latent features
over tasks and encodes reward signals into policies for distinguishing
different tasks. In RID, we calculate the corresponding variational lower bound
of the log-likelihood on the data, which includes a novel term to distinguish
different tasks via states, based on reward-informed world models. Finally,
extensive experiments in DeepMind control suite demonstrate that RID can
significantly improve the performance of handling different tasks at the same
time, especially for those with high TDR, and further generalize to unseen
tasks effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification of Systematic Errors of Image Classifiers on Rare
  Subgroups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Hendrik Metzen, Robin Hutmacher, N. Grace Hua, Valentyn Boreiko, Dan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite excellent average-case performance of many image classifiers, their
performance can substantially deteriorate on semantically coherent subgroups of
the data that were under-represented in the training data. These systematic
errors can impact both fairness for demographic minority groups as well as
robustness and safety under domain shift. A major challenge is to identify such
subgroups with subpar performance when the subgroups are not annotated and
their occurrence is very rare. We leverage recent advances in text-to-image
models and search in the space of textual descriptions of subgroups ("prompts")
for subgroups where the target model has low performance on the
prompt-conditioned synthesized data. To tackle the exponentially growing number
of subgroups, we employ combinatorial testing. We denote this procedure as
PromptAttack as it can be interpreted as an adversarial attack in a prompt
space. We study subgroup coverage and identifiability with PromptAttack in a
controlled setting and find that it identifies systematic errors with high
accuracy. Thereupon, we apply PromptAttack to ImageNet classifiers and identify
novel systematic errors on rare subgroups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conceptual Reinforcement Learning for Language-Conditioned Tasks <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohui Peng, Xing Hu, Rui Zhang, Jiaming Guo, Qi Yi, Ruizhi Chen, Zidong Du, Ling Li, Qi Guo, Yunji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the broad application of deep reinforcement learning (RL),
transferring and adapting the policy to unseen but similar environments is
still a significant challenge. Recently, the language-conditioned policy is
proposed to facilitate policy transfer through learning the joint
representation of observation and text that catches the compact and invariant
information across environments. Existing studies of language-conditioned RL
methods often learn the joint representation as a simple latent layer for the
given instances (episode-specific observation and text), which inevitably
includes noisy or irrelevant information and cause spurious correlations that
are dependent on instances, thus hurting generalization performance and
training efficiency. To address this issue, we propose a conceptual
reinforcement learning (CRL) framework to learn the concept-like joint
representation for language-conditioned policy. The key insight is that
concepts are compact and invariant representations in human cognition through
extracting similarities from numerous instances in real-world. In CRL, we
propose a multi-level attention encoder and two mutual information constraints
for learning compact and invariant concepts. Verified in two challenging
environments, RTFM and Messenger, CRL significantly improves the training
efficiency (up to 70%) and generalization ability (up to 30%) to the new
environment dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Federated Learning for Collaborative Intelligence in Massive IoT
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanli Ni, Jingheng Zheng, Hui Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing existing federated learning in massive Internet of Things (IoT)
networks faces critical challenges such as imbalanced and statistically
heterogeneous data and device diversity. To this end, we propose a
semi-federated learning (SemiFL) framework to provide a potential solution for
the realization of intelligent IoT. By seamlessly integrating the centralized
and federated paradigms, our SemiFL framework shows high scalability in terms
of the number of IoT devices even in the presence of computing-limited sensors.
Furthermore, compared to traditional learning approaches, the proposed SemiFL
can make better use of distributed data and computing resources, due to the
collaborative model training between the edge server and local devices.
Simulation results show the effectiveness of our SemiFL framework for massive
IoT networks. The code can be found at https://github.com/niwanli/SemiFL_IoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for publication in the IEEE Internet of
  Things Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invertible Kernel PCA with Random Fourier Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Gedon, Antôni H. Ribeiro, Niklas Wahlström, Thomas B. Schön
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel principal component analysis (kPCA) is a widely studied method to
construct a low-dimensional data representation after a nonlinear
transformation. The prevailing method to reconstruct the original input signal
from kPCA -- an important task for denoising -- requires us to solve a
supervised learning problem. In this paper, we present an alternative method
where the reconstruction follows naturally from the compression step. We first
approximate the kernel with random Fourier features. Then, we exploit the fact
that the nonlinear transformation is invertible in a certain subdomain. Hence,
the name \emph{invertible kernel PCA (ikPCA)}. We experiment with different
data modalities and show that ikPCA performs similarly to kPCA with supervised
reconstruction on denoising tasks, making it a strong alternative.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Contextual Structure to <span class="highlight-title">Generate</span> Useful Auxiliary Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedict Quartey, Ankit Shah, George Konidaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning requires interaction with an environment, which is
expensive for robots. This constraint necessitates approaches that work with
limited environmental interaction by maximizing the reuse of previous
experiences. We propose an approach that maximizes experience reuse while
learning to solve a given task by generating and simultaneously learning useful
auxiliary tasks. To generate these tasks, we construct an abstract temporal
logic representation of the given task and leverage large language models to
generate context-aware object embeddings that facilitate object replacements.
Counterfactual reasoning and off-policy methods allow us to simultaneously
learn these auxiliary tasks while solving the given target task. We combine
these insights into a novel framework for multitask reinforcement learning and
experimentally show that our generated auxiliary tasks share similar underlying
exploration requirements as the given task, thereby maximizing the utility of
directed exploration. Our approach allows agents to automatically learn
additional useful policies without extra environment interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gauges and Accelerated Optimization over Smooth and/or Strongly Convex
  Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Liu, Benjamin Grimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider feasibility and constrained optimization problems defined over
smooth and/or strongly convex sets. These notions mirror their popular function
counterparts but are much less explored in the first-order optimization
literature. We propose new scalable, projection-free, accelerated first-order
methods in these settings. Our methods avoid linear optimization or projection
oracles, only using cheap one-dimensional linesearches and normal vector
computations. Despite this, we derive optimal accelerated convergence
guarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smooth
problems, and accelerated linear convergence given both. Our algorithms and
analysis are based on novel characterizations of the Minkowski gauge of smooth
and/or strongly convex sets, which may be of independent interest: although the
gauge is neither smooth nor strongly convex, we show the gauge squared inherits
any structure present in the set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22pages (32pages with references and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-distribution Detection with Implicit Outlier Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhou Wang, Junjie Ye, Feng Liu, Quanyu Dai, Marcus Kalander, Tongliang Liu, Jianye Hao, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outlier exposure (OE) is powerful in out-of-distribution (OOD) detection,
enhancing detection capability via model fine-tuning with surrogate OOD data.
However, surrogate data typically deviate from test OOD data. Thus, the
performance of OE, when facing unseen OOD data, can be weakened. To address
this issue, we propose a novel OE-based approach that makes the model perform
well for unseen OOD situations, even for unseen OOD cases. It leads to a
min-max learning scheme -- searching to synthesize OOD data that leads to worst
judgments and learning from such OOD data for uniform performance in OOD
detection. In our realization, these worst OOD data are synthesized by
transforming original surrogate ones. Specifically, the associated transform
functions are learned implicitly based on our novel insight that model
perturbation leads to data transformation. Our methodology offers an efficient
way of synthesizing OOD data, which can further benefit the detection model,
besides the surrogate OOD data. We conduct extensive experiments under various
OOD detection setups, demonstrating the effectiveness of our method against its
advanced counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSL^2: <span class="highlight-title">Self-Supervised</span> Learning meets Semi-Supervised Learning: Multiple
  Sclerosis Segmentation in 7T-MRI from large-scale 3T-MRI <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wang, Hao Li, Han Liu, Dewei Hu, Daiwei Lu, Keejin Yoon, Kelsey Barter, Francesca Bagnato, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated segmentation of multiple sclerosis (MS) lesions from MRI scans is
important to quantify disease progression. In recent years, convolutional
neural networks (CNNs) have shown top performance for this task when a large
amount of labeled data is available. However, the accuracy of CNNs suffers when
dealing with few and/or sparsely labeled datasets. A potential solution is to
leverage the information available in large public datasets in conjunction with
a target dataset which only has limited labeled data. In this paper, we propose
a training framework, SSL2 (self-supervised-semi-supervised), for
multi-modality MS lesion segmentation with limited supervision. We adopt
self-supervised learning to leverage the knowledge from large public 3T
datasets to tackle the limitations of a small 7T target dataset. To leverage
the information from unlabeled 7T data, we also evaluate state-of-the-art
semi-supervised methods for other limited annotation settings, such as small
labeled training size and sparse annotations. We use the shifted-window (Swin)
transformer1 as our backbone network. The effectiveness of self-supervised and
semi-supervised training strategies is evaluated in our in-house 7T MRI
dataset. The results indicate that each strategy improves lesion segmentation
for both limited training data size and for sparse labeling scenarios. The
combined overall framework further improves the performance substantially
compared to either of its components alone. Our proposed framework thus
provides a promising solution for future data/label-hungry 7T MS studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Society for Optics and Photonics -
  Medical Imaging (SPIE-MI) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phase transition for detecting a small community in a large network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashun Jin, Zheng Tracy Ke, Paxton Turner, Anru R. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to detect a small community in a large network is an interesting problem,
including clique detection as a special case, where a naive degree-based
$\chi^2$-test was shown to be powerful in the presence of an Erd\H{o}s-Renyi
background. Using Sinkhorn's theorem, we show that the signal captured by the
$\chi^2$-test may be a modeling artifact, and it may disappear once we replace
the Erd\H{o}s-Renyi model by a broader network model. We show that the recent
SgnQ test is more appropriate for such a setting. The test is optimal in
detecting communities with sizes comparable to the whole network, but has never
been studied for our setting, which is substantially different and more
challenging. Using a degree-corrected block model (DCBM), we establish phase
transitions of this testing problem concerning the size of the small community
and the edge densities in small and large communities. When the size of the
small community is larger than $\sqrt{n}$, the SgnQ test is optimal for it
attains the computational lower bound (CLB), the information lower bound for
methods allowing polynomial computation time. When the size of the small
community is smaller than $\sqrt{n}$, we establish the parameter regime where
the SgnQ test has full power and make some conjectures of the CLB. We also
study the classical information lower bound (LB) and show that there is always
a gap between the CLB and LB in our range of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Regret Bounds for Online Kernel Selection under Bandit Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfan Li, Shizhong Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we improve the regret bound for online kernel selection under
bandit feedback. Previous algorithm enjoys a $O((\Vert
f\Vert^2_{\mathcal{H}_i}+1)K^{\frac{1}{3}}T^{\frac{2}{3}})$ expected bound for
Lipschitz loss functions. We prove two types of regret bounds improving the
previous bound. For smooth loss functions, we propose an algorithm with a
$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$
expected bound where $L_T(f^\ast_i)$ is the cumulative losses of optimal
hypothesis in $\mathbb{H}_{i}=\{f\in\mathcal{H}_i:\Vert
f\Vert_{\mathcal{H}_i}\leq U\}$. The data-dependent bound keeps the previous
worst-case bound and is smaller if most of candidate kernels match well with
the data. For Lipschitz loss functions, we propose an algorithm with a
$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$ expected bound asymptotically improving the
previous bound. We apply the two algorithms to online kernel selection with
time constraint and prove new regret bounds matching or improving the previous
$O(\sqrt{T\ln{K}} +\Vert
f\Vert^2_{\mathcal{H}_i}\max\{\sqrt{T},\frac{T}{\sqrt{\mathcal{R}}}\})$
expected bound where $\mathcal{R}$ is the time budget. Finally, we empirically
verify our algorithms on online regression and classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Representation for Anomaly Detection of Vehicle Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Jiao, Juyang Bai, Xiangguo Liu, Takami Sato, Xiaowei Yuan, Qi Alfred Chen, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future trajectories of surrounding vehicles based on their
history trajectories is a critical task in autonomous driving. However, when
small crafted perturbations are introduced to those history trajectories, the
resulting anomalous (or adversarial) trajectories can significantly mislead the
future trajectory prediction module of the ego vehicle, which may result in
unsafe planning and even fatal accidents. Therefore, it is of great importance
to detect such anomalous trajectories of the surrounding vehicles for system
safety, but few works have addressed this issue. In this work, we propose two
novel methods for learning effective and efficient representations for online
anomaly detection of vehicle trajectories. Different from general time-series
anomaly detection, anomalous vehicle trajectory detection deals with much
richer contexts on the road and fewer observable patterns on the anomalous
trajectories themselves. To address these challenges, our methods exploit
contrastive learning techniques and trajectory semantics to capture the
patterns underlying the driving scenarios for effective anomaly detection under
supervised and unsupervised settings, respectively. We conduct extensive
experiments to demonstrate that our supervised method based on contrastive
learning and unsupervised method based on reconstruction with semantic latent
space can significantly improve the performance of anomalous trajectory
detection in their corresponding settings over various baseline methods. We
also demonstrate our methods' generalization ability to detect unseen patterns
of anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages paper, in anomaly detection of vehicle trajectory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvature-Sensitive Predictive Coding with Approximate Laplace Monte
  Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umais Zahid, Qinghai Guo, Karl Friston, Zafeirios Fountas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive coding (PC) accounts of perception now form one of the dominant
computational theories of the brain, where they prescribe a general algorithm
for inference and learning over hierarchical latent probabilistic models.
Despite this, they have enjoyed little export to the broader field of machine
learning, where comparative generative modelling techniques have flourished. In
part, this has been due to the poor performance of models trained with PC when
evaluated by both sample quality and marginal likelihood. By adopting the
perspective of PC as a variational Bayes algorithm under the Laplace
approximation, we identify the source of these deficits to lie in the exclusion
of an associated Hessian term in the PC objective function, which would
otherwise regularise the sharpness of the probability landscape and prevent
over-certainty in the approximate posterior. To remedy this, we make three
primary contributions: we begin by suggesting a simple Monte Carlo estimated
evidence lower bound which relies on sampling from the Hessian-parameterised
variational posterior. We then derive a novel block diagonal approximation to
the full Hessian matrix that has lower memory requirements and favourable
mathematical properties. Lastly, we present an algorithm that combines our
method with standard PC to reduce memory complexity further. We evaluate models
trained with our approach against the standard PC framework on image benchmark
datasets. Our approach produces higher log-likelihoods and qualitatively better
samples that more closely capture the diversity of the data-generating
distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blackwell's Approachability with Time-Dependent Outcome Functions and
  Dot Products. Application to the Big Match 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joon Kwon, Bruno Ziliotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blackwell's approachability is a very general sequential decision framework
where a Decision Maker obtains vector-valued outcomes, and aims at the
convergence of the average outcome to a given "target" set. Blackwell gave a
sufficient condition for the decision maker having a strategy guaranteeing such
a convergence against an adversarial environment, as well as what we now call
the Blackwell's algorithm, which then ensures convergence. Blackwell's
approachability has since been applied to numerous problems, in online learning
and game theory, in particular. We extend this framework by allowing the
outcome function and the dot product to be time-dependent. We establish a
general guarantee for the natural extension to this framework of Blackwell's
algorithm. In the case where the target set is an orthant, we present a family
of time-dependent dot products which yields different convergence speeds for
each coordinate of the average outcome. We apply this framework to the Big
Match (one of the most important toy examples of stochastic games) where an
$\epsilon$-uniformly optimal strategy for Player I is given by Blackwell's
algorithm in a well-chosen auxiliary approachability problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF
  Grasp Pose Synthesis on RGB-D input <span class="chip">IROS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Ruinian Xu, Yunzhi Lin, Patricio A. Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based
on keypoints. Keypoint-based grasp detector from image input has demonstrated
promising results in the previous study, where the additional visual
information provided by color images compensates for the noisy depth
perception. However, it relies heavily on accurately predicting the location of
keypoints in the image space. In this paper, we devise a new grasp generation
network that reduces the dependency on precise keypoint estimation. Given an
RGB-D input, our network estimates both the grasp pose from keypoint detection
as well as scale towards the camera. We further re-design the keypoint output
space in order to mitigate the negative impact of keypoint prediction noise to
Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method
outperforms the baseline by a large margin, validating the efficacy of our
approach. Finally, despite trained on simple synthetic objects, our method
demonstrate sim-to-real capacity by showing competitive results in real-world
robot experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Data Augmentation Scheme for Model Predictive Control Policy
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinesh Krishnamoorthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of data generation for MPC policy
approximation. Learning an approximate MPC policy from expert demonstrations
requires a large data set consisting of optimal state-action pairs, sampled
across the feasible state space. Yet, the key challenge of efficiently
generating the training samples has not been studied widely. Recently, a
sensitivity-based data augmentation framework for MPC policy approximation was
proposed, where the parametric sensitivities are exploited to cheaply generate
several additional samples from a single offline MPC computation. The error due
to augmenting the training data set with inexact samples was shown to increase
with the size of the neighborhood around each sample used for data
augmentation. Building upon this work, this letter paper presents an improved
data augmentation scheme based on predictor-corrector steps that enforces a
user-defined level of accuracy, and shows that the error bound of the augmented
samples are independent of the size of the neighborhood used for data
augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variance-aware robust reinforcement learning with linear function
  approximation with heavy-tailed rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Qiang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two algorithms, AdaOFUL and VARA, for online sequential
decision-making in the presence of heavy-tailed rewards with only finite
variances. For linear stochastic bandits, we address the issue of heavy-tailed
rewards by modifying the adaptive Huber regression and proposing AdaOFUL.
AdaOFUL achieves a state-of-the-art regret bound of
$\widetilde{\mathcal{O}}\big(d\big(\sum_{t=1}^T \nu_{t}^2\big)^{1/2}+d\big)$ as
if the rewards were uniformly bounded, where $\nu_{t}^2$ is the observed
conditional variance of the reward at round $t$, $d$ is the feature dimension,
and $\widetilde{\mathcal{O}}(\cdot)$ hides logarithmic dependence. Building
upon AdaOFUL, we propose VARA for linear MDPs, which achieves a tighter
variance-aware regret bound of
$\widetilde{\mathcal{O}}(d\sqrt{H\mathcal{G}^*K})$. Here, $H$ is the length of
episodes, $K$ is the number of episodes, and $\mathcal{G}^*$ is a smaller
instance-dependent quantity that can be bounded by other instance-dependent
quantities when additional structural conditions on the MDP are satisfied. Our
regret bound is superior to the current state-of-the-art bounds in three ways:
(1) it depends on a tighter instance-dependent quantity and has optimal
dependence on $d$ and $H$, (2) we can obtain further instance-dependent bounds
of $\mathcal{G}^*$ under additional structural conditions on the MDP, and (3)
our regret bound is valid even when rewards have only finite variances,
achieving a level of generality unmatched by previous works. Overall, our
modified adaptive Huber regression algorithm may serve as a useful building
block in the design of algorithms for online problems with heavy-tailed
rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 page main text, 42 page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Wrong Lessons: Inserting Trojans During <span class="highlight-title">Knowledge</span>
  Distillation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Tang, Tom Shlomi, Alexander Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, knowledge distillation has become a cornerstone of
efficiently deployed machine learning, with labs and industries using knowledge
distillation to train models that are inexpensive and resource-optimized.
Trojan attacks have contemporaneously gained significant prominence, revealing
fundamental vulnerabilities in deep learning models. Given the widespread use
of knowledge distillation, in this work we seek to exploit the unlabelled data
knowledge distillation process to embed Trojans in a student model without
introducing conspicuous behavior in the teacher. We ultimately devise a Trojan
attack that effectively reduces student accuracy, does not alter teacher
performance, and is efficiently constructible in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization analysis of an unfolding network for analysis-based
  Compressed Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Kouni, Yannis Panagakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unfolding networks have shown promising results in the Compressed Sensing
(CS) field. Yet, the investigation of their generalization ability is still in
its infancy. In this paper, we perform generalization analysis of a
state-of-the-art ADMM-based unfolding network, which jointly learns a decoder
for CS and a sparsifying redundant analysis operator. To this end, we first
impose a structural constraint on the learnable sparsifier, which parametrizes
the network's hypothesis class. For the latter, we estimate its Rademacher
complexity. With this estimate in hand, we deliver generalization error bounds
for the examined network. Finally, the validity of our theory is assessed and
numerical comparisons to a state-of-the-art unfolding network are made, on
synthetic and real-world datasets. Our experimental results demonstrate that
our proposed framework complies with our theoretical findings and outperforms
the baseline, consistently for all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploration of the search space of Gaussian graphical models for paired
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Roverato, Dung Ngoc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning a Gaussian graphical model in the case
where the observations come from two dependent groups sharing the same
variables. We focus on a family of coloured Gaussian graphical models
specifically suited for the paired data problem. Commonly, graphical models are
ordered by the submodel relationship so that the search space is a lattice,
called the model inclusion lattice. We introduce a novel order between models,
named the twin order. We show that, embedded with this order, the model space
is a lattice that, unlike the model inclusion lattice, is distributive.
Furthermore, we provide the relevant rules for the computation of the
neighbours of a model. The latter are more efficient than the same operations
in the model inclusion lattice, and are then exploited to achieve a more
efficient exploration of the search space. These results can be applied to
improve the efficiency of both greedy and Bayesian model search procedures.
Here we implement a stepwise backward elimination procedure and evaluate its
performance by means of simulations. Finally, the procedure is applied to learn
a brain network from fMRI data where the two groups correspond to the left and
right hemispheres, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal active particle navigation meets machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Nasiri, Hartmut Löwen, Benno Liebchen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The question of how "smart" active agents, like insects, microorganisms, or
future colloidal robots need to steer to optimally reach or discover a target,
such as an odor source, food, or a cancer cell in a complex environment has
recently attracted great interest. Here, we provide an overview of recent
developments, regarding such optimal navigation problems, from the micro- to
the macroscale, and give a perspective by discussing some of the challenges
which are ahead of us. Besides exemplifying an elementary approach to optimal
navigation problems, the article focuses on works utilizing machine
learning-based methods. Such learning-based approaches can uncover highly
efficient navigation strategies even for problems that involve e.g. chaotic,
high-dimensional, or unknown environments and are hardly solvable based on
conventional analytical or simulation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EfficientTempNet: Temporal Super-Resolution of Radar Rainfall <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bekir Z Demiray, Muhammed Sit, Ibrahim Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rainfall data collected by various remote sensing instruments such as radars
or satellites has different space-time resolutions. This study aims to improve
the temporal resolution of radar rainfall products to help with more accurate
climate change modeling and studies. In this direction, we introduce a solution
based on EfficientNetV2, namely EfficientTempNet, to increase the temporal
resolution of radar-based rainfall products from 10 minutes to 5 minutes. We
tested EfficientRainNet over a dataset for the state of Iowa, US, and compared
its performance to three different baselines to show that EfficientTempNet
presents a viable option for better climate change monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a workshop paper at Tackling Climate Change with Machine
  Learning, ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Paper on <span class="highlight-title">Dataset</span> Engineering to Accelerate Science <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilio Vital Brazil, Eduardo Soares, Lucas Villa Real, Leonardo Azevedo, Vinicius Segura, Luiz Zerkowski, Renato Cerqueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a critical element in any discovery process. In the last decades, we
observed exponential growth in the volume of available data and the technology
to manipulate it. However, data is only practical when one can structure it for
a well-defined task. For instance, we need a corpus of text broken into
sentences to train a natural language machine-learning model. In this work, we
will use the token \textit{dataset} to designate a structured set of data built
to perform a well-defined task. Moreover, the dataset will be used in most
cases as a blueprint of an entity that at any moment can be stored as a table.
Specifically, in science, each area has unique forms to organize, gather and
handle its datasets. We believe that datasets must be a first-class entity in
any knowledge-intensive process, and all workflows should have exceptional
attention to datasets' lifecycle, from their gathering to uses and evolution.
We advocate that science and engineering discovery processes are extreme
instances of the need for such organization on datasets, claiming for new
approaches and tooling. Furthermore, these requirements are more evident when
the discovery workflow uses artificial intelligence methods to empower the
subject-matter expert. In this work, we discuss an approach to bringing
datasets as a critical entity in the discovery process in science. We
illustrate some concepts using material discovery as a use case. We chose this
domain because it leverages many significant problems that can be generalized
to other science fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 2nd Annual AAAI Workshop on AI to Accelerate Science and
  Engineering (AI2ASE)
  https://ai-2-ase.github.io/papers/16%5cSubmission%5cAAAI_Dataset_Engineering-8.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Online Reinforcement Learning with Offline Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02948v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02948v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip J. Ball, Laura Smith, Ilya Kostrikov, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficiency and exploration remain major challenges in online
reinforcement learning (RL). A powerful approach that can be applied to address
these issues is the inclusion of offline data, such as prior trajectories from
a human expert or a sub-optimal exploration policy. Previous methods have
relied on extensive modifications and additional complexity to ensure the
effective use of this data. Instead, we ask: can we simply apply existing
off-policy methods to leverage offline data when learning online? In this work,
we demonstrate that the answer is yes; however, a set of minimal but important
changes to existing off-policy RL algorithms are required to achieve reliable
performance. We extensively ablate these design choices, demonstrating the key
factors that most affect performance, and arrive at a set of recommendations
that practitioners can readily apply, whether their data comprise a small
number of expert demonstrations or large volumes of sub-optimal trajectories.
We see that correct application of these simple recommendations can provide a
$\mathbf{2.5\times}$ improvement over existing approaches across a diverse set
of competitive benchmarks, with no additional computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To reproduce our results and use our codebase, see
  https://github.com/ikostrikov/rlpd</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01850v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01850v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Potter, Benny Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to integrate weapon system features (such as weapon system
manufacturer, deployment time and location, storage time and location, etc.)
into a parameterized Cox-Weibull [1] reliability model via a neural network,
like DeepSurv [2], to improve predictive maintenance. In parallel, we develop
an alternative Bayesian model by parameterizing the Weibull parameters with a
neural network and employing dropout methods such as Monte-Carlo (MC)-dropout
for comparative purposes. Due to data collection procedures in weapon system
testing we employ a novel interval-censored log-likelihood which incorporates
Monte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters during
gradient descent optimization. We compare classification metrics such as
receiver operator curve (ROC) area under the curve (AUC), precision-recall (PR)
AUC, and F scores to show our model generally outperforms traditional powerful
models such as XGBoost and the current standard conditional Weibull probability
density estimation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print with minor revisions, published at The 69th Annual
  Reliability and Maintainability Symposium, January 23-26, 2023, FL, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Universal Causal Deep Learning Models: The Geometric
  (Hyper)<span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Acciaio, Anastasis Kratsios, Gudmund Pammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several problems in stochastic analysis are defined through their geometry,
and preserving that geometric structure is essential to generating meaningful
predictions. Nevertheless, how to design principled deep learning (DL) models
capable of encoding these geometric structures remains largely unknown. We
address this open problem by introducing a universal causal geometric DL
framework in which the user specifies a suitable pair of metric spaces
$\mathscr{X}$ and $\mathscr{Y}$ and our framework returns a DL model capable of
causally approximating any ``regular'' map sending time series in
$\mathscr{X}^{\mathbb{Z}}$ to time series in $\mathscr{Y}^{\mathbb{Z}}$ while
respecting their forward flow of information throughout time. Suitable
geometries on $\mathscr{Y}$ include various (adapted) Wasserstein spaces
arising in optimal stopping problems, a variety of statistical manifolds
describing the conditional distribution of continuous-time finite state Markov
chains, and all Fr\'{e}chet spaces admitting a Schauder basis, e.g. as in
classical finance. Suitable spaces $\mathscr{X}$ are compact subsets of any
Euclidean space. Our results all quantitatively express the number of
parameters needed for our DL model to achieve a given approximation error as a
function of the target map's regularity and the geometric structure both of
$\mathscr{X}$ and of $\mathscr{Y}$. Even when omitting any temporal structure,
our universal approximation theorems are the first guarantees that H\"{o}lder
functions, defined between such $\mathscr{X}$ and $\mathscr{Y}$ can be
approximated by DL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Body: 31 Pages, Proofs: 16 Pages, Figures: 13, Tables: 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Safe Reinforcement Learning with Step-wise Violation
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuoya Xiong, Yihan Du, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate a novel safe reinforcement learning problem
with step-wise violation constraints. Our problem differs from existing works
in that we consider stricter step-wise violation constraints and do not assume
the existence of safe actions, making our formulation more suitable for
safety-critical applications which need to ensure safety in all decision steps
and may not always possess safe actions, e.g., robot control and autonomous
driving. We propose a novel algorithm SUCBVI, which guarantees
$\widetilde{O}(\sqrt{ST})$ step-wise violation and
$\widetilde{O}(\sqrt{H^3SAT})$ regret. Lower bounds are provided to validate
the optimality in both violation and regret performance with respect to $S$ and
$T$. Moreover, we further study a novel safe reward-free exploration problem
with step-wise violation constraints. For this problem, we design an
$(\varepsilon,\delta)$-PAC algorithm SRF-UCRL, which achieves nearly
state-of-the-art sample complexity
$\widetilde{O}((\frac{S^2AH^2}{\varepsilon}+\frac{H^4SA}{\varepsilon^2})(\log(\frac{1}{\delta})+S))$,
and guarantees $\widetilde{O}(\sqrt{ST})$ violation during the exploration. The
experimental results demonstrate the superiority of our algorithms in safety
performance, and corroborate our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matching Map Recovery with an Unknown Number of Outliers <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshak Minasyan, Tigran Galstyan, Sona Hunanyan, Arnak Dalalyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding the matching map between two sets of
$d$-dimensional noisy feature-vectors. The distinctive feature of our setting
is that we do not assume that all the vectors of the first set have their
corresponding vector in the second set. If $n$ and $m$ are the sizes of these
two sets, we assume that the matching map that should be recovered is defined
on a subset of unknown cardinality $k^*\le \min(n,m)$. We show that, in the
high-dimensional setting, if the signal-to-noise ratio is larger than
$5(d\log(4nm/\alpha))^{1/4}$, then the true matching map can be recovered with
probability $1-\alpha$. Interestingly, this threshold does not depend on $k^*$
and is the same as the one obtained in prior work in the case of $k =
\min(n,m)$. The procedure for which the aforementioned property is proved is
obtained by a data-driven selection among candidate mappings
$\{\hat\pi_k:k\in[\min(n,m)]\}$. Each $\hat\pi_k$ minimizes the sum of squares
of distances between two sets of size $k$. The resulting optimization problem
can be formulated as a minimum-cost flow problem, and thus solved efficiently.
Finally, we report the results of numerical experiments on both synthetic and
real-world data that illustrate our theoretical results and provide further
insight into the properties of the algorithms studied in this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures, 1 table; AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the state-of-the-art of hybrid language models
architectures and strategies for "complex" question-answering (QA, CQA, CPS).
Large Language Models (LLM) are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, methods,
sensitive data protection, explainability, human approval and versatile
feedback... We identify key elements augmenting LLM to solve complex questions
or problems. We extend findings from the robust community edited research
papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and
challenges of LLM in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like
ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential
as well as the equally strong limitations of language models in complex QA.
Hybridizing these models with different components could allow to overcome
these different limits and go much further. We discuss some challenges
associated with complex QA, including domain adaptation, decomposition and
efficient multi-step QA, long form and non-factoid QA, safety and
multi-sensitivity data protection, multimodal search, hallucinations,
explainability and truthfulness, temproal reasoning. Therefore, we analyze
current solutions and promising research trends, using elements such as: hybrid
LLM architectures, active human reinforcement learning supervised with AI,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, iterated decomposition and others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELODIN: Naming Concepts in Embedding Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mello, Filipe Calegario, Geber Ramalho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements, the field of text-to-image synthesis still
suffers from lack of fine-grained control. Using only text, it remains
challenging to deal with issues such as concept coherence and concept
contamination. We propose a method to enhance control by generating specific
concepts that can be reused throughout multiple images, effectively expanding
natural language with new words that can be combined much like a painter's
palette. Unlike previous contributions, our method does not copy visuals from
input data and can generate concepts through text alone. We perform a set of
comparisons that finds our method to be a significant improvement over
text-only prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added quantitative data, fixed formatting issues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Three New Validators and a Large-Scale Benchmark Ranking for
  Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07360v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07360v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Musgrave, Serge Belongie, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Changes to hyperparameters can have a dramatic effect on model accuracy.
Thus, the tuning of hyperparameters plays an important role in optimizing
machine-learning models. An integral part of the hyperparameter-tuning process
is the evaluation of model checkpoints, which is done through the use of
"validators". In a supervised setting, these validators evaluate checkpoints by
computing accuracy on a validation set that has labels. In contrast, in an
unsupervised setting, the validation set has no such labels. Without any
labels, it is impossible to compute accuracy, so validators must estimate
accuracy instead. But what is the best approach to estimating accuracy? In this
paper, we consider this question in the context of unsupervised domain
adaptation (UDA). Specifically, we propose three new validators, and we compare
and rank them against five other existing validators, on a large dataset of
1,000,000 checkpoints. Extensive experimental results show that two of our
proposed validators achieve state-of-the-art performance in various settings.
Finally, we find that in many cases, the state-of-the-art is obtained by a
simple baseline method. To the best of our knowledge, this is the largest
empirical study of UDA validators to date. Code is available at
https://www.github.com/KevinMusgrave/powerful-benchmarker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was previously titled Benchmarking Validation Methods for
  Unsupervised Domain Adaptation. This version contains new experiments,
  analysis, and figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizer Preset Interpolation using <span class="highlight-title">Transformer</span> Auto-Encoders <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gwendal Le Vaillant, Thierry Dutoit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound synthesizers are widespread in modern music production but they
increasingly require expert skills to be mastered. This work focuses on
interpolation between presets, i.e., sets of values of all sound synthesis
parameters, to enable the intuitive creation of new sounds from existing ones.
  We introduce a bimodal auto-encoder neural network, which simultaneously
processes presets using multi-head attention blocks, and audio using
convolutions. This model has been tested on a popular frequency modulation
synthesizer with more than one hundred parameters. Experiments have compared
the model to related architectures and methods, and have demonstrated that it
performs smoother interpolations. After training, the proposed model can be
integrated into commercial synthesizers for live interpolation or sound design
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep network series for large-scale high-dynamic range imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Aghabiglou, Matthieu Terris, Adrian Jackson, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for large-scale high-dynamic range computational
imaging. Deep Neural Networks (DNNs) trained end-to-end can solve linear
inverse imaging problems almost instantaneously. While unfolded architectures
provide robustness to measurement setting variations, embedding large-scale
measurement operators in DNN architectures is impractical. Alternative
Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the
measurement setting, have proven effective to address scalability and
high-dynamic range challenges, but rely on highly iterative algorithms. We
propose a residual DNN series approach, also interpretable as a learned version
of matching pursuit, where the reconstructed image is a sum of residual images
progressively increasing the dynamic range, and estimated iteratively by DNNs
taking the back-projected data residual of the previous iteration as input. We
demonstrate on radio-astronomical imaging simulations that a series of only few
terms provides a reconstruction quality competitive with PnP, at a fraction of
the cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LidarCLIP or: How I Learned to Talk to Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Hess, Adam Tonderski, Christoffer Petersson, Kalle Åström, Lennart Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research connecting text and images has recently seen several breakthroughs,
with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection
between text and other visual modalities, such as lidar data, has received less
attention, prohibited by the lack of text-lidar datasets. In this work, we
propose LidarCLIP, a mapping from automotive point clouds to a pre-existing
CLIP embedding space. Using image-lidar pairs, we supervise a point cloud
encoder with the image CLIP embeddings, effectively relating text and lidar
data with the image domain as an intermediary. We show the effectiveness of
LidarCLIP by demonstrating that lidar-based retrieval is generally on par with
image-based retrieval, but with complementary strengths and weaknesses. By
combining image and lidar features, we improve upon both single-modality
methods and enable a targeted search for challenging detection scenarios under
adverse sensor conditions. We also explore zero-shot classification and show
that LidarCLIP outperforms existing attempts to use CLIP for point clouds by a
large margin. Finally, we leverage our compatibility with CLIP to explore a
range of applications, such as point cloud captioning and lidar-to-image
generation, without any additional training. Code and pre-trained models are
available at https://github.com/atonderski/lidarclip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effect of Modeling Human Rationality Level on Learning Rewards from
  Multiple Feedback Types <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav R. Ghosal, Matthew Zurek, Daniel S. Brown, Anca D. Dragan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When inferring reward functions from human behavior (be it demonstrations,
comparisons, physical corrections, or e-stops), it has proven useful to model
the human as making noisy-rational choices, with a "rationality coefficient"
capturing how much noise or entropy we expect to see in the human behavior.
Prior work typically sets the rationality level to a constant value, regardless
of the type, or quality, of human feedback. However, in many settings, giving
one type of feedback (e.g. a demonstration) may be much more difficult than a
different type of feedback (e.g. answering a comparison query). Thus, we expect
to see more or less noise depending on the type of human feedback. In this
work, we advocate that grounding the rationality coefficient in real data for
each feedback type, rather than assuming a default value, has a significant
positive effect on reward learning. We test this in both simulated experiments
and in a user study with real human feedback. We find that overestimating human
rationality can have dire effects on reward learning accuracy and regret. We
also find that fitting the rationality coefficient to human data enables better
reward learning, even when the human deviates significantly from the
noisy-rational choice model due to systematic biases. Further, we find that the
rationality level affects the informativeness of each feedback type:
surprisingly, demonstrations are not always the most informative -- when the
human acts very suboptimally, comparisons actually become more informative,
even when the rationality level is the same for both. Ultimately, our results
emphasize the importance and advantage of paying attention to the assumed
human-rationality level, especially when agents actively learn from multiple
types of human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at AAAI 2023; 10 pages, 5 figures plus appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Normalizing Flows with Stochastic Interpolants <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15571v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15571v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael S. Albergo, Eric Vanden-Eijnden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generative model based on a continuous-time normalizing flow between any
pair of base and target probability densities is proposed. The velocity field
of this flow is inferred from the probability current of a time-dependent
density that interpolates between the base and the target in finite time.
Unlike conventional normalizing flow inference methods based the maximum
likelihood principle, which require costly backpropagation through ODE solvers,
our interpolant approach leads to a simple quadratic loss for the velocity
itself which is expressed in terms of expectations that are readily amenable to
empirical estimation. The flow can be used to generate samples from either the
base or target, and to estimate the likelihood at any time along the
interpolant. In addition, the flow can be optimized to minimize the path length
of the interpolant density, thereby paving the way for building optimal
transport maps. In situations where the base is a Gaussian density, we also
show that the velocity of our normalizing flow can also be used to construct a
diffusion model to sample the target as well as estimate its score. However,
our approach shows that we can bypass this diffusion completely and work at the
level of the probability flow with greater simplicity, opening an avenue for
methods based solely on ordinary differential equations as an alternative to
those based on stochastic differential equations. Benchmarking on density
estimation tasks illustrates that the learned flow can match and surpass
conventional continuous flows at a fraction of the cost, and compares well with
diffusions on image generation on CIFAR-10 and ImageNet $32\times32$. The
method scales ab-initio ODE flows to previously unreachable image resolutions,
demonstrated up to $128\times128$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured State Space Models for In-Context Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, Feryal Behbahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured state space sequence (S4) models have recently achieved
state-of-the-art performance on long-range sequence modeling tasks. These
models also have fast inference speeds and parallelisable training, making them
potentially useful in many reinforcement learning settings. We propose a
modification to a variant of S4 that enables us to initialise and reset the
hidden state in parallel, allowing us to tackle reinforcement learning tasks.
We show that our modified architecture runs asymptotically faster than
Transformers and performs better than LSTM models on a simple memory-based
task. Then, by leveraging the model's ability to handle long-range sequences,
we achieve strong performance on a challenging meta-learning task in which the
agent is given a randomly-sampled continuous control environment, combined with
a randomly-sampled linear projection of the environment's observations and
actions. Furthermore, we show the resulting model can adapt to
out-of-distribution held-out tasks. Overall, the results presented in this
paper suggest that the S4 models are a strong contender for the default
architecture used for in-context reinforcement learning
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoder for <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing on Lidar Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Hess, Johan Jaxing, Elias Svensson, David Hagerman, Christoffer Petersson, Lennart Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked autoencoding has become a successful pretraining paradigm for
Transformer models for text, images, and, recently, point clouds. Raw
automotive datasets are suitable candidates for self-supervised pre-training as
they generally are cheap to collect compared to annotations for tasks like 3D
object detection (OD). However, the development of masked autoencoders for
point clouds has focused solely on synthetic and indoor data. Consequently,
existing methods have tailored their representations and models toward small
and dense point clouds with homogeneous point densities. In this work, we study
masked autoencoding for point clouds in an automotive setting, which are sparse
and for which the point density can vary drastically among objects in the same
scene. To this end, we propose Voxel-MAE, a simple masked autoencoding
pre-training scheme designed for voxel representations. We pre-train the
backbone of a Transformer-based 3D object detector to reconstruct masked voxels
and to distinguish between empty and non-empty voxels. Our method improves the
3D OD performance by 1.75 mAP points and 1.05 NDS on the challenging nuScenes
dataset. Further, we show that by pre-training with Voxel-MAE, we require only
40% of the annotated data to outperform a randomly initialized equivalent. Code
available at https://github.com/georghess/voxel-mae
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hindsight States: Blending Sim and Real Task Elements for Efficient
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Guist, Jan Schneider, Alexander Dittrich, Vincent Berenz, Bernhard Schölkopf, Dieter Büchler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning has shown great potential in solving complex tasks
when large amounts of data can be generated with little effort. In robotics,
one approach to generate training data builds on simulations based on dynamics
models derived from first principles. However, for tasks that, for instance,
involve complex soft robots, devising such models is substantially more
challenging. Being able to train effectively in increasingly complicated
scenarios with reinforcement learning enables to take advantage of complex
systems such as soft robots. Here, we leverage the imbalance in complexity of
the dynamics to learn more sample-efficiently. We (i) abstract the task into
distinct components, (ii) off-load the simple dynamics parts into the
simulation, and (iii) multiply these virtual parts to generate more data in
hindsight. Our new method, Hindsight States (HiS), uses this data and selects
the most useful transitions for training. It can be used with an arbitrary
off-policy algorithm. We validate our method on several challenging simulated
tasks and demonstrate that it improves learning both alone and when combined
with an existing hindsight algorithm, Hindsight Experience Replay (HER).
Finally, we evaluate HiS on a physical system and show that it boosts
performance on a complex table tennis task with a muscular robot. Videos and
code of the experiments can be found on webdav.tuebingen.mpg.de/his/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearly Mapping from Image to Text Space <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extent to which text-only language models (LMs) learn to represent
features of the non-linguistic world is an open question. Prior work has shown
that pretrained LMs can be taught to caption images when a vision model's
parameters are optimized to encode images in the language space. We test a
stronger hypothesis: that the conceptual representations learned by frozen
text-only models and vision-only models are similar enough that this can be
achieved with a linear map. We show that the image representations from vision
models can be transferred as continuous prompts to frozen LMs by training only
a single linear projection. Using these to prompt the LM achieves competitive
performance on captioning and visual question answering tasks compared to
models that tune both the image encoder and text decoder (such as the MAGMA
model). We compare three image encoders with increasing amounts of linguistic
supervision seen during pretraining: BEIT (no linguistic information),
NF-ResNET (lexical category information), and CLIP (full natural language
descriptions). We find that all three encoders perform equally well at
transferring visual property information to the language model (e.g., whether
an animal is large or small), but that image encoders pretrained with
linguistic supervision more saliently encode category information (e.g.,
distinguishing hippo vs. elephant) and thus perform significantly better on
benchmark language-and-vision tasks. Our results indicate that LMs encode
conceptual information structurally similarly to vision-based models, even
those that are solely trained on images. Code is available here:
https://github.com/jmerullo/limber
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Latency Attacks via Sponge Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Emanuele Cinà, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sponge examples are test-time inputs carefully optimized to increase energy
consumption and latency of neural networks when deployed on hardware
accelerators. In this work, we are the first to demonstrate that sponge
examples can also be injected at training time, via an attack that we call
sponge poisoning. This attack allows one to increase the energy consumption and
latency of machine-learning models indiscriminately on each test-time input. We
present a novel formalization for sponge poisoning, overcoming the limitations
related to the optimization of test-time sponge examples, and show that this
attack is possible even if the attacker only controls a few model updates; for
instance, if model training is outsourced to an untrusted third-party or
distributed via federated learning. Our extensive experimental analysis shows
that sponge poisoning can almost completely vanish the effect of hardware
accelerators. We also analyze the activations of poisoned models, identifying
which components are more vulnerable to this attack. Finally, we examine the
feasibility of countermeasures against sponge poisoning to decrease energy
consumption, showing that sanitization methods may be overly expensive for most
of the users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint;16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Specific De Novo Design of Drug Candidate Molecules with Graph
  <span class="highlight-title">Transformer</span>-based Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07868v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07868v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atabey Ünlü, Elif Çevrim, Ahmet Sarıgün, Hayriye Çelikbilek, Heval Ataş Güvenilir, Altay Koyaş, Deniz Cansen Kahraman, Abdurrahman Olğaç, Ahmet Rifaioğlu, Tunca Doğan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering novel drug candidate molecules is one of the most fundamental and
critical steps in drug development. Generative deep learning models, which
create synthetic data given a probability distribution, have been developed
with the purpose of picking completely new samples from a partially known
space. Generative models offer high potential for designing de novo molecules;
however, in order for them to be useful in real-life drug development
pipelines, these models should be able to design target-specific molecules,
which is the next step in this field. In this study, we propose DrugGEN, for
the de novo design of drug candidate molecules that interact with selected
target proteins. The proposed system represents compounds and protein
structures as graphs and processes them via serially connected two generative
adversarial networks comprising graph transformers. DrugGEN is trained using a
large dataset of compounds from ChEMBL and target-specific bioactive molecules,
to design effective and specific inhibitory molecules against the AKT1 protein,
which has critical importance for developing treatments against various types
of cancer. On fundamental benchmarks, DrugGEN models have either competitive or
better performance against other methods. To assess the target-specific
generation performance, we conducted further in silico analysis with molecular
docking and deep learning-based bioactivity prediction. Results indicate that
de novo molecules have high potential for interacting with the AKT1 protein
structure in the level of its native ligand. DrugGEN can be used to design
completely novel and effective target-specific drug candidate molecules for any
druggable protein, given target features and a dataset of experimental
bioactivities. Code base, datasets, results and trained models of DrugGEN are
available at https://github.com/HUBioDataLab/DrugGEN
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds via Information Density and Conditional
  Information Density 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.08044v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.08044v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a general approach, based on exponential inequalities, to derive
bounds on the generalization error of randomized learning algorithms. Using
this approach, we provide bounds on the average generalization error as well as
bounds on its tail probability, for both the PAC-Bayesian and single-draw
scenarios. Specifically, for the case of sub-Gaussian loss functions, we obtain
novel bounds that depend on the information density between the training data
and the output hypothesis. When suitably weakened, these bounds recover many of
the information-theoretic bounds available in the literature. We also extend
the proposed exponential-inequality approach to the setting recently introduced
by Steinke and Zakynthinou (2020), where the learning algorithm depends on a
randomly selected subset of the available training data. For this setup, we
present bounds for bounded loss functions in terms of the conditional
information density between the output hypothesis and the random variable
determining the subset choice, given all training data. Through our approach,
we recover the average generalization bound presented by Steinke and
Zakynthinou (2020) and extend it to the PAC-Bayesian and single-draw scenarios.
For the single-draw scenario, we also obtain novel bounds in terms of the
conditional $\alpha$-mutual information and the conditional maximal leakage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Journal on Selected Areas in Information Theory (JSAIT).
  This version incorporates a correction to the JSAIT version. The correction
  is detailed at https://gdurisi.github.io/files/2021/jsait-correction.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing <span class="highlight-title">Knowledge</span> Graph Embedding Models with Semantic-driven <span class="highlight-title">Loss</span>
  Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Hubert, Pierre Monnin, Armelle Brun, Davy Monticolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph embedding models (KGEMs) are used for various tasks related
to knowledge graphs (KGs), including link prediction. They are trained with
loss functions that are computed considering a batch of scored triples and
their corresponding labels. Traditional approaches consider the label of a
triple to be either true or false. However, recent works suggest that all
negative triples should not be valued equally. In line with this recent
assumption, we posit that semantically valid negative triples might be
high-quality negative triples. As such, loss functions should treat them
differently from semantically invalid negative ones. To this aim, we propose
semantic-driven versions for the three main loss functions for link prediction.
In particular, we treat the scores of negative triples differently by injecting
background knowledge about relation domains and ranges into the loss functions.
In an extensive and controlled experimental setting, we show that the proposed
loss functions systematically provide satisfying results on three public
benchmark KGs underpinned with different schemas, which demonstrates both the
generality and superiority of our proposed approach. In fact, the proposed loss
functions do (1) lead to better MRR and Hits@$10$ values, (2) drive KGEMs
towards better semantic awareness. This highlights that semantic information
globally improves KGEMs, and thus should be incorporated into loss functions.
Domains and ranges of relations being largely available in schema-defined KGs,
this makes our approach both beneficial and widely usable in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Balancing Weights via Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshiaki Kitazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present generalized balancing weights, Neural Balancing Weights (NBW), to
estimate the causal effects for an arbitrary mixture of discrete and continuous
interventions. The weights were obtained by directly estimating the density
ratio between the source and balanced distributions by optimizing the
variational representation of $f$-divergence. For this, we selected
$\alpha$-divergence since it has good properties for optimization: It has an
estimator whose sample complexity is independent of it's ground truth value and
unbiased mini-batch gradients and is advantageous for the vanishing gradient
problem. In addition, we provide a method for checking the balance of the
distribution changed by the weights. If the balancing is imperfect, the weights
can be improved by adding new balancing weights. Our method can be conveniently
implemented with any present deep-learning libraries, and weights can be used
in most state-of-the-art supervised algorithms. The code for our method is
available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wild Patterns Reloaded: A <span class="highlight-title">Survey</span> of Machine Learning Security against
  Training Data Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Sebastiano Vascon, Werner Zellinger, Bernhard A. Moser, Alina Oprea, Battista Biggio, Marcello Pelillo, Fabio Roli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of machine learning is fueled by the increasing availability of
computing power and large training datasets. The training data is used to learn
new models or update existing ones, assuming that it is sufficiently
representative of the data that will be encountered at test time. This
assumption is challenged by the threat of poisoning, an attack that manipulates
the training data to compromise the model's performance at test time. Although
poisoning has been acknowledged as a relevant threat in industry applications,
and a variety of different attacks and defenses have been proposed so far, a
complete systematization and critical review of the field is still missing. In
this survey, we provide a comprehensive systematization of poisoning attacks
and defenses in machine learning, reviewing more than 100 papers published in
the field in the last 15 years. We start by categorizing the current threat
models and attacks, and then organize existing defenses accordingly. While we
focus mostly on computer-vision applications, we argue that our systematization
also encompasses state-of-the-art attacks and defenses for other data
modalities. Finally, we discuss existing resources for research in poisoning,
and shed light on the current limitations and open research questions in this
research field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, Accepted at ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM as an Optimal Relaxation of Bayes <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Möllenhoff, Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness-aware minimization (SAM) and related adversarial deep-learning
methods can drastically improve generalization, but their underlying mechanisms
are not yet fully understood. Here, we establish SAM as a relaxation of the
Bayes objective where the expected negative-loss is replaced by the optimal
convex lower bound, obtained by using the so-called Fenchel biconjugate. The
connection enables a new Adam-like extension of SAM to automatically obtain
reasonable uncertainty estimates, while sometimes also improving its accuracy.
By connecting adversarial and Bayesian methods, our work opens a new path to
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejie Li, Jia-Wang Bian, Robert Castle, Philip H. S. Torr, Victor Adrian Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality 3D ground-truth shapes are critical for 3D object reconstruction
evaluation. However, it is difficult to create a replica of an object in
reality, and even 3D reconstructions generated by 3D scanners have artefacts
that cause biases in evaluation. To address this issue, we introduce a novel
multi-view RGBD dataset captured using a mobile device, which includes highly
precise 3D ground-truth annotations for 153 object models featuring a diverse
set of 3D structures. We obtain precise 3D ground-truth shape without relying
on high-end 3D scanners by utilising LEGO models with known geometry as the 3D
structures for image capture. The distinct data modality offered by
high-resolution RGB images and low-resolution depth maps captured on a mobile
device, when combined with precise 3D geometry annotations, presents a unique
opportunity for future research on high-fidelity 3D reconstruction.
Furthermore, we evaluate a range of 3D reconstruction algorithms on the
proposed dataset. Project page: http://code.active.vision/MobileBrick/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared at CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Certified Training: Small Boxes are All You Need <span class="chip">ICLR23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Niklas Müller, Franziska Eckert, Marc Fischer, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To obtain, deterministic guarantees of adversarial robustness, specialized
training methods are used. We propose, SABR, a novel such certified training
method, based on the key insight that propagating interval bounds for a small
but carefully selected subset of the adversarial input region is sufficient to
approximate the worst-case loss over the whole region while significantly
reducing approximation errors. We show in an extensive empirical evaluation
that SABR outperforms existing certified defenses in terms of both standard and
certifiable accuracies across perturbation magnitudes and datasets, pointing to
a new class of certified training methods promising to alleviate the
robustness-accuracy trade-off.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR23 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A view of mini-batch SGD via generating functions: conditions of
  convergence, phase transitions, benefit from negative momenta <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Velikanov, Denis Kuznedelev, Dmitry Yarotsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mini-batch SGD with momentum is a fundamental algorithm for learning large
predictive models. In this paper we develop a new analytic framework to analyze
noise-averaged properties of mini-batch SGD for linear models at constant
learning rates, momenta and sizes of batches. Our key idea is to consider the
dynamics of the second moments of model parameters for a special family of
"Spectrally Expressible" approximations. This allows to obtain an explicit
expression for the generating function of the sequence of loss values. By
analyzing this generating function, we find, in particular, that 1) the SGD
dynamics exhibits several convergent and divergent regimes depending on the
spectral distributions of the problem; 2) the convergent regimes admit explicit
stability conditions, and explicit loss asymptotics in the case of power-law
spectral distributions; 3) the optimal convergence rate can be achieved at
negative momenta. We verify our theoretical predictions by extensive
experiments with MNIST, CIFAR10 and synthetic problems, and find a good
quantitative agreement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The revised version accepted at ICLR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More is Less: Inducing Sparsity via Overparameterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11027v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11027v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung-Hsu Chou, Johannes Maly, Holger Rauhut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning it is common to overparameterize neural networks, that is,
to use more parameters than training samples. Quite surprisingly training the
neural network via (stochastic) gradient descent leads to models that
generalize very well, while classical statistics would suggest overfitting. In
order to gain understanding of this implicit bias phenomenon we study the
special case of sparse recovery (compressed sensing) which is of interest on
its own. More precisely, in order to reconstruct a vector from underdetermined
linear measurements, we introduce a corresponding overparameterized square loss
functional, where the vector to be reconstructed is deeply factorized into
several vectors. We show that, if there exists an exact solution, vanilla
gradient flow for the overparameterized loss functional converges to a good
approximation of the solution of minimal $\ell_1$-norm. The latter is
well-known to promote sparse solutions. As a by-product, our results
significantly improve the sample complexity for compressed sensing via gradient
flow/descent on overparameterized models derived in previous works. The theory
accurately predicts the recovery rate in numerical experiments. Our proof
relies on analyzing a certain Bregman divergence of the flow. This bypasses the
obstacles caused by non-convexity and should be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Regularization in Solving Extensive-Form Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Liu, Asuman Ozdaglar, Tiancheng Yu, Kaiqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the power of {\it regularization}, a common
technique in reinforcement learning and optimization, in solving extensive-form
games (EFGs). We propose a series of new algorithms based on regularizing the
payoff functions of the game, and establish a set of convergence results that
strictly improve over the existing ones, with either weaker assumptions or
stronger convergence guarantees. In particular, we first show that dilated
optimistic mirror descent (DOMD), an efficient variant of OMD for solving EFGs,
with adaptive regularization can achieve a fast $\tilde O(1/T)$ last-iterate
convergence in terms of duality gap and distance to the set of Nash equilibrium
(NE) without uniqueness assumption of the NE. Second, we show that regularized
counterfactual regret minimization (\texttt{Reg-CFR}), with a variant of
optimistic mirror descent algorithm as regret-minimizer, can achieve
$O(1/T^{1/4})$ best-iterate, and $O(1/T^{3/4})$ average-iterate convergence
rate for finding NE in EFGs. Finally, we show that \texttt{Reg-CFR} can achieve
asymptotic last-iterate convergence, and optimal $O(1/T)$ average-iterate
convergence rate, for finding the NE of perturbed EFGs, which is useful for
finding approximate extensive-form perfect equilibria (EFPE). To the best of
our knowledge, they constitute the first last-iterate convergence results for
CFR-type algorithms, while matching the state-of-the-art average-iterate
convergence rate in finding NE for non-perturbed EFGs. We also provide
numerical results to corroborate the advantages of our algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning for Monolingual End-to-End Automatic Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09427v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09427v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vander Eeckt, Hugo Van hamme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Automatic Speech Recognition (ASR) models to new domains results in
a deterioration of performance on the original domain(s), a phenomenon called
Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to
new accents, dialects, topics, etc. without suffering from CF, making them
unable to be continually enhanced without storing all past data. Fortunately,
Continual Learning (CL) methods, which aim to enable continual adaptation while
overcoming CF, can be used. In this paper, we implement an extensive number of
CL methods for End-to-End ASR and test and compare their ability to extend a
monolingual Hybrid CTC-Transformer model across four new tasks. We find that
the best performing CL method closes the gap between the fine-tuned model
(lower bound) and the model trained jointly on all tasks (upper bound) by more
than 40%, while requiring access to only 0.6% of the original data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EUSIPCO 2022. 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Non-Probabilistic Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05818v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05818v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Ellsaesser, Guido Fioretti, Gail E. James
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two reasons why uncertainty about the future yield of investments
may not be adequately described by Probability Theory. The first one is due to
unique or nearly-unique events, that either never realized or occurred too
seldom for probabilities to be reliable. The second one arises when when one
fears that something may happen, that one is not even able to figure out, e.g.,
if one asks: "Climate change, financial crises, pandemic, war, what next?"
  In both cases, simple one-to-one causal mappings between available
alternatives and possible consequences eventually melt down. However, such
destructions reflect into the changing narratives of business executives,
employees and other stakeholders in specific, identifiable and differential
ways. In particular, texts such as consultants' reports or letters to
shareholders can be analysed in order to detect the impact of both sorts of
uncertainty onto the causal relations that normally guide decision-making.
  We propose structural measures of causal mappings as a means to measure
non-probabilistic uncertainty, eventually suggesting that automated text
analysis can greatly augment the possibilities offered by these techniques.
Prospective applications may concern statistical institutes, stock market
traders, as well as businesses wishing to compare their own vision to those
prevailing in their industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aerial View Localization with Reinforcement Learning: Towards Emulating
  Search-and-Rescue <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03694v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03694v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksis Pirinen, Anton Samuelsson, John Backsund, Kalle Åström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate-induced disasters are and will continue to be on the rise, and thus
search-and-rescue (SAR) operations, where the task is to localize and assist
one or several people who are missing, become increasingly relevant. In many
cases the rough location may be known and a UAV can be deployed to explore a
given, confined area to precisely localize the missing people. Due to time and
battery constraints it is often critical that localization is performed as
efficiently as possible. In this work we approach this type of problem by
abstracting it as an aerial view goal localization task in a framework that
emulates a SAR-like setup without requiring access to actual UAVs. In this
framework, an agent operates on top of an aerial image (proxy for a search
area) and is tasked with localizing a goal that is described in terms of visual
cues. To further mimic the situation on an actual UAV, the agent is not able to
observe the search area in its entirety, not even at low resolution, and thus
it has to operate solely based on partial glimpses when navigating towards the
goal. To tackle this task, we propose AiRLoc, a reinforcement learning
(RL)-based model that decouples exploration (searching for distant goals) and
exploitation (localizing nearby goals). Extensive evaluations show that AiRLoc
outperforms heuristic search methods as well as alternative learnable
approaches, and that it generalizes across datasets, e.g. to disaster-hit areas
without seeing a single disaster scenario during training. We also conduct a
proof-of-concept study which indicates that the learnable methods outperform
humans on average. Code and models have been made publicly available at
https://github.com/aleksispi/airloc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023 Workshop on Machine Learning for Remote Sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Algorithms for Latent Bandits with Cluster Structure <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyabrata Pal, Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of latent bandits with cluster structure where there
are multiple users, each with an associated multi-armed bandit problem. These
users are grouped into \emph{latent} clusters such that the mean reward vectors
of users within the same cluster are identical. At each round, a user, selected
uniformly at random, pulls an arm and observes a corresponding noisy reward.
The goal of the users is to maximize their cumulative rewards. This problem is
central to practical recommendation systems and has received wide attention of
late \cite{gentile2014online, maillard2014latent}. Now, if each user acts
independently, then they would have to explore each arm independently and a
regret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M},
\mathsf{N}$ are the number of arms and users, respectively. Instead, we propose
LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of the
latent cluster structure to provide the minimax optimal regret of
$\widetilde{O}(\sqrt{(\mathsf{M}+\mathsf{N})\mathsf{T}})$, when the number of
clusters is $\widetilde{O}(1)$. This is the first algorithm to guarantee such
strong regret bound. LATTICE is based on a careful exploitation of arm
information within a cluster while simultaneously clustering users.
Furthermore, it is computationally efficient and requires only
$O(\log{\mathsf{T}})$ calls to an offline matrix completion oracle across all
$\mathsf{T}$ rounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages. Accepted to AISTATS 2023. Added Experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Good Practices in Evaluating Transfer Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, Michael Backes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer adversarial attacks raise critical security concerns in real-world,
black-box scenarios. However, the actual progress of this field is difficult to
assess due to two common limitations in existing evaluations. First, different
methods are often not systematically and fairly evaluated in a one-to-one
comparison. Second, only transferability is evaluated but another key attack
property, stealthiness, is largely overlooked. In this work, we design good
practices to address these limitations, and we present the first comprehensive
evaluation of transfer attacks, covering 23 representative attacks against 9
defenses on ImageNet. In particular, we propose to categorize existing attacks
into five categories, which enables our systematic category-wise analyses.
These analyses lead to new findings that even challenge existing knowledge and
also help determine the optimal attack hyperparameters for our attack-wise
comprehensive evaluation. We also pay particular attention to stealthiness, by
adopting diverse imperceptibility metrics and looking into new, finer-grained
characteristics. Overall, our new insights into transferability and
stealthiness lead to actionable good practices for future evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and a list of categorized attacks are publicly available at
  https://github.com/ZhengyuZhao/TransferAttackEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hair and Scalp Disease Detection using Machine Learning and Image
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinmoy Roy, Anica Tasnim Protity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Almost 80 million Americans suffer from hair loss due to aging, stress,
medication, or genetic makeup. Hair and scalp-related diseases often go
unnoticed in the beginning. Sometimes, a patient cannot differentiate between
hair loss and regular hair fall. Diagnosing hair-related diseases is
time-consuming as it requires professional dermatologists to perform visual and
medical tests. Because of that, the overall diagnosis gets delayed, which
worsens the severity of the illness. Due to the image-processing ability,
neural network-based applications are used in various sectors, especially
healthcare and health informatics, to predict deadly diseases like cancers and
tumors. These applications assist clinicians and patients and provide an
initial insight into early-stage symptoms. In this study, we used a deep
learning approach that successfully predicts three main types of hair loss and
scalp-related diseases: alopecia, psoriasis, and folliculitis. However, limited
study in this area, unavailability of a proper dataset, and degree of variety
among the images scattered over the internet made the task challenging. 150
images were obtained from various sources and then preprocessed by denoising,
image equalization, enhancement, and data balancing, thereby minimizing the
error rate. After feeding the processed data into the 2D convolutional neural
network (CNN) model, we obtained overall training accuracy of 96.2%, with a
validation accuracy of 91.1%. The precision and recall score of alopecia,
psoriasis, and folliculitis are 0.895, 0.846, and 1.0, respectively. We also
created a dataset of the scalp images for future prospective researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp
  and motion optimization through diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julen Urain, Niklas Funk, Jan Peters, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective optimization problems are ubiquitous in robotics, e.g., the
optimization of a robot manipulation task requires a joint consideration of
grasp pose configurations, collisions and joint limits. While some demands can
be easily hand-designed, e.g., the smoothness of a trajectory, several
task-specific objectives need to be learned from data. This work introduces a
method for learning data-driven SE(3) cost functions as diffusion models.
Diffusion models can represent highly-expressive multimodal distributions and
exhibit proper gradients over the entire space due to their score-matching
training objective. Learning costs as diffusion models allows their seamless
integration with other costs into a single differentiable objective function,
enabling joint gradient-based motion optimization. In this work, we focus on
learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel
framework for joint grasp and motion optimization without needing to decouple
grasp selection from trajectory generation. We evaluate the representation
power of our SE(3) diffusion models w.r.t. classical generative models, and we
showcase the superior performance of our proposed optimization framework in a
series of simulated and real-world robotic manipulation tasks against
representative baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>diffusion models, SE(3), grasping,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of <span class="highlight-title">Dataset</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Szyller, Rui Zhang, Jian Liu, N. Asokan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models are costly to train as they can require a
significant amount of data, computational resources and technical expertise.
Thus, they constitute valuable intellectual property that needs protection from
adversaries wanting to steal them. Ownership verification techniques allow the
victims of model stealing attacks to demonstrate that a suspect model was in
fact stolen from theirs. Although a number of ownership verification techniques
based on watermarking or fingerprinting have been proposed, most of them fall
short either in terms of security guarantees (well-equipped adversaries can
evade verification) or computational cost. A fingerprinting technique
introduced at ICLR '21, Dataset Inference (DI), has been shown to offer better
robustness and efficiency than prior methods. The authors of DI provided a
correctness proof for linear (suspect) models. However, in the same setting, we
prove that DI suffers from high false positives (FPs) -- it can incorrectly
identify an independent model trained with non-overlapping data from the same
distribution as stolen. We further prove that DI also triggers FPs in
realistic, non-linear suspect models. We then confirm empirically that DI leads
to FPs, with high confidence. Second, we show that DI also suffers from false
negatives (FNs) -- an adversary can fool DI by regularising a stolen model's
decision boundaries using adversarial training, thereby leading to an FN. To
this end, we demonstrate that DI fails to identify a model adversarially
trained from a stolen dataset -- the setting where DI is the hardest to evade.
Finally, we discuss the implications of our findings, the viability of
fingerprinting-based ownership verification in general, and suggest directions
for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Recovery Learning using Model Predictive Meta-<span class="highlight-title">Reasoning</span> <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Vats, Maxim Likhachev, Oliver Kroemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating under real world conditions is challenging due to the possibility
of a wide range of failures induced by execution errors and state uncertainty.
In relatively benign settings, such failures can be overcome by retrying or
executing one of a small number of hand-engineered recovery strategies. By
contrast, contact-rich sequential manipulation tasks, like opening doors and
assembling furniture, are not amenable to exhaustive hand-engineering. To
address this issue, we present a general approach for robustifying manipulation
strategies in a sample-efficient manner. Our approach incrementally improves
robustness by first discovering the failure modes of the current strategy via
exploration in simulation and then learning additional recovery skills to
handle these failures. To ensure efficient learning, we propose an online
algorithm called Meta-Reasoning for Skill Learning (MetaReSkill) that monitors
the progress of all recovery policies during training and allocates training
resources to recoveries that are likely to improve the task performance the
most. We use our approach to learn recovery skills for door-opening and
evaluate them both in simulation and on a real robot with little fine-tuning.
Compared to open-loop execution, our experiments show that even a limited
amount of recovery learning improves task success substantially from 71% to
92.4% in simulation and from 75% to 90% on a real robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Conference on Robotics and Automation
  (ICRA) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIFFormer: Scalable (Graph) <span class="highlight-title">Transformer</span>s Induced by Energy Constrained
  Diffusion <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data generation often involves complex inter-dependencies among
instances, violating the IID-data hypothesis of standard learning paradigms and
posing a challenge for uncovering the geometric structures for learning desired
instance representations. To this end, we introduce an energy constrained
diffusion model which encodes a batch of instances from a dataset into
evolutionary states that progressively incorporate other instances' information
by their interactions. The diffusion process is constrained by descent criteria
w.r.t.~a principled energy function that characterizes the global consistency
of instance representations over latent structures. We provide rigorous theory
that implies closed-form optimal estimates for the pairwise diffusion strength
among arbitrary instance pairs, which gives rise to a new class of neural
encoders, dubbed as DIFFormer (diffusion-based Transformers), with two
instantiations: a simple version with linear complexity for prohibitive
instance numbers, and an advanced version for learning complex structures.
Experiments highlight the wide applicability of our model as a general-purpose
encoder backbone with superior performance in various tasks, such as node
classification on large graphs, semi-supervised image/text classification, and
spatial-temporal dynamics prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023 as a spotlight presentation, the
  implementation code is available at https://github.com/qitianwu/DIFFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling Skill Learning from Robotic Control for Generalizable Object
  Manipulation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Lu, Bo Yang, Bing Wang, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in robotic manipulation through reinforcement learning (RL) or
imitation learning (IL) have shown potential for tackling a range of tasks
e.g., opening a drawer or a cupboard. However, these techniques generalize
poorly to unseen objects. We conjecture that this is due to the
high-dimensional action space for joint control. In this paper, we take an
alternative approach and separate the task of learning 'what to do' from 'how
to do it' i.e., whole-body control. We pose the RL problem as one of
determining the skill dynamics for a disembodied virtual manipulator
interacting with articulated objects. The whole-body robotic kinematic control
is optimized to execute the high-dimensional joint motion to reach the goals in
the workspace. It does so by solving a quadratic programming (QP) model with
robotic singularity and kinematic constraints. Our experiments on manipulating
complex articulated objects show that the proposed approach is more
generalizable to unseen objects with large intra-class variations,
outperforming previous approaches. The evaluation results indicate that our
approach generates more compliant robotic motion and outperforms the pure RL
and IL baselines in task success rates. Additional information and videos are
available at https://kl-research.github.io/decoupskill
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-based Out-of-Distribution Detection for Graph Neural Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitian Wu, Yiting Chen, Chenxiao Yang, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning on graphs, where instance nodes are inter-connected, has become one
of the central problems for deep learning, as relational structures are
pervasive and induce data inter-dependence which hinders trivial adaptation of
existing approaches that assume inputs to be i.i.d.~sampled. However, current
models mostly focus on improving testing performance of in-distribution data
and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing
samples that may cause negative outcome if the prediction is overconfident on
them. In this paper, we investigate the under-explored problem, OOD detection
on graph-structured data, and identify a provably effective OOD discriminator
based on an energy function directly extracted from graph neural networks
trained with standard classification loss. This paves a way for a simple,
powerful and efficient OOD detection model for GNN-based learning on graphs,
which we call GNNSafe. It also has nice theoretical properties that guarantee
an overall distinguishable margin between the detection scores for
in-distribution and OOD samples, which, more critically, can be further
strengthened by a learning-free energy belief propagation scheme. For
comprehensive evaluation, we introduce new benchmark settings that evaluate the
model for detecting OOD data from both synthetic and real distribution shifts
(cross-domain graph shifts and temporal graph shifts). The results show that
GNNSafe achieves up to $17.0\%$ AUROC improvement over state-of-the-arts and it
could serve as simple yet strong baselines in such an under-developed area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023, the implementation code is available at
  https://github.com/qitianwu/GraphOOD-GNNSafe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time
  Series Forecasting <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, Yanjie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distribution shift in Time Series Forecasting (TSF), indicating series
distribution changes over time, largely hinders the performance of TSF models.
Existing works towards distribution shift in time series are mostly limited in
the quantification of distribution and, more importantly, overlook the
potential shift between lookback and horizon windows. To address above
challenges, we systematically summarize the distribution shift in TSF into two
categories. Regarding lookback windows as input-space and horizon windows as
output-space, there exist (i) intra-space shift, that the distribution within
the input-space keeps shifted over time, and (ii) inter-space shift, that the
distribution is shifted between input-space and output-space. Then we
introduce, Dish-TS, a general neural paradigm for alleviating distribution
shift in TSF. Specifically, for better distribution estimation, we propose the
coefficient net (CONET), which can be any neural architectures, to map input
sequences into learnable distribution coefficients. To relieve intra-space and
inter-space shift, we organize Dish-TS as a Dual-CONET framework to separately
learn the distribution of input- and output-space, which naturally captures the
distribution difference of two spaces. In addition, we introduce a more
effective training strategy for intractable CONET learning. Finally, we conduct
extensive experiments on several datasets coupled with different
state-of-the-art forecasting models. Experimental results show Dish-TS
consistently boosts them with a more than 20% average improvement. Code is
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Affinity Assisted <span class="highlight-title">Knowledge</span> Distillation and Quantization of
  Deep Neural Networks on Label-Free Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Li, Biao Yang, Penghang Yin, Yingyong Qi, Jack Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a feature affinity (FA) assisted knowledge
distillation (KD) method to improve quantization-aware training of deep neural
networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role
of teaching middle steps of a solution to a student instead of only giving
final answers in the conventional KD where the loss acts on the network logits
at the output level. Combining logit loss and FA loss, we found that the
quantized student network receives stronger supervision than from the labeled
ground-truth data. The resulting FAQD is capable of compressing model on
label-free data, which brings immediate practical benefits as pre-trained
teacher models are readily available and unlabeled data are abundant. In
contrast, data labeling is often laborious and expensive. Finally, we propose a
fast feature affinity (FFA) loss that accurately approximates FA loss with a
lower order of computational complexity, which helps speed up training for high
resolution image input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantics-Native Communication with Contextual <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.05681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.05681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyowoon Seo, Jihong Park, Mehdi Bennis, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spurred by a huge interest in the post-Shannon communication, it has recently
been shown that leveraging semantics can significantly improve the
communication effectiveness across many tasks. In this article, inspired by
human communication, we propose a novel stochastic model of System 1
semantics-native communication (SNC) for generic tasks, where a speaker has an
intention of referring to an entity, extracts the semantics, and communicates
its symbolic representation to a target listener. To further reach its full
potential, we additionally infuse contextual reasoning into SNC such that the
speaker locally and iteratively self-communicates with a virtual agent built on
the physical listener's unique way of coding its semantics, i.e., communication
context. The resultant System 2 SNC allows the speaker to extract the most
effective semantics for its listener. Leveraging the proposed stochastic model,
we show that the reliability of System 2 SNC increases with the number of
meaningful concepts, and derive the expected semantic representation (SR) bit
length which quantifies the extracted effective semantics. It is also shown
that System 2 SNC significantly reduces the SR length without compromising
communication reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures, in IEEE Transactions on Cognitive
  Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.11202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.11202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao He, Kaiwen Zha, Dina Katabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indiscriminate data poisoning attacks are quite effective against supervised
learning. However, not much is known about their impact on unsupervised
contrastive learning (CL). This paper is the first to consider indiscriminate
poisoning attacks of contrastive learning. We propose Contrastive Poisoning
(CP), the first effective such attack on CL. We empirically show that
Contrastive Poisoning, not only drastically reduces the performance of CL
algorithms, but also attacks supervised learning models, making it the most
generalizable indiscriminate poisoning attack. We also show that CL algorithms
with a momentum encoder are more robust to indiscriminate poisoning, and
propose a new countermeasure based on matrix completion. Code is available at:
https://github.com/kaiwenzha/contrastive-poisoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 Spotlight (notable top 25%). The first two authors
  contributed equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn2Agree: Fitting with Multiple Annotators without Objective Ground
  Truth <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03596v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03596v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongyang Wang, Yuan Gao, Chenyou Fan, Junjie Hu, Tin Lun Lam, Nicholas D. Lane, Nadia Bianchi-Berthouze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The annotation of domain experts is important for some medical applications
where the objective ground truth is ambiguous to define, e.g., the
rehabilitation for some chronic diseases, and the prescreening of some
musculoskeletal abnormalities without further medical examinations. However,
improper uses of the annotations may hinder developing reliable models. On one
hand, forcing the use of a single ground truth generated from multiple
annotations is less informative for the modeling. On the other hand, feeding
the model with all the annotations without proper regularization is noisy given
existing disagreements. For such issues, we propose a novel Learning to
Agreement (Learn2Agree) framework to tackle the challenge of learning from
multiple annotators without objective ground truth. The framework has two
streams, with one stream fitting with the multiple annotators and the other
stream learning agreement information between annotators. In particular, the
agreement learning stream produces regularization information to the classifier
stream, tuning its decision to be better in line with the agreement between
annotators. The proposed method can be easily added to existing backbones, with
experiments on two medical datasets showed better agreement levels with
annotators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the TML4H workshop at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why (and When) does Local SGD Generalize Better than SGD? <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Gu, Kaifeng Lyu, Longbo Huang, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local SGD is a communication-efficient variant of SGD for large-scale
training, where multiple GPUs perform SGD independently and average the model
parameters periodically. It has been recently observed that Local SGD can not
only achieve the design goal of reducing the communication overhead but also
lead to higher test accuracy than the corresponding SGD baseline (Lin et al.,
2020b), though the training regimes for this to happen are still in debate
(Ortiz et al., 2021). This paper aims to understand why (and when) Local SGD
generalizes better based on Stochastic Differential Equation (SDE)
approximation. The main contributions of this paper include (i) the derivation
of an SDE that captures the long-term behavior of Local SGD in the small
learning rate regime, showing how noise drives the iterate to drift and diffuse
after it has reached close to the manifold of local minima, (ii) a comparison
between the SDEs of Local SGD and SGD, showing that Local SGD induces a
stronger drift term that can result in a stronger effect of regularization,
e.g., a faster reduction of sharpness, and (iii) empirical evidence validating
that having a small learning rate and long enough training time enables the
generalization improvement over SGD but removing either of the two conditions
leads to no improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Federated Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Sun, Yonghui Xu, Yong Liu, Wei He, Lanju Kong, Fangzhao Wu, Yali Jiang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has recently been applied to recommendation systems to
protect user privacy. In federated learning settings, recommendation systems
can train recommendation models only collecting the intermediate parameters
instead of the real user data, which greatly enhances the user privacy. Beside,
federated recommendation systems enable to collaborate with other data
platforms to improve recommended model performance while meeting the regulation
and privacy constraints. However, federated recommendation systems faces many
new challenges such as privacy, security, heterogeneity and communication
costs. While significant research has been conducted in these areas, gaps in
the surveying literature still exist. In this survey, we-(1) summarize some
common privacy mechanisms used in federated recommendation systems and discuss
the advantages and limitations of each mechanism; (2) review some robust
aggregation strategies and several novel attacks against security; (3)
summarize some approaches to address heterogeneity and communication costs
problems; (4)introduce some open source platforms that can be used to build
federated recommendation systems; (5) present some prospective research
directions in the future. This survey can guide researchers and practitioners
understand the research progress in these areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Rank Tensor Completion With Generalized CP Decomposition and
  Nonnegative Integer Tensor Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiran Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor completion is important to many areas such as computer vision, data
analysis, and signal processing. Previously, a category of methods known as
low-rank tensor completion has been proposed and developed, involving the
enforcement of low-rank structures on completed tensors. While such methods
have been constantly improved, none considered exploiting the numerical
properties of tensor elements. This work attempts to construct a new
methodological framework called GCDTC (Generalized CP Decomposition Tensor
Completion) based on numerical properties to achieve higher accuracy in tensor
completion. In this newly introduced framework, a generalized form of the CP
Decomposition is applied to low-rank tensor completion. This paper also
proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for
nonnegative integer tensor completion as an application of the GCDTC framework.
Through experimentation with real-life data, it is verified that this method
could produce results superior in completion accuracy to current
state-of-the-art methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Confusion and Reward Misidentification in Preference-Based Reward
  Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06601v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06601v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D. Dragan, Daniel S. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning policies via preference-based reward learning is an increasingly
popular method for customizing agent behavior, but has been shown anecdotally
to be prone to spurious correlations and reward hacking behaviors. While much
prior work focuses on causal confusion in reinforcement learning and behavioral
cloning, we focus on a systematic study of causal confusion and reward
misidentification when learning from preferences. In particular, we perform a
series of sensitivity and ablation analyses on several benchmark domains where
rewards learned from preferences achieve minimal test error but fail to
generalize to out-of-distribution states -- resulting in poor policy
performance when optimized. We find that the presence of non-causal distractor
features, noise in the stated preferences, and partial state observability can
all exacerbate reward misidentification. We also identify a set of methods with
which to interpret misidentified learned rewards. In general, we observe that
optimizing misidentified rewards drives the policy off the reward's training
distribution, resulting in high predicted (learned) rewards but low true
rewards. These findings illuminate the susceptibility of preference learning to
reward misidentification and causal confusion -- failure to consider even one
of many factors can result in unexpected, undesirable behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the proceedings of the Eleventh International Conference on
  Learning Representations (ICLR 2023).
  $\href{https://iclr.cc/virtual/2023/poster/10822}{\text{URL}}$</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group Fairness in Non-monotone Submodular Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yuan, Shaojie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maximizing a submodular function has a wide range of applications in machine
learning and data mining. One such application is data summarization whose goal
is to select a small set of representative and diverse data items from a large
dataset. However, data items might have sensitive attributes such as race or
gender, in this setting, it is important to design \emph{fairness-aware}
algorithms to mitigate potential algorithmic bias that may cause over- or
under- representation of particular groups. Motivated by that, we propose and
study the classic non-monotone submodular maximization problem subject to novel
group fairness constraints. Our goal is to select a set of items that maximizes
a non-monotone submodular function, while ensuring that the number of selected
items from each group is proportionate to its size, to the extent specified by
the decision maker. We develop the first constant-factor approximation
algorithms for this problem. We also extend the basic model to incorporate an
additional global size constraint on the total number of selected items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in the Journal on
  Combinatorial Optimization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Risks of Stealing the Decoding Algorithms of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near Optimal Memory-Regret Tradeoff for Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binghui Peng, Aviad Rubinstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the experts problem, on each of $T$ days, an agent needs to follow the
advice of one of $n$ ``experts''. After each day, the loss associated with each
expert's advice is revealed. A fundamental result in learning theory says that
the agent can achieve vanishing regret, i.e. their cumulative loss is within
$o(T)$ of the cumulative loss of the best-in-hindsight expert.
  Can the agent perform well without sufficient space to remember all the
experts? We extend a nascent line of research on this question in two
directions:
  $\bullet$ We give a new algorithm against the oblivious adversary, improving
over the memory-regret tradeoff obtained by [PZ23], and nearly matching the
lower bound of [SWXZ22].
  $\bullet$ We also consider an adaptive adversary who can observe past experts
chosen by the agent. In this setting we give both a new algorithm and a novel
lower bound, proving that roughly $\sqrt{n}$ memory is both necessary and
sufficient for obtaining $o(T)$ regret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Hybrid Reinforcement Learning for Latency and Reliability
  Optimization in the Metaverse over Wireless Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yu, Terence Jie Chua, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technology advancements in wireless communications and high-performance
Extended Reality (XR) have empowered the developments of the Metaverse. The
demand for the Metaverse applications and hence, real-time digital twinning of
real-world scenes is increasing. Nevertheless, the replication of 2D physical
world images into 3D virtual objects is computationally intensive and requires
computation offloading. The disparity in transmitted object dimension (2D as
opposed to 3D) leads to asymmetric data sizes in uplink (UL) and downlink (DL).
To ensure the reliability and low latency of the system, we consider an
asynchronous joint UL-DL scenario where in the UL stage, the smaller data size
of the physical world images captured by multiple extended reality users (XUs)
will be uploaded to the Metaverse Console (MC) to be construed and rendered. In
the DL stage, the larger-size 3D virtual objects need to be transmitted back to
the XUs. We design a novel multi-agent reinforcement learning algorithm
structure, namely Asynchronous Actors Hybrid Critic (AAHC), to optimize the
decisions pertaining to computation offloading and channel assignment in the UL
stage and optimize the DL transmission power in the DL stage. Extensive
experiments demonstrate that compared to proposed baselines, AAHC obtains
better solutions with satisfactory training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper appears in IEEE Journal on Selected Areas in
  Communications (JSAC), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08229v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08229v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning (CL) pre-trains general-purpose encoders using an
unlabeled pre-training dataset, which consists of images or image-text pairs.
CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an
attacker injects poisoned inputs into the pre-training dataset so the encoder
is backdoored. However, existing DPBAs achieve limited effectiveness. In this
work, we propose new DPBAs called CorruptEncoder to CL. CorruptEncoder uses a
theory-guided method to create optimal poisoned inputs to maximize attack
effectiveness. Our experiments show that CorruptEncoder substantially
outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA
that achieves more than 90% attack success rates with only a few (3) reference
images and a small poisoning ratio (0.5%). Moreover, we also propose a defense,
called localized cropping, to defend against DPBAs. Our results show that our
defense can reduce the effectiveness of DPBAs, though it slightly sacrifices
the utility of the encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking AutoML algorithms on a collection of synthetic
  classification problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02704v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02704v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique Ribeiro, Patryk Orzechowski, Joost Wagenaar, Jason H. Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated machine learning (AutoML) algorithms have grown in popularity due
to their high performance and flexibility to adapt to different problems and
data sets. With the increasing number of AutoML algorithms, deciding which
would best suit a given problem becomes increasingly more work. Therefore, it
is essential to use complex and challenging benchmarks which would be able to
differentiate the AutoML algorithms from each other. This paper compares the
performance of four different AutoML algorithms: Tree-based Pipeline
Optimization Tool (TPOT), Auto-Sklearn, Auto-Sklearn 2, and H2O AutoML. We use
the Diverse and Generative ML benchmark (DIGEN), a diverse set of synthetic
datasets derived from generative functions designed to highlight the strengths
and weaknesses of the performance of common machine learning algorithms. We
confirm that AutoML can identify pipelines that perform well on all included
datasets. Most AutoML algorithms performed similarly; however, there were some
differences depending on the specific dataset and metric used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Part-Based Models Improve Adversarial Robustness <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chawin Sitawarin, Kornrapat Pongmala, Yizheng Chen, Nicholas Carlini, David Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that combining human prior knowledge with end-to-end learning can
improve the robustness of deep neural networks by introducing a part-based
model for object classification. We believe that the richer form of annotation
helps guide neural networks to learn more robust features without requiring
more samples or larger models. Our model combines a part segmentation model
with a tiny classifier and is trained end-to-end to simultaneously segment
objects into parts and then classify the segmented object. Empirically, our
part-based models achieve both higher accuracy and higher adversarial
robustness than a ResNet-50 baseline on all three datasets. For instance, the
clean accuracy of our part models is up to 15 percentage points higher than the
baseline's, given the same level of robustness. Our experiments indicate that
these models also reduce texture bias and yield better robustness against
common corruptions and spurious correlations. The code is publicly available at
https://github.com/chawins/adv-part-model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2023 (poster). Code can be found at
  https://github.com/chawins/adv-part-model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Predictive Inference with Feature Conformal Prediction <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaye Teng, Chuan Wen, Dinghuai Zhang, <span class="highlight-author">Yoshua Bengio</span>, Yang Gao, Yang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is a distribution-free technique for establishing valid
prediction intervals. Although conventionally people conduct conformal
prediction in the output space, this is not the only possibility. In this
paper, we propose feature conformal prediction, which extends the scope of
conformal prediction to semantic feature spaces by leveraging the inductive
bias of deep representation learning. From a theoretical perspective, we
demonstrate that feature conformal prediction provably outperforms regular
conformal prediction under mild assumptions. Our approach could be combined
with not only vanilla conformal prediction, but also other adaptive conformal
prediction methods. Apart from experiments on existing predictive inference
benchmarks, we also demonstrate the state-of-the-art performance of the
proposed methods on large-scale tasks such as ImageNet classification and
Cityscapes image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TDSTF: <span class="highlight-title">Transformer</span>-based Diffusion probabilistic model for Sparse Time
  series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Chang, Huayu Li, Stuart F. Quan, Janet Roveda, Ao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \noindent \textbf{Background and objective:} In the intensive care unit
(ICU), vital sign monitoring is critical, and an accurate predictive system is
required. This study will create a novel model to forecast Heart Rate (HR),
Systolic Blood Pressure (SBP), and Diastolic Blood Pressure (DBP) in ICU. These
vital signs are crucial for prompt interventions for patients. We extracted
$24,886$ ICU stays from the MIMIC-III database, which contains data from over
$46$ thousand patients, to train and test the model.
  \noindent \textbf{Methods:} The model proposed in this study, areansformerin
intensive careabilistic Model for Sparse Time Series Forecasting (TDSTF), uses
a deep learning technique called the Transformer. The TDSTF model showed
state-of-the-art performance in predicting vital signs in the ICU,
outperforming other models' ability to predict distributions of vital signs and
being more computationally efficient. The code is available at
https://github.com/PingChang818/TDSTF.
  \noindent \textbf{Results:} The results of the study showed that TDSTF
achieved a Normalized Average Continuous Ranked Probability Score (NACRPS) of
$0.4438$ and a Mean Squared Error (MSE) of $0.4168$, an improvement of $18.9\%$
and $34.3\%$ over the best baseline model, respectively.
  \noindent \textbf{Conclusion:} In conclusion, TDSTF is an effective and
efficient solution for forecasting vital signs in the ICU, and it shows a
significant improvement compared to other models in the field.
  \noindent \textbf{Keywords: deep learning, time series forecasting, sparse
data, vital signs, ICU}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Adaptive Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wu, Feihu Huang, Zhengmian Hu, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has attracted increasing attention with the emergence of
distributed data. While extensive federated learning algorithms have been
proposed for the non-convex distributed problem, federated learning in practice
still faces numerous challenges, such as the large training iterations to
converge since the sizes of models and datasets keep increasing, and the lack
of adaptivity by SGD-based model updates. Meanwhile, the study of adaptive
methods in federated learning is scarce and existing works either lack a
complete theoretical convergence guarantee or have slow sample complexity. In
this paper, we propose an efficient adaptive algorithm (i.e., FAFED) based on
the momentum-based variance-reduced technique in cross-silo FL. We first
explore how to design the adaptive algorithm in the FL setting. By providing a
counter-example, we prove that a simple combination of FL and adaptive methods
could lead to divergence. More importantly, we provide a convergence analysis
for our method and prove that our algorithm is the first adaptive FL algorithm
to reach the best-known samples $O(\epsilon^{-3})$ and $O(\epsilon^{-2})$
communication rounds to find an $\epsilon$-stationary point without large
batches. The experimental results on the language modeling task and image
classification task with heterogeneous data demonstrate the efficiency of our
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Validation of a Hospital Digital Twin with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aurangzeb Ahmad, Vijay Chickarmane, Farinaz Sabz Ali Pour, Nima Shariari, Taposh Dutta Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a surge of interest in developing Digital Twins of
process flows in healthcare to better understand bottlenecks and areas of
improvement. A key challenge is in the validation process. We describe a work
in progress for a digital twin using an agent based simulation model for
determining bed turnaround time for patients in hospitals. We employ a strategy
using machine learning for validating the model and implementing sensitivity
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> of Random Relaxations for Crystal Structure Search of Li-Si
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.02920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.02920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gowoon Cheon, Lusann Yang, Kevin McCloskey, Evan J. Reed, Ekin D. Cubuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystal structure search is a long-standing challenge in materials design. We
present a dataset of more than 100,000 structural relaxations of potential
battery anode materials from randomized structures using density functional
theory calculations. We illustrate the usage of the dataset by training graph
neural networks to predict structural relaxations from randomly generated
structures. Our models directly predict stresses in addition to forces, which
allows them to accurately simulate relaxations of both ionic positions and
lattice vectors. We show that models trained on the molecular dynamics
simulations fail to simulate relaxations from random structures, while training
on our data leads to up to two orders of magnitude decrease in error for the
same task. Our model is able to find an experimentally verified structure of a
stoichiometry held out from training. We find that randomly perturbing atomic
positions during training improves both the accuracy and out of domain
generalization of the models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Rates of Stochastic Zeroth-order Gradient Descent for Ł
  ojasiewicz Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16997v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16997v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Wang, Yasong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD)
algorithms for Lojasiewicz functions. The SZGD algorithm iterates as
\begin{align*}
  \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t),
\qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function
that satisfies the \L ojasiewicz inequality with \L ojasiewicz exponent
$\theta$, $\eta_t$ is the step size (learning rate), and $ \widehat{\nabla} f
(\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order
information only.
  Our results show that $ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in
\mathbb{N} } $ can converge faster than $ \{ \| \mathbf{x}_t -
\mathbf{x}_\infty \| \}_{t \in \mathbb{N} }$, regardless of whether the
objective $f$ is smooth or nonsmooth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V3: more than major revision. Y. Feng is added to the author list.
  V4: length cut and some typo corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Backdoor Detection and Mitigation in Competitive Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03609v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03609v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Guo, Ang Li, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While real-world applications of reinforcement learning are becoming popular,
the security and robustness of RL systems are worthy of more attention and
exploration. In particular, recent works have revealed that, in a multi-agent
RL environment, backdoor trigger actions can be injected into a victim agent
(a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it
sees the backdoor trigger action. To ensure the security of RL agents against
malicious backdoors, in this work, we propose the problem of Backdoor Detection
in a multi-agent competitive reinforcement learning system, with the objective
of detecting Trojan agents as well as the corresponding potential trigger
actions, and further trying to mitigate their Trojan behavior. In order to
solve this problem, we propose PolicyCleanse that is based on the property that
the activated Trojan agents accumulated rewards degrade noticeably after
several timesteps. Along with PolicyCleanse, we also design a machine
unlearning-based approach that can effectively mitigate the detected backdoor.
Extensive experiments demonstrate that the proposed methods can accurately
detect Trojan agents, and outperform existing backdoor mitigation baseline
approaches by at least 3% in winning rate across various types of agents and
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pishgu: Universal Path Prediction Network Architecture for Real-time
  Cyber-physical Edge Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghazal Alinezhad Noghre, Vinit Katariya, Armin Danesh Pazho, Christopher Neff, Hamed Tabkhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path prediction is an essential task for many real-world Cyber-Physical
Systems (CPS) applications, from autonomous driving and traffic
monitoring/management to pedestrian/worker safety. These real-world CPS
applications need a robust, lightweight path prediction that can provide a
universal network architecture for multiple subjects (e.g., pedestrians and
vehicles) from different perspectives. However, most existing algorithms are
tailor-made for a unique subject with a specific camera perspective and
scenario. This article presents Pishgu, a universal lightweight network
architecture, as a robust and holistic solution for path prediction. Pishgu's
architecture can adapt to multiple path prediction domains with different
subjects (vehicles, pedestrians), perspectives (bird's-eye, high-angle), and
scenes (sidewalk, highway). Our proposed architecture captures the
inter-dependencies within the subjects in each frame by taking advantage of
Graph Isomorphism Networks and the attention module. We separately train and
evaluate the efficacy of our architecture on three different CPS domains across
multiple perspectives (vehicle bird's-eye view, pedestrian bird's-eye view, and
human high-angle view). Pishgu outperforms state-of-the-art solutions in the
vehicle bird's-eye view domain by 42% and 61% and pedestrian high-angle view
domain by 23% and 22% in terms of ADE and FDE, respectively. Additionally, we
analyze the domain-specific details for various datasets to understand their
effect on path prediction and model interpretation. Finally, we report the
latency and throughput for all three domains on multiple embedded platforms
showcasing the robustness and adaptability of Pishgu for real-world integration
into CPS applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient recurrent architectures through activity sparsity and sparse
  back-propagation through time <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06178v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06178v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs) are well suited for solving sequence tasks
in resource-constrained systems due to their expressivity and low computational
requirements. However, there is still a need to bridge the gap between what
RNNs are capable of in terms of efficiency and performance and real-world
application requirements. The memory and computational requirements arising
from propagating the activations of all the neurons at every time step to every
connected neuron, together with the sequential dependence of activations,
contribute to the inefficiency of training and using RNNs. We propose a
solution inspired by biological neuron dynamics that makes the communication
between RNN units sparse and discrete. This makes the backward pass with
backpropagation through time (BPTT) computationally sparse and efficient as
well. We base our model on the gated recurrent unit (GRU), extending it with
units that emit discrete events for communication triggered by a threshold so
that no information is communicated to other units in the absence of events. We
show theoretically that the communication between units, and hence the
computation required for both the forward and backward passes, scales with the
number of events in the network. Our model achieves efficiency without
compromising task performance, demonstrating competitive performance compared
to state-of-the-art recurrent network models in real-world tasks, including
language modeling. The dynamic activity sparsity mechanism also makes our model
well suited for novel energy-efficient neuromorphic hardware. Code is available
at https://github.com/KhaleelKhan/EvNN/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as notable-top-25% paper in ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guaranteed Conformance of Neurosymbolic Models to Natural Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01346v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01346v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh Sridhar, Souradeep Dutta, James Weimer, Insup Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have emerged as the workhorse for a large section of
robotics and control applications, especially as models for dynamical systems.
Such data-driven models are in turn used for designing and verifying autonomous
systems. This is particularly useful in modeling medical systems where data can
be leveraged to individualize treatment. In safety-critical applications, it is
important that the data-driven model is conformant to established knowledge
from the natural sciences. Such knowledge is often available or can often be
distilled into a (possibly black-box) model $M$. For instance, the unicycle
model (which encodes Newton's laws) for an F1 racing car. In this light, we
consider the following problem - given a model $M$ and state transition
dataset, we wish to best approximate the system model while being bounded
distance away from $M$. We propose a method to guarantee this conformance. Our
first step is to distill the dataset into few representative samples called
memories, using the idea of a growing neural gas. Next, using these memories we
partition the state space into disjoint subsets and compute bounds that should
be respected by the neural network, when the input is drawn from a particular
subset. This serves as a symbolic wrapper for guaranteed conformance. We argue
theoretically that this only leads to bounded increase in approximation error;
which can be controlled by increasing the number of memories. We experimentally
show that on three case studies (Car Model, Drones, and Artificial Pancreas),
our constrained neurosymbolic models conform to specified $M$ models (each
encoding various constraints) with order-of-magnitude improvements compared to
the augmented Lagrangian and vanilla training methods. Our code can be found at
https://github.com/kaustubhsridhar/Constrained_Models
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low Dimensional Invariant Embeddings for Universal Geometric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Dym, Steven J. Gortler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies separating invariants: mappings on $D$ dimensional domains
which are invariant to an appropriate group action, and which separate orbits.
The motivation for this study comes from the usefulness of separating
invariants in proving universality of equivariant neural network architectures.
  We observe that in several cases the cardinality of separating invariants
proposed in the machine learning literature is much larger than the dimension
$D$. As a result, the theoretical universal constructions based on these
separating invariants is unrealistically large. Our goal in this paper is to
resolve this issue.
  We show that when a continuous family of semi-algebraic separating invariants
is available, separation can be obtained by randomly selecting 2D+1 of these
invariants. We apply this methodology to obtain an efficient scheme for
computing separating invariants for several classical group actions which have
been studied in the invariant learning literature. Examples include matrix
multiplication actions on point clouds by permutations, rotations, and various
other linear groups.
  Often the requirement of invariant separation is relaxed and only generic
separation is required. In this case, we show that only D+1 invariants are
required. More importantly, generic invariants are often significantly easier
to compute, as we illustrate by discussing generic and full separation for
weighted graphs. Finally we outline an approach for proving that separating
invariants can be constructed also when the random parameters have finite
precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Typology for Exploring the Mitigation of Shortcut Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03668v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03668v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models become increasingly larger, trained weakly
supervised on large, possibly uncurated data sets, it becomes increasingly
important to establish mechanisms for inspecting, interacting, and revising
models to mitigate learning shortcuts and guarantee their learned knowledge is
aligned with human knowledge. The recently proposed XIL framework was developed
for this purpose, and several such methods have been introduced, each with
individual motivations and methodological details. In this work, we provide a
unification of various XIL methods into a single typology by establishing a
common set of basic modules. In doing so, we pave the way for a principled
comparison of existing, but, importantly, also future XIL approaches. In
addition, we discuss existing and introduce novel measures and benchmarks for
evaluating the overall abilities of a XIL method. Given this extensive toolbox,
including our typology, measures, and benchmarks, we finally compare several
recent XIL methods methodologically and quantitatively. In our evaluations, all
methods prove to revise a model successfully. However, we found remarkable
differences in individual benchmark tasks, revealing valuable
application-relevant aspects for integrating these benchmarks in developing
future methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning POD of Complex Dynamics Using Heavy-ball Neural ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Baker, Elena Cherkaev, Akil Narayan, Bao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proper orthogonal decomposition (POD) allows reduced-order modeling of
complex dynamical systems at a substantial level, while maintaining a high
degree of accuracy in modeling the underlying dynamical systems. Advances in
machine learning algorithms enable learning POD-based dynamics from data and
making accurate and fast predictions of dynamical systems. In this paper, we
leverage the recently proposed heavy-ball neural ODEs (HBNODEs) [Xia et al.
NeurIPS, 2021] for learning data-driven reduced-order models (ROMs) in the POD
context, in particular, for learning dynamics of time-varying coefficients
generated by the POD analysis on training snapshots generated from solving full
order models. HBNODE enjoys several practical advantages for learning POD-based
ROMs with theoretical guarantees, including 1) HBNODE can learn long-term
dependencies effectively from sequential observations and 2) HBNODE is
computationally efficient in both training and testing. We compare HBNODE with
other popular ROMs on several complex dynamical systems, including the von
K\'{a}rm\'{a}n Street flow, the Kurganov-Petrova-Popov equation, and the
one-dimensional Euler equations for fluids modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linked Data Science Powered by <span class="highlight-title">Knowledge</span> Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mossad Helali, Shubham Vashisth, Philippe Carrier, Katja Hose, Essam Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have witnessed a growing interest in data science not
only from academia but particularly from companies investing in data science
platforms to analyze large amounts of data. In this process, a myriad of data
science artifacts, such as datasets and pipeline scripts, are created. Yet,
there has so far been no systematic attempt to holistically exploit the
collected knowledge and experiences that are implicitly contained in the
specification of these pipelines, e.g., compatible datasets, cleansing steps,
ML algorithms, parameters, etc. Instead, data scientists still spend a
considerable amount of their time trying to recover relevant information and
experiences from colleagues, trial and error, lengthy exploration, etc. In this
paper, we, therefore, propose a scalable system (KGLiDS) that employs machine
learning to extract the semantics of data science pipelines and captures them
in a knowledge graph, which can then be exploited to assist data scientists in
various ways. This abstraction is the key to enabling Linked Data Science since
it allows us to share the essence of pipelines between platforms, companies,
and institutions without revealing critical internal information and instead
focusing on the semantics of what is being processed and how. Our comprehensive
evaluation uses thousands of datasets and more than thirteen thousand pipeline
scripts extracted from data discovery benchmarks and the Kaggle portal and
shows that KGLiDS significantly outperforms state-of-the-art systems on related
tasks, such as dataset recommendation and pipeline classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Activation-based Structured Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10520v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10520v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqi Zhao, Animesh Jain, Ming Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning is a promising approach to compress complex deep learning models in
order to deploy them on resource-constrained edge devices. However, many
existing pruning solutions are based on unstructured pruning, which yields
models that cannot efficiently run on commodity hardware and require users to
manually explore and tune the pruning process, which is time-consuming and
often leads to sub-optimal results. To address these limitations, this paper
presents an adaptive, activation-based, structured pruning approach to
automatically and efficiently generate small, accurate, and hardware-efficient
models that meet user requirements. First, it proposes iterative structured
pruning using activation-based attention feature maps to effectively identify
and prune unimportant filters. Then, it proposes adaptive pruning policies for
automatically meeting the pruning objectives of accuracy-critical,
memory-constrained, and latency-sensitive tasks. A comprehensive evaluation
shows that the proposed method can substantially outperform the
state-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets.
For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method
achieves the largest parameter reduction (79.11%), outperforming the related
works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%),
outperforming the related works by 14.13% to 26.53%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Denoising via Amplification and Stable Rank Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Gryak, Kayvan Najarian, Harm Derksen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensors in the form of multilinear arrays are ubiquitous in data science
applications. Captured real-world data, including video, hyperspectral images,
and discretized physical systems, naturally occur as tensors and often come
with attendant noise. Under the additive noise model and with the assumption
that the underlying clean tensor has low rank, many denoising methods have been
created that utilize tensor decomposition to effect denoising through low rank
tensor approximation. However, all such decomposition methods require
estimating the tensor rank, or related measures such as the tensor spectral and
nuclear norms, all of which are NP-hard problems.
  In this work we leverage our previously developed framework of
$\textit{tensor amplification}$, which provides good approximations of the
spectral and nuclear tensor norms, to denoising synthetic tensors of various
sizes, ranks, and noise levels, along with real-world tensors derived from
physiological signals. We also introduce two new notions of tensor rank --
$\textit{stable slice rank}$ and $\textit{stable }$$X$$\textit{-rank}$ -- and
new denoising methods based on their estimation. The experimental results show
that in the low rank context, tensor-based amplification provides comparable
denoising performance in high signal-to-noise ratio (SNR) settings and superior
performance in noisy (i.e., low SNR) settings, while the stable $X$-rank method
achieves superior denoising performance on the physiological signal data.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMCosine: Multi-Modal Cosine <span class="highlight-title">Loss</span> Towards Balanced Audio-Visual
  Fine-Grained Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruize Xu, Ruoxuan Feng, Shi-Xiong Zhang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual learning helps to comprehensively understand the world by fusing
practical information from multiple modalities. However, recent studies show
that the imbalanced optimization of uni-modal encoders in a joint-learning
model is a bottleneck to enhancing the model's performance. We further find
that the up-to-date imbalance-mitigating methods fail on some audio-visual
fine-grained tasks, which have a higher demand for distinguishable feature
distribution. Fueled by the success of cosine loss that builds hyperspherical
feature spaces and achieves lower intra-class angular variability, this paper
proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$
normalization to features and weights towards balanced and better multi-modal
fine-grained learning. We demonstrate that our method can alleviate the
imbalanced optimization from the perspective of weight norm and fully exploit
the discriminability of the cosine metric. Extensive experiments prove the
effectiveness of our method and the versatility with advanced multi-modal
fusion strategies and up-to-date imbalance-mitigating methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Few-Shot</span> Learning for Talking Face System with TTS Data
  Augmentation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chen, Ziyang Ma, Tao Liu, Xu Tan, Qu Lu, Xie Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face has attracted broad interest from academia and
industry recently. However, data acquisition and labeling in audio-driven
talking face are labor-intensive and costly. The lack of data resource results
in poor synthesis effect. To alleviate this issue, we propose to use TTS
(Text-To-Speech) for data augmentation to improve few-shot ability of the
talking face system. The misalignment problem brought by the TTS audio is
solved with the introduction of soft-DTW, which is first adopted in the talking
face task. Moreover, features extracted by HuBERT are explored to utilize
underlying information of audio, and found to be superior over other features.
The proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW score
and user study preference repectively over the baseline model, which shows the
effectiveness of improving few-shot learning for talking face system with TTS
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Image-in-Audio Deep Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaume Ros Alonso, Margarita Geleta, Jordi Pons, Xavier Giro-i-Nieto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of steganography has experienced a surge of interest due to the
recent advancements in AI-powered techniques, particularly in the context of
multimodal setups that enable the concealment of signals within signals of a
different nature. The primary objectives of all steganographic methods are to
achieve perceptual transparency, robustness, and large embedding capacity -
which often present conflicting goals that classical methods have struggled to
reconcile. This paper extends and enhances an existing image-in-audio deep
steganography method by focusing on improving its robustness. The proposed
enhancements include modifications to the loss function, utilization of the
Short-Time Fourier Transform (STFT), introduction of redundancy in the encoding
process for error correction, and buffering of additional information in the
pixel subconvolution operation. The results demonstrate that our approach
outperforms the existing method in terms of robustness and perceptual
transparency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud
  Compression <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Sheng Liu, Jia-Fong Yeh, Hao Hsu, Hung-Ting Su, Ming-Sui Lee, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large amount of data collected by LiDAR sensors brings the issue of LiDAR
point cloud compression (PCC). Previous works on LiDAR PCC have used range
image representations and followed the predictive coding paradigm to create a
basic prototype of a coding framework. However, their prediction methods give
an inaccurate result due to the negligence of invalid pixels in range images
and the omission of future frames in the time step. Moreover, their handcrafted
design of residual coding methods could not fully exploit spatial redundancy.
To remedy this, we propose a coding framework BIRD-PCC. Our prediction module
is aware of the coordinates of invalid pixels in range images and takes a
bidirectional scheme. Also, we introduce a deep-learned residual coding module
that can further exploit spatial redundancy within a residual frame.
Experiments conducted on SemanticKITTI and KITTI-360 datasets show that
BIRD-PCC outperforms other methods in most bitrate conditions and generalizes
well to unseen environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-08T00:00:00Z">2023-03-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Event Representations using Event <span class="highlight-title">Knowledge</span> Graphs and
  Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin Kuculo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has utilised knowledge-aware approaches to natural language
understanding, question answering, recommendation systems, and other tasks.
These approaches rely on well-constructed and large-scale knowledge graphs that
can be useful for many downstream applications and empower knowledge-aware
models with commonsense reasoning. Such knowledge graphs are constructed
through knowledge acquisition tasks such as relation extraction and knowledge
graph completion. This work seeks to utilise and build on the growing body of
work that uses findings from the field of natural language processing (NLP) to
extract knowledge from text and build knowledge graphs. The focus of this
research project is on how we can use transformer-based approaches to extract
and contextualise event information, matching it to existing ontologies, to
build a comprehensive knowledge of graph-based event representations.
Specifically, sub-event extraction is used as a way of creating sub-event-aware
event representations. These event representations are then further enriched
through fine-grained location extraction and contextualised through the
alignment of historically relevant quotes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the author's version of the work. It is posted here for your
  personal use. Not for redistribution. The definitive Version of Record was
  published in Companion Proceedings of the Web Conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Risks of Stealing the Decoding Algorithms of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending the <span class="highlight-title">Pre-Train</span>ing of BLOOM for Improved Support of Traditional
  Chinese: Models, Methods and Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Ennen, Po-Chun Hsu, Chan-Jan Hsu, Chang-Le Liu, Yen-Chen Wu, Yin-Hsiang Liao, Chin-Tung Lin, Da-Shan Shiu, Wei-Yun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the multilingual language model BLOOM-zh that
features enhanced support for Traditional Chinese. BLOOM-zh has its origins in
the open-source BLOOM models presented by BigScience in 2022. Starting from
released models, we extended the pre-training of BLOOM by additional 7.4
billion tokens in Traditional Chinese and English covering a variety of domains
such as news articles, books, encyclopedias, educational materials as well as
spoken language. In order to show the properties of BLOOM-zh, both existing and
newly created benchmark scenarios are used for evaluating the performance.
BLOOM-zh outperforms its predecessor on most Traditional Chinese benchmarks
while maintaining its English capability. We release all our models to the
research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-contained Beta-with-Spikes Approximation for Inference Under a
  Wright-Fisher Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Guerrero Montero, Richard A. Blythe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We construct a reliable estimation of evolutionary parameters within the
Wright-Fisher model, which describes changes in allele frequencies due to
selection and genetic drift, from time-series data. Such data exists for
biological populations, for example via artificial evolution experiments, and
for the cultural evolution of behavior, such as linguistic corpora that
document historical usage of different words with similar meanings. Our method
of analysis builds on a Beta-with-Spikes approximation to the distribution of
allele frequencies predicted by the Wright-Fisher model. We introduce a
self-contained scheme for estimating the parameters in the approximation, and
demonstrate its robustness with synthetic data, especially in the
strong-selection and near-extinction regimes where previous approaches fail. We
further apply to allele frequency data for baker's yeast (Saccharomyces
cerevisiae), finding a significant signal of selection in cases where
independent evidence supports such a conclusion. We further demonstrate the
possibility of detecting time-points at which evolutionary parameters change in
the context of a historical spelling reform in the Spanish language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Effective Hyperparameter Optimization for Large Language Model
  <span class="highlight-title">Generation</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like GPT-3 have sparked significant interest in
their generative capabilities, leading to the development of various commercial
applications. The high cost of using the models drives application builders to
maximize the value of generation under a limited inference budget. This paper
presents a study of optimizing inference hyperparameters like the number of
responses, temperature and max tokens, which significantly affects the
utility/cost of text generation. We design a framework named EcoOptiGen which
leverages economical hyperparameter optimization and cost-based pruning.
Experiments with the latest GPT-3.5 models on a variety of tasks verify its
effectiveness. EcoOptiGen is implemented in the FLAML library:
https://github.com/microsoft/FLAML, and we provide one example of using it at:
https://microsoft.github.io/FLAML/docs/Examples/Integrate%20-%20OpenAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrapolative Controlled Sequence <span class="highlight-title">Generation</span> via Iterative Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakh Padmakumar, Richard Yuanzhe Pang, He He, Ankur P. Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of extrapolative controlled generation, i.e., generating
sequences with attribute values beyond the range seen in training. This task is
of significant importance in automated design, especially drug discovery, where
the goal is to design novel proteins that are \textit{better} (e.g., more
stable) than existing sequences. Thus, by definition, the target sequences and
their attribute values are out of the training distribution, posing challenges
to existing methods that aim to directly generate the target sequence. Instead,
in this work, we propose Iterative Controlled Extrapolation (ICE) which
iteratively makes local edits to a sequence to enable extrapolation. We train
the model on synthetically generated sequence pairs that demonstrate small
improvement in the attribute value. Results on one natural language task
(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV
fitness) show that ICE considerably outperforms state-of-the-art approaches
despite its simplicity. Our code and models are available at:
https://github.com/vishakhpk/iter-extrapolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Models of symbol emergence in communication: a conceptual <span class="highlight-title">review</span> and a
  guide for avoiding local minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Zubek, Tomasz Korbak, Joanna Rączaszek-Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational simulations are a popular method for testing hypotheses about
the emergence of communication. This kind of research is performed in a variety
of traditions including language evolution, developmental psychology, cognitive
science, machine learning, robotics, etc. The motivations for the models are
different, but the operationalizations and methods used are often similar. We
identify the assumptions and explanatory targets of several most representative
models and summarise the known results. We claim that some of the assumptions
-- such as portraying meaning in terms of mapping, focusing on the descriptive
function of communication, modelling signals with amodal tokens -- may hinder
the success of modelling. Relaxing these assumptions and foregrounding the
interactions of embodied and situated agents allows one to systematise the
multiplicity of pressures under which symbolic systems evolve. In line with
this perspective, we sketch the road towards modelling the emergence of
meaningful symbolic communication, where symbols are simultaneously grounded in
action and perception and form an abstract system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Student's t-Distribution: On Measuring the Inter-Rater Reliability When
  the Observations are Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serge Gladkoff, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language processing (NLP) we always rely on human judgement as the
golden quality evaluation method. However, there has been an ongoing debate on
how to better evaluate inter-rater reliability (IRR) levels for certain
evaluation tasks, such as translation quality evaluation (TQE), especially when
the data samples (observations) are very scarce. In this work, we first
introduce the study on how to estimate the confidence interval for the
measurement value when only one data (evaluation) point is available. Then,
this leads to our example with two human-generated observational scores, for
which, we introduce ``Student's \textit{t}-Distribution'' method and explain
how to use it to measure the IRR score using only these two data points, as
well as the confidence intervals (CIs) of the quality evaluation. We give
quantitative analysis on how the evaluation confidence can be greatly improved
by introducing more observations, even if only one extra observation. We
encourage researchers to report their IRR scores in all possible means, e.g.
using Student's \textit{t}-Distribution method whenever possible; thus making
the NLP evaluation more meaningful, transparent, and trustworthy. This
\textit{t}-Distribution method can be also used outside of NLP fields to
measure IRR level for trustworthy evaluation of experimental investigations,
whenever the observational data is scarce.
  Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence
Intervals (CIs); Natural Language Processing (NLP); Translation Quality
Evaluation (TQE); Student's \textit{t}-Distribution
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MenuCraft: Interactive Menu System Design with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hossein Kargaran, Nafiseh Nikeghbal, Abbas Heydarnoori, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Menu system design is a challenging task involving many design options and
various human factors. For example, one crucial factor that designers need to
consider is the semantic and systematic relation of menu commands. However,
capturing these relations can be challenging due to limited available
resources. With the advancement of neural language models, large language
models can utilize their vast pre-existing knowledge in designing and refining
menu systems.
  In this paper, we propose MenuCraft, an AI-assisted designer for menu design
that enables collaboration between the designer and a dialogue system to design
menus. MenuCraft offers an interactive language-based menu design tool that
simplifies the menu design process and enables easy customization of design
options. MenuCraft supports a variety of interactions through dialog that
allows performing few-shot learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query-Utterance Attention with Joint modeling for Query-Focused Meeting
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxian Liu, Bin Duan, Bo Xiao, Yajing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-focused meeting summarization (QFMS) aims to generate summaries from
meeting transcripts in response to a given query. Previous works typically
concatenate the query with meeting transcripts and implicitly model the query
relevance only at the token level with attention mechanism. However, due to the
dilution of key query-relevant information caused by long meeting transcripts,
the original transformer-based model is insufficient to highlight the key parts
related to the query. In this paper, we propose a query-aware framework with
joint modeling token and utterance based on Query-Utterance Attention. It
calculates the utterance-level relevance to the query with a dense retrieval
module. Then both token-level query relevance and utterance-level query
relevance are combined and incorporated into the generation process with
attention mechanism explicitly. We show that the query relevance of different
granularities contributes to generating a summary more related to the query.
Experimental results on the QMSum dataset show that the proposed model achieves
new state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>icassp 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL-entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL-entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Auditing Large Language Models via Discrete Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auditing large language models for unexpected behaviors is critical to
preempt catastrophic deployments, yet remains challenging. In this work, we
cast auditing as an optimization problem, where we automatically search for
input-output pairs that match a desired target behavior. For example, we might
aim to find a non-toxic input that starts with "Barack Obama" that a model maps
to a toxic output. This optimization problem is difficult to solve as the set
of feasible points is sparse, the space is discrete, and the language models we
audit are non-linear and high-dimensional. To combat these challenges, we
introduce a discrete optimization algorithm, ARCA, that jointly and efficiently
optimizes over inputs and outputs. Our approach automatically uncovers
derogatory completions about celebrities (e.g. "Barack Obama is a legalized
unborn" -> "child murderer"), produces French inputs that complete to English
outputs, and finds inputs that generate a specific name. Our work offers a
promising new tool to uncover models' failure-modes before deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample Efficient Multimodal Semantic Augmentation for Incremental
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanta Bhattacharyya, Ramesh Manuvinakurike, Sahisnu Mazumder, Saurav Sahay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we develop a prompting approach for incremental summarization
of task videos. We develop a sample-efficient few-shot approach for extracting
semantic concepts as an intermediate step. We leverage an existing model for
extracting the concepts from the images and extend it to videos and introduce a
clustering and querying approach for sample efficiency, motivated by the recent
advances in perceiver-based architectures. Our work provides further evidence
that an approach with richer input context with relevant entities and actions
from the videos and using these as prompts could enhance the summaries
generated by the model. We show the results on a relevant dataset and discuss
possible directions for the work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Synthetic Data <span class="highlight-title">Generation</span> of LLMs Help Clinical Text Mining? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have led to the
development of highly potent models like OpenAI's ChatGPT. These models have
exhibited exceptional performance in a variety of tasks, such as question
answering, essay composition, and code generation. However, their effectiveness
in the healthcare sector remains uncertain. In this study, we seek to
investigate the potential of ChatGPT to aid in clinical text mining by
examining its ability to extract structured information from unstructured
healthcare texts, with a focus on biological named entity recognition and
relation extraction. However, our preliminary results indicate that employing
ChatGPT directly for these tasks resulted in poor performance and raised
privacy concerns associated with uploading patients' information to the ChatGPT
API. To overcome these limitations, we propose a new training paradigm that
involves generating a vast quantity of high-quality synthetic data with labels
utilizing ChatGPT and fine-tuning a local model for the downstream task. Our
method has resulted in significant improvements in the performance of
downstream tasks, improving the F1-score from 23.37% to 63.99% for the named
entity recognition task and from 75.86% to 83.59% for the relation extraction
task. Furthermore, generating data using ChatGPT can significantly reduce the
time and effort required for data collection and labeling, as well as mitigate
data privacy concerns. In summary, the proposed framework presents a promising
solution to enhance the applicability of LLM models to clinical text mining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexical Complexity Prediction: An <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai North, Marcos Zampieri, Matthew Shardlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The occurrence of unknown words in texts significantly hinders reading
comprehension. To improve accessibility for specific target populations,
computational modelling has been applied to identify complex words in texts and
substitute them for simpler alternatives. In this paper, we present an overview
of computational approaches to lexical complexity prediction focusing on the
work carried out on English data. We survey relevant approaches to this problem
which include traditional machine learning classifiers (e.g. SVMs, logistic
regression) and deep neural networks as well as a variety of features, such as
those inspired by literature in psycholinguistics as well as word frequency,
word length, and many others. Furthermore, we introduce readers to past
competitions and available datasets created on this topic. Finally, we include
brief sections on applications of lexical complexity prediction, such as
readability and text simplification, together with related studies on languages
other than English.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Casual <span class="highlight-title">Conversation</span>s v2 <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Porgali, Vítor Albiero, Jordan Ryda, Cristian Canton Ferrer, Caner Hazirbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new large consent-driven dataset aimed at assisting
in the evaluation of algorithmic bias and robustness of computer vision and
audio speech models in regards to 11 attributes that are self-provided or
labeled by trained annotators. The dataset includes 26,467 videos of 5,567
unique paid participants, with an average of almost 5 videos per person,
recorded in Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the
USA, representing diverse demographic characteristics. The participants agreed
for their data to be used in assessing fairness of AI models and provided
self-reported age, gender, language/dialect, disability status, physical
adornments, physical attributes and geo-location information, while trained
annotators labeled apparent skin tone using the Fitzpatrick Skin Type and Monk
Skin Tone scales, and voice timbre. Annotators also labeled for different
recording setups and per-second activity annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ disco: a toolkit for Distributional Control of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germán Kruszewski, Jos Rozen, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models and other generative models have revolutionized
NLP and beyond. However, these models tend to reproduce undesirable biases
present in their training data. Also, they may overlook patterns that are
important but challenging to capture. To address these limitations, researchers
have introduced distributional control techniques. These techniques, not
limited to language, allow controlling the prevalence (i.e., expectations) of
any features of interest in the model's outputs. Despite their potential, the
widespread adoption of these techniques has been hindered by the difficulty in
adapting complex, disconnected code. Here, we present disco, an open-source
Python library that brings these techniques to the broader public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Detection of Industry Sectors in Legal Articles Using Machine
  Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yang, Stella Hadjiantoni, Yunfei Long, Ruta Petraityte, Berthold Lausen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to automatically identify industry sector coverage in articles on
legal developments, or any kind of news articles for that matter, can bring
plentiful of benefits both to the readers and the content creators themselves.
By having articles tagged based on industry coverage, readers from all around
the world would be able to get to legal news that are specific to their region
and professional industry. Simultaneously, writers would benefit from
understanding which industries potentially lack coverage or which industries
readers are currently mostly interested in and thus, they would focus their
writing efforts towards more inclusive and relevant legal news coverage. In
this paper, a Machine Learning-powered industry analysis approach which
combined Natural Language Processing (NLP) with Statistical and Machine
Learning (ML) techniques was investigated. A dataset consisting of over 1,700
annotated legal articles was created for the identification of six industry
sectors. Text and legal based features were extracted from the text. Both
traditional ML methods (e.g. gradient boosting machine algorithms, and
decision-tree based algorithms) and deep neural network (e.g. transformer
models) were applied for performance comparison of predictive models. The
system achieved promising results with area under the receiver operating
characteristic curve scores above 0.90 and F-scores above 0.81 with respect to
the six industry sectors. The experimental results show that the suggested
automated industry analysis which employs ML techniques allows the processing
of large collections of text data in an easy, efficient, and scalable way.
Traditional ML methods perform better than deep neural networks when only a
small and domain-specific training data is available for the study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 3 tables. Paper was presented at 'Classification
  and Data Science in the Digital Age', 17th conference of the International
  Federation of Classification Societies (IFCS2022), Porto, Portugal,
  https://ifcs2022.fep.up.pt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceChat: An Emotion-Aware Face-to-face <span class="highlight-title">Dialogue</span> Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deema Alnuhait, Qingyang Wu, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current dialogue systems like ChatGPT have made significant
advancements in text-based interactions, they often overlook the potential of
other modalities in enhancing the overall user experience. We present FaceChat,
a web-based dialogue framework that enables emotionally-sensitive and
face-to-face conversations. By seamlessly integrating cutting-edge technologies
in natural language processing, computer vision, and speech processing,
FaceChat delivers a highly immersive and engaging user experience. FaceChat
framework has a wide range of potential applications, including counseling,
emotional support, and personalized customer service. The system is designed to
be simple and flexible as a platform for future researchers to advance the
field of multimodal dialogue systems. The code is publicly available at
https://github.com/qywu/FaceChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lila: A Unified Benchmark for Mathematical <span class="highlight-title">Reasoning</span> <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning skills are essential for general-purpose intelligent
systems to perform tasks from grocery shopping to climate modeling. Towards
evaluating and improving AI systems in this domain, we propose LILA, a unified
mathematical reasoning benchmark consisting of 23 diverse tasks along four
dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language
format e.g., question-answering, fill-in-the-blanks (iii) language diversity
e.g., no language, simple language (iv) external knowledge e.g., commonsense,
physics. We construct our benchmark by extending 20 datasets benchmark by
collecting task instructions and solutions in the form of Python programs,
thereby obtaining explainable solutions in addition to the correct answer. We
additionally introduce two evaluation datasets to measure out-of-distribution
performance and robustness to language perturbation. Finally, we introduce
BHASKARA, a general-purpose mathematical reasoning model trained on LILA.
Importantly, we find that multi-tasking leads to significant improvements
(average relative improvement of 21.83% F1 score vs. single-task models), while
the best performing model only obtains 60.40%, indicating the room for
improvement in general mathematical reasoning and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised Concept Map <span class="highlight-title">Generation</span> through Task-Guided Graph
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Lu, Xiangjue Dong, Carl Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the rapid development of concept map generation
techniques due to their advantages in providing well-structured summarization
of knowledge from free texts. Traditional unsupervised methods do not generate
task-oriented concept maps, whereas deep generative models require large
amounts of training data. In this work, we present GT-D2G (Graph
Translation-based Document To Graph), an automatic concept map generation
framework that leverages generalized NLP pipelines to derive semantic-rich
initial graphs, and translates them into more concise structures under the weak
supervision of downstream task labels. The concept maps generated by GT-D2G can
provide interpretable summarization of structured knowledge for the input
texts, which are demonstrated through human evaluation and case studies on
three real-world corpora. Further experiments on the downstream task of
document classification show that GT-D2G beats other concept map generation
methods. Moreover, we specifically validate the labeling efficiency of GT-D2G
in the label-efficient learning setting and the flexibility of generated graph
sizes in controlled hyper-parameter studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE. All code and data available at
  https://github.com/lujiaying/GT-doc2graph</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptation of <span class="highlight-title">Transformer</span>-Based Models using Unlabeled Data for
  Relevance and Polarity Classification of German Customer Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Idrissi-Yaghir, Henning Schäfer, Nadja Bauer, Christoph M. Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding customer feedback is becoming a necessity for companies to
identify problems and improve their products and services. Text classification
and sentiment analysis can play a major role in analyzing this data by using a
variety of machine and deep learning approaches. In this work, different
transformer-based models are utilized to explore how efficient these models are
when working with a German customer feedback dataset. In addition, these
pre-trained models are further analyzed to determine if adapting them to a
specific domain using unlabeled data can yield better results than
off-the-shelf pre-trained models. To evaluate the models, two downstream tasks
from the GermEval 2017 are considered. The experimental results show that
transformer-based models can reach significant improvements compared to a
fastText baseline and outperform the published scores and previous models. For
the subtask Relevance Classification, the best models achieve a micro-averaged
$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a
score of 85.1 % and 85.3 % for the subtask Polarity Classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Complete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Inferential Reproducibility of Machine Learning Research <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hagmann, Philipp Meier, Stefan Riezler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliability of machine learning evaluation -- the consistency of observed
evaluation scores across replicated model training runs -- is affected by
several sources of nondeterminism which can be regarded as measurement noise.
Current tendencies to remove noise in order to enforce reproducibility of
research results neglect inherent nondeterminism at the implementation level
and disregard crucial interaction effects between algorithmic noise factors and
data properties. This limits the scope of conclusions that can be drawn from
such experiments. Instead of removing noise, we propose to incorporate several
sources of variance, including their interaction with data properties, into an
analysis of significance and reliability of machine learning evaluation, with
the aim to draw inferences beyond particular instances of trained models. We
show how to use linear mixed effects models (LMEMs) to analyze performance
evaluation scores, and to conduct statistical inference with a generalized
likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources
of noise like meta-parameter variations into statistical significance testing,
and to assess performance differences conditional on data properties.
Furthermore, a variance component analysis (VCA) enables the analysis of the
contribution of noise sources to overall variance and the computation of a
reliability coefficient by the ratio of substantial to total variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023 (see https://openreview.net/pdf?id=li4GQCQWkv)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding Language with Visual Affordances over Unstructured Data <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oier Mees, Jessica Borja-Diaz, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that Large Language Models (LLMs) can be applied to
ground natural language to a wide variety of robot skills. However, in
practice, learning multi-task, language-conditioned robotic skills typically
requires large-scale data collection and frequent human intervention to reset
the environment or help correcting the current policies. In this work, we
propose a novel approach to efficiently learn general-purpose
language-conditioned robot skills from unstructured, offline and reset-free
data in the real world by exploiting a self-supervised visuo-lingual affordance
model, which requires annotating as little as 1% of the total data with
language. We evaluate our method in extensive experiments both in simulated and
real-world robotic tasks, achieving state-of-the-art performance on the
challenging CALVIN benchmark and learning over 25 distinct visuomotor
manipulation tasks with a single policy in the real world. We find that when
paired with LLMs to break down abstract natural language instructions into
subgoals via few-shot prompting, our method is capable of completing
long-horizon, multi-tier tasks in the real world, while requiring an order of
magnitude less data than previous approaches. Code and videos are available at
http://hulc2.cs.uni-freiburg.de
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE International Conference on Robotics and
  Automation (ICRA). Project website: http://hulc2.cs.uni-freiburg.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Language Maps for Robot Navigation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding language to the visual observations of a navigating agent can be
performed using off-the-shelf visual-language models pretrained on
Internet-scale data (e.g., image captions). While this is useful for matching
images to natural language descriptions of object goals, it remains disjoint
from the process of mapping the environment, so that it lacks the spatial
precision of classic geometric maps. To address this problem, we propose
VLMaps, a spatial map representation that directly fuses pretrained
visual-language features with a 3D reconstruction of the physical world. VLMaps
can be autonomously built from video feed on robots using standard exploration
approaches and enables natural language indexing of the map without additional
labeled data. Specifically, when combined with large language models (LLMs),
VLMaps can be used to (i) translate natural language commands into a sequence
of open-vocabulary navigation goals (which, beyond prior work, can be spatial
by construction, e.g., "in between the sofa and TV" or "three meters to the
right of the chair") directly localized in the map, and (ii) can be shared
among multiple robots with different embodiments to generate new obstacle maps
on-the-fly (by using a list of obstacle categories). Extensive experiments
carried out in simulated and real world environments show that VLMaps enable
navigation according to more complex language instructions than existing
methods. Videos are available at https://vlmaps.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE International Conference on Robotics and
  Automation (ICRA). Project page: https://vlmaps.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">ChatGPT</span>: Beginning of an End of Manual Linguistic Data Annotation? Use
  Case of Automatic Genre Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taja Kuzman, Igor Mozetič, Nikola Ljubešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has shown strong capabilities in natural language generation tasks,
which naturally leads researchers to explore where its abilities end. In this
paper, we examine whether ChatGPT can be used for zero-shot text
classification, more specifically, automatic genre identification. We compare
ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on
datasets, manually annotated with genres. The models are compared on test sets
in two languages: English and Slovenian. Results show that ChatGPT outperforms
the fine-tuned model when applied to the dataset which was not seen before by
either of the models. Even when applied on Slovenian language as an
under-resourced language, ChatGPT's performance is no worse than when applied
to English. However, if the model is fully prompted in Slovenian, the
performance drops significantly, showing the current limitations of ChatGPT
usage on smaller languages. The presented results lead us to questioning
whether this is the beginning of an end of laborious manual annotation
campaigns even for smaller languages, such as Slovenian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Describe me an Aucklet: Generating Grounded Perceptual Category
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bill Noble, Nikolai Ilinykh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language users can generate descriptions of perceptual concepts beyond
instance-level representations and also use such descriptions to learn
provisional class-level representations. However, the ability of computational
models to learn and operate with class representations is under-investigated in
the language-and-vision field. In this paper, we train separate neural networks
to generate and interpret class-level descriptions. We then use the zero-shot
classification performance of the interpretation model as a measure of
communicative success and class-level conceptual grounding. We investigate the
performance of prototype- and exemplar-based neural representations grounded
category description. Finally, we show that communicative success reveals
performance issues in the generation model that are not captured by traditional
intrinsic NLG evaluation metrics, and argue that these issues can be traced to
a failure to properly ground language in vision at the class level. We observe
that the interpretation model performs better with descriptions that are low in
diversity on the class level, possibly indicating a strong reliance on
frequently occurring features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Larger language models do in-context learning differently 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how in-context learning (ICL) in language models is affected by
semantic priors versus input-label mappings. We investigate two setups-ICL with
flipped labels and ICL with semantically-unrelated labels-across various model
families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments
on ICL with flipped labels show that overriding semantic priors is an emergent
ability of model scale. While small language models ignore flipped labels
presented in-context and thus rely primarily on semantic priors from
pretraining, large models can override semantic priors when presented with
in-context exemplars that contradict priors, despite the stronger semantic
priors that larger models may hold. We next study semantically-unrelated label
ICL (SUL-ICL), in which labels are semantically unrelated to their inputs
(e.g., foo/bar instead of negative/positive), thereby forcing language models
to learn the input-label mappings shown in in-context exemplars in order to
perform the task. The ability to do SUL-ICL also emerges primarily with scale,
and large-enough language models can even perform linear classification in a
SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that
instruction tuning strengthens both the use of semantic priors and the capacity
to learn input-label mappings, but more of the former.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoP: Text-Video Co-operative <span class="highlight-title">Prompt</span> Tuning for Cross-Modal Retrieval <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent studies leverage the pre-trained CLIP for text-video cross-modal
retrieval by tuning the backbone with additional heavy modules, which not only
brings huge computational burdens with much more parameters, but also leads to
the knowledge forgetting from upstream models.In this work, we propose the VoP:
Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video
retrieval task. The proposed VoP is an end-to-end framework with both video &
text prompts introducing, which can be regarded as a powerful baseline with
only 0.1% trainable parameters. Further, based on the spatio-temporal
characteristics of videos, we develop three novel video prompt mechanisms to
improve the performance with different scales of trainable parameters. The
basic idea of the VoP enhancement is to model the frame position, frame
context, and layer function with specific trainable prompts, respectively.
Extensive experiments show that compared to full fine-tuning, the enhanced VoP
achieves a 1.4% average R@1 gain across five text-video retrieval benchmarks
with 6x less parameter overhead. The code will be available at
https://github.com/bighuang624/VoP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SumREN: Summarizing Reported Speech about Events in News <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Revanth Gangi Reddy, Heba Elfardy, Hou Pong Chan, Kevin Small, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A primary objective of news articles is to establish the factual record for
an event, frequently achieved by conveying both the details of the specified
event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and
how people reacted to it (i.e., reported statements). However, existing work on
news summarization almost exclusively focuses on the event details. In this
work, we propose the novel task of summarizing the reactions of different
speakers, as expressed by their reported statements, to a given event. To this
end, we create a new multi-document summarization benchmark, SUMREN, comprising
745 summaries of reported statements from various public figures obtained from
633 news articles discussing 132 events. We propose an automatic silver
training data generation approach for our task, which helps smaller models like
BART achieve GPT-3 level performance on this task. Finally, we introduce a
pipeline-based framework for summarizing reported speech, which we empirically
show to generate summaries that are more abstractive and factual than baseline
query-focused summarization approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal resources for quantum computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        D. -S. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unravelling the source of quantum computing power has been a major goal in
the field of quantum information science. In recent years, the quantum resource
theory (QRT) has been established to characterize various quantum resources,
yet their roles in quantum computing tasks still require investigation. The
so-called universal quantum computing model (UQCM), e.g., the circuit model,
has been the main framework to guide the design of quantum algorithms, creation
of real quantum computers etc. In this work, we combine the study of UQCM
together with QRT. We find on one hand, using QRT can provide a
resource-theoretic characterization of a UQCM, the relation among models and
inspire new ones, and on the other hand, using UQCM offers a framework to apply
resources, study relation among resources and classify them.
  We develop the theory of universal resources in the setting of UQCM, and find
a rich spectrum of UQCMs and the corresponding universal resources. Depending
on a hierarchical structure of resource theories, we find models can be
classified into families. In this work, we study three natural families of
UQCMs in details: the amplitude family, the quasi-probability family, and the
Hamiltonian family. They include some well known models, like the
measurement-based model and adiabatic model, and also inspire new models such
as the contextual model we introduce. Each family contains at least a triplet
of models, and such a succinct structure of families of UQCMs offers a unifying
picture to investigate resources and design models. It also provides a rigorous
framework to resolve puzzles, such as the role of entanglement vs.
interference, and unravel resource-theoretic features of quantum algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask-guided <span class="highlight-title">BERT</span> for Few Shot Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiong Liao, Zhengliang Liu, Haixing Dai, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Yuzhong Chen, Xi Jiang, Wei Liu, Dajiang Zhu, Tianming Liu, Sheng Li, Xiang Li, Hongmin Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models have achieved significant success in
various domains. However, the data-intensive nature of the transformer
architecture requires much labeled data, which is challenging in low-resource
scenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the
difficulty of training robust models on small amounts of samples, which
frequently leads to overfitting. Here we present Mask-BERT, a simple and
modular framework to help BERT-based architectures tackle FSL. The proposed
approach fundamentally differs from existing FSL strategies such as prompt
tuning and meta-learning. The core idea is to selectively apply masks on text
inputs and filter out irrelevant information, which guides the model to focus
on discriminative tokens that influence prediction results. In addition, to
make the text representations from different categories more separable and the
text representations from the same category more compact, we introduce a
contrastive learning loss function. Experimental results on public-domain
benchmark datasets demonstrate the effectiveness of Mask-BERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Check Your Facts and Try Again: Improving Large Language Models with
  External <span class="highlight-title">Knowledge</span> and Automated Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as ChatGPT, are able to generate
human-like, fluent responses for many downstream tasks, e.g., task-oriented
dialog and question answering. However, applying LLMs to real-world,
mission-critical applications remains challenging mainly due to their tendency
to generate hallucinations and their inability to use external knowledge. This
paper proposes a LLM-Augmenter system, which augments a black-box LLM with a
set of plug-and-play modules. Our system makes the LLM generate responses
grounded in external knowledge, e.g., stored in task-specific databases. It
also iteratively revises LLM prompts to improve model responses using feedback
generated by utility functions, e.g., the factuality score of a LLM-generated
response. The effectiveness of LLM-Augmenter is empirically validated on two
types of scenarios, task-oriented dialog and open-domain question answering.
LLM-Augmenter significantly reduces ChatGPT's hallucinations without
sacrificing the fluency and informativeness of its responses. We make the
source code and models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ kogito: A Commonsense <span class="highlight-title">Knowledge</span> Inference Toolkit <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present kogito, an open-source tool for generating
commonsense inferences about situations described in text. kogito provides an
intuitive and extensible interface to interact with natural language generation
models that can be used for hypothesizing commonsense knowledge inference from
a textual input. In particular, kogito offers several features for targeted,
multi-granularity knowledge generation. These include a standardized API for
training and evaluating knowledge models, and generating and filtering
inferences from them. We also include helper functions for converting natural
language texts into a format ingestible by knowledge models - intermediate
pipeline stages such as knowledge head extraction from text, heuristic and
model-based knowledge head-relation matching, and an ability to define and use
custom knowledge relations. We make the code for kogito available at
https://github.com/epfl-nlp/kogito along with thorough documentation at
https://kogito.readthedocs.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 Camera ready, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A data science and machine learning approach to continuous analysis of
  Shakespeare's plays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Swisher, Lior Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of quantitative methods that can analyze text has provided
new ways of examining literature in a manner that was not available in the
pre-information era. Here we apply comprehensive machine learning analysis to
the work of William Shakespeare. The analysis shows clear change in style of
writing over time, with the most significant changes in the sentence length,
frequency of adjectives and adverbs, and the sentiments expressed in the text.
Applying machine learning to make a stylometric prediction of the year of the
play shows a Pearson correlation of 0.71 between the actual and predicted year,
indicating that Shakespeare's writing style as reflected by the quantitative
measurements changed over time. Additionally, it shows that the stylometrics of
some of the plays is more similar to plays written either before or after the
year they were written. For instance, Romeo and Juliet is dated 1596, but is
more similar in stylometrics to plays written by Shakespeare after 1600. The
source code for the analysis is available for free download.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of Data Mining and Digital Humanities, accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fillers in Spoken Language Understanding: Computational and
  Psycholinguistic Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanvi Dinkar, Chloé Clavel, Ioana Vasilescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disfluencies (i.e. interruptions in the regular flow of speech), are
ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that
occur the most frequently compared to other kinds of disfluencies. Yet, to the
best of our knowledge, there isn't a resource that brings together the research
perspectives influencing Spoken Language Understanding (SLU) on these speech
events. This aim of this article is to survey a breadth of perspectives in a
holistic way; i.e. from considering underlying (psycho)linguistic theory, to
their annotation and consideration in Automatic Speech Recognition (ASR) and
SLU systems, to lastly, their study from a generation standpoint. This article
aims to present the perspectives in an approachable way to the SLU and
Conversational AI community, and discuss moving forward, what we believe are
the trends and challenges in each area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in TAL Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Context Pattern <span class="highlight-title">Generation</span> for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Ruiyang Liu, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various Natural
Language Processing (NLP) and Information Retrieval (IR) downstream
applications have benefited from ESE due to its ability to discover knowledge.
Although existing corpus-based ESE methods have achieved great progress, they
still rely on corpora with high-quality entity information annotated, because
most of them need to obtain the context patterns through the position of the
entity in a sentence. Therefore, the quality of the given corpora and their
entity annotation has become the bottleneck that limits the performance of such
methods. To overcome this dilemma and make the ESE models free from the
dependence on entity annotation, our work aims to explore a new ESE paradigm,
namely corpus-independent ESE. Specifically, we devise a context pattern
generation module that utilizes autoregressive language models (e.g., GPT-2) to
automatically generate high-quality context patterns for entities. In addition,
we propose the GAPA, a novel ESE framework that leverages the aforementioned
GenerAted PAtterns to expand target entities. Extensive experiments and
detailed analyses on three widely used datasets demonstrate the effectiveness
of our method. All the codes of our experiments are available at
https://github.com/geekjuruo/GAPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastFill: Efficient Compatible Model Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Jaeckle, Fartash Faghri, Ali Farhadi, Oncel Tuzel, Hadi Pouransari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many retrieval systems the original high dimensional data (e.g., images)
is mapped to a lower dimensional feature through a learned embedding model. The
task of retrieving the most similar data from a gallery set to a given query
data is performed through a similarity comparison on features. When the
embedding model is updated, it might produce features that are not
comparable/compatible with features already in the gallery computed with the
old model. Subsequently, all features in the gallery need to be re-computed
using the new embedding model -- a computationally expensive process called
backfilling. Recently, compatible representation learning methods have been
proposed to avoid backfilling. Despite their relative success, there is an
inherent trade-off between the new model performance and its compatibility with
the old model. In this work, we introduce FastFill: a compatible model update
process using feature alignment and policy based partial backfilling to
promptly elevate retrieval performance. We show that previous backfilling
strategies suffer from decreased performance and demonstrate the importance of
both the training objective and the ordering in online partial backfilling. We
propose a new training method for feature alignment between old and new
embedding models using uncertainty estimation. Compared to previous works, we
obtain significantly improved backfilling results on a variety of datasets: mAP
on ImageNet (+4.4\%), Places-365 (+2.7\%), and VGG-Face2 (+1.3\%). Further, we
demonstrate that when updating a biased model with FastFill, the minority
subgroup accuracy gap promptly vanishes with a small fraction of partial
backfilling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in The Eleventh International Conference on Learning
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span> Log Analysis of Text-to-Image <span class="highlight-title">Generation</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xie, Zhaoying Pan, Jinge Ma, Jie Luo, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in diffusion models have unleashed the astonishing
capabilities of text-to-image generation systems to synthesize high-quality
images that are faithful to a given reference text, known as a "prompt." These
systems, once released to the public, have immediately received tons of
attention from researchers, creators, and common users. Despite the plenty of
efforts to improve the underneath generative models, there is limited work on
understanding the information needs of the real users of these systems, e.g.,
by investigating the prompts the users input at scale. In this paper, we take
the initiative to conduct a comprehensive analysis of large-scale prompt logs
collected from multiple text-to-image generation systems. Our work is analogous
to analyzing the query log of Web search engines, a line of work that has made
critical contributions to the glory of the Web search industry and research. We
analyze over two million user-input prompts submitted to three popular
text-to-image systems at scale. Compared to Web search queries, text-to-image
prompts are significantly longer, often organized into unique structures, and
present different categories of information needs. Users tend to make more
edits within creation sessions, showing remarkable exploratory patterns. Our
findings provide concrete implications on how to improve text-to-image
generation systems for creation purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel-CF: Collaborative filtering done right with social network
  analysis and kernel smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is the simplest but oldest machine learning algorithm
in the field of recommender systems. In spite of its long history, it remains a
discussion topic in research venues. Usually people use users/items whose
similarity scores with the target customer greater than 0 to compute the
algorithms. However, this might not be the optimal solution after careful
scrutiny. In this paper, we transform the recommender system input data into a
2-D social network, and apply kernel smoothing to compute preferences for
unknown values in the user item rating matrix. We unifies the theoretical
framework of recommender system and non-parametric statistics and provides an
algorithmic procedure with optimal parameter selection method to achieve the
goal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class Cardinality Comparison as a Fermi Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrestha Ghosh, Simon Razniewski, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questions on class cardinality comparisons are quite tricky to answer and
come with its own challenges. They require some kind of reasoning since web
documents and knowledge bases, indispensable sources of information, rarely
store direct answers to questions, such as, ``Are there more astronauts or
Physics Nobel Laureates?'' We tackle questions on class cardinality comparison
by tapping into three sources for absolute cardinalities as well as the
cardinalities of orthogonal subgroups of the classes. We propose novel
techniques for aggregating signals with partial coverage for more reliable
estimates and evaluate them on a dataset of 4005 class pairs, achieving an
accuracy of 83.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL-entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL-entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achievable Rates and Low-Complexity Encoding of Posterior Matching for
  the BSC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaael Antonini, Rita Gimeshein, Richard Wesel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Horstein, Burnashev, Shayevitz and Feder, Naghshvar et al. and others have
studied sequential transmission of a K-bit message over the binary symmetric
channel (BSC) with full, noiseless feedback using posterior matching. Yang et
al. provide an improved lower bound on the achievable rate using martingale
analysis that relies on the small-enough difference (SED) partitioning
introduced by Naghshvar et al. SED requires a relatively complex encoder and
decoder. To reduce complexity, this paper replaces SED with relaxed constraints
that admit the small enough absolute difference (SEAD) partitioning rule. The
main analytical results show that achievable-rate bounds higher than those
found by Yang et al. are possible even under the new constraints, which are
less restrictive than SED. The new analysis does not use martingale theory for
the confirmation phase and applies a surrogate channel technique to tighten the
results. An initial systematic transmission further increases the achievable
rate bound. The simplified encoder associated with SEAD has a complexity below
order O(K^2) and allows simulations for message sizes of at least 1000 bits.
For example, simulations achieve 99% of of the channel's 0.50-bit capacity with
an average block size of 200 bits for a target codeword error rate of 10^(-3).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper consists of 26 pages and contains 6 figures. An earlier
  version of the algorithm included in this paper was published at the 2020
  IEEE International Symposium on Information Theory (ISIT), (DOI:
  10.1109/ISIT44484.2020.9174232)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Learning to Rank with Biased Continuous Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Hongyan Tang, Siwen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a well-known challenge to learn an unbiased ranker with biased
feedback. Unbiased learning-to-rank(LTR) algorithms, which are verified to
model the relative relevance accurately based on noisy feedback, are appealing
candidates and have already been applied in many applications with single
categorical labels, such as user click signals. Nevertheless, the existing
unbiased LTR methods cannot properly handle continuous feedback, which are
essential for many industrial applications, such as content recommender
systems.
  To provide personalized high-quality recommendation results, recommender
systems need model both categorical and continuous biased feedback, such as
click and dwell time. Accordingly, we design a novel unbiased LTR algorithm to
tackle the challenges, which innovatively models position bias in the pairwise
fashion and introduces the pairwise trust bias to separate the position bias,
trust bias, and user relevance explicitly and can work for both continuous and
categorical feedback. Experiment results on public benchmark datasets and
internal live traffic of a large-scale recommender system at Tencent News show
superior results for continuous labels and also competitive performance for
categorical labels of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. arXiv admin note: substantial text overlap with
  arXiv:2111.12929</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLCC: A General Framework for Graph-Level Clustering <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11879v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11879v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ju, Yiyang Gu, Binqi Chen, Gongbo Sun, Yifang Qin, Xingyuming Liu, Xiao Luo, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of graph-level clustering, which is a novel
yet challenging task. This problem is critical in a variety of real-world
applications such as protein clustering and genome analysis in bioinformatics.
Recent years have witnessed the success of deep clustering coupled with graph
neural networks (GNNs). However, existing methods focus on clustering among
nodes given a single graph, while exploring clustering on multiple graphs is
still under-explored. In this paper, we propose a general graph-level
clustering framework named Graph-Level Contrastive Clustering (GLCC) given
multiple graphs. Specifically, GLCC first constructs an adaptive affinity graph
to explore instance- and cluster-level contrastive learning (CL).
Instance-level CL leverages graph Laplacian based contrastive loss to learn
clustering-friendly representations while cluster-level CL captures
discriminative cluster representations incorporating neighbor information of
each sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the
optimization of representation learning. The two steps can be alternatively
trained to collaborate and benefit each other. Experiments on a range of
well-known datasets demonstrate the superiority of our proposed GLCC over
competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RETEXO: Scalable Neural Network Training over Distributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashish Kolluri, Sarthak Choudhary, Bryan Hooi, Prateek Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks offer a promising approach to supervised learning over
graph data. Graph data, especially when it is privacy-sensitive or too large to
train on centrally, is often stored partitioned across disparate processing
units (clients) which want to minimize the communication costs during
collaborative training. The fully-distributed setup takes such partitioning to
its extreme, wherein features of only a single node and its adjacent edges are
kept locally with one client processor. Existing GNNs are not architected for
training in such setups and incur prohibitive costs therein. We propose RETEXO,
a novel transformation of existing GNNs that improves the communication
efficiency during training in the fully-distributed setup. We experimentally
confirm that RETEXO offers up to 6 orders of magnitude better communication
efficiency even when training shallow GNNs, with a minimal trade-off in
accuracy for supervised node classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Line Graph Contrastive Learning for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Zhang, Shilin Sun, Guixiang Ma, Caiming Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction tasks focus on predicting possible future connections. Most
existing researches measure the likelihood of links by different similarity
scores on node pairs and predict links between nodes. However, the
similarity-based approaches have some challenges in information loss on nodes
and generalization ability on similarity indexes. To address the above issues,
we propose a Line Graph Contrastive Learning(LGCL) method to obtain rich
information with multiple perspectives. LGCL obtains a subgraph view by h-hop
subgraph sampling with target node pairs. After transforming the sampled
subgraph into a line graph, the link prediction task is converted into a node
classification task, which graph convolution progress can learn edge embeddings
from graphs more effectively. Then we design a novel cross-scale contrastive
learning framework on the line graph and the subgraph to maximize the mutual
information of them, so that fuses the structure and feature information. The
experimental results demonstrate that the proposed LGCL outperforms the
state-of-the-art methods and has better performance on generalization and
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Context Pattern <span class="highlight-title">Generation</span> for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Ruiyang Liu, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various Natural
Language Processing (NLP) and Information Retrieval (IR) downstream
applications have benefited from ESE due to its ability to discover knowledge.
Although existing corpus-based ESE methods have achieved great progress, they
still rely on corpora with high-quality entity information annotated, because
most of them need to obtain the context patterns through the position of the
entity in a sentence. Therefore, the quality of the given corpora and their
entity annotation has become the bottleneck that limits the performance of such
methods. To overcome this dilemma and make the ESE models free from the
dependence on entity annotation, our work aims to explore a new ESE paradigm,
namely corpus-independent ESE. Specifically, we devise a context pattern
generation module that utilizes autoregressive language models (e.g., GPT-2) to
automatically generate high-quality context patterns for entities. In addition,
we propose the GAPA, a novel ESE framework that leverages the aforementioned
GenerAted PAtterns to expand target entities. Extensive experiments and
detailed analyses on three widely used datasets demonstrate the effectiveness
of our method. All the codes of our experiments are available at
https://github.com/geekjuruo/GAPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Debiased Learning from Positive, Unlabeled, and Exposure Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato, Shuting Wu, Kodai Kureishi, Shota Yasui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the issue of binary classification from positive and unlabeled
data (PU classification) with a selection bias in the positive data. During the
observation process, (i) a sample is exposed to a user, (ii) the user then
returns the label for the exposed sample, and (iii) we however can only observe
the positive samples. Therefore, the positive labels that we observe are a
combination of both the exposure and the labeling, which creates a selection
bias problem for the observed positive samples. This scenario represents a
conceptual framework for many practical applications, such as recommender
systems, which we refer to as ``learning from positive, unlabeled, and exposure
data'' (PUE classification). To tackle this problem, we initially assume access
to data with exposure labels. Then, we propose a method to identify the
function of interest using a strong ignorability assumption and develop an
``Automatic Debiased PUE'' (ADPUE) learning method. This algorithm directly
debiases the selection bias without requiring intermediate estimates, such as
the propensity score, which is necessary for other learning methods. Through
experiments, we demonstrate that our approach outperforms traditional PU
learning methods on various semi-synthetic datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ewald-based Long-Range Message Passing for Molecular Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Kosmala, Johannes Gasteiger, Nicholas Gao, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architectures that learn potential energy surfaces from molecular data
have undergone fast improvement in recent years. A key driver of this success
is the Message Passing Neural Network (MPNN) paradigm. Its favorable scaling
with system size partly relies upon a spatial distance limit on messages. While
this focus on locality is a useful inductive bias, it also impedes the learning
of long-range interactions such as electrostatics and van der Waals forces. To
address this drawback, we propose Ewald message passing: a nonlocal Fourier
space scheme which limits interactions via a cutoff on frequency instead of
distance, and is theoretically well-founded in the Ewald summation method. It
can serve as an augmentation on top of existing MPNN architectures as it is
computationally cheap and agnostic to other architectural details. We test the
approach with four baseline models and two datasets containing diverse periodic
(OC20) and aperiodic structures (OE62). We observe robust improvements in
energy mean absolute errors across all models and datasets, averaging 10% on
OC20 and 16% on OE62. Our analysis shows an outsize impact of these
improvements on structures with high long-range contributions to the ground
truth energy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Non-Linear Quantum Operations through Variational Quantum
  Splines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Antonio Inajetovic, Filippo Orazi, Antonio Macaluso, Stefano Lodi, Claudio Sartori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The postulates of quantum mechanics impose only unitary transformations on
quantum states, which is a severe limitation for quantum machine learning
algorithms. Quantum Splines (QSplines) have recently been proposed to
approximate quantum activation functions to introduce non-linearity in quantum
algorithms. However, QSplines make use of the HHL as a subroutine and require a
fault-tolerant quantum computer to be correctly implemented. This work proposes
the Generalised QSplines (GQSplines), a novel method for approximating
non-linear quantum activation functions using hybrid quantum-classical
computation. The GQSplines overcome the highly demanding requirements of the
original QSplines in terms of quantum hardware and can be implemented using
near-term quantum computers. Furthermore, the proposed method relies on a
flexible problem representation for non-linear approximation and it is suitable
to be embedded in existing quantum neural network architectures. In addition,
we provide a practical implementation of GQSplines using Pennylane and show
that our model outperforms the original QSplines in terms of quality of
fitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier-MIONet: Fourier-enhanced multiple-input neural operators for
  multiphase modeling of geological carbon sequestration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyi Jiang, Min Zhu, Dongzhuo Li, Qiuzi Li, Yanhua O. Yuan, Lu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geologic Carbon Storage (GCS) is an important technology that aims to reduce
the amount of carbon dioxide in the atmosphere. Multiphase flow in porous media
is essential to understand CO2 migration and pressure fields in the subsurface
associated with GCS. However, numerical simulation for such problems in 4D is
computationally challenging and expensive, due to the multiphysics and
multiscale nature of the highly nonlinear governing partial differential
equations (PDEs). It prevents us from considering multiple subsurface scenarios
and conducting real-time optimization. Here, we develop a Fourier-enhanced
multiple-input neural operator (Fourier-MIONet) to learn the solution operator
of the problem of multiphase flow in porous media. Fourier-MIONet utilizes the
recently developed framework of the multiple-input deep neural operators
(MIONet) and incorporates the Fourier neural operator (FNO) in the network
architecture. Once Fourier-MIONet is trained, it can predict the evolution of
saturation and pressure of the multiphase flow under various reservoir
conditions, such as permeability and porosity heterogeneity, anisotropy,
injection configurations, and multiphase flow properties. Compared to the
enhanced FNO (U-FNO), the proposed Fourier-MIONet has 90% fewer unknown
parameters, and it can be trained in significantly less time (about 3.5 times
faster) with much lower CPU memory (< 15%) and GPU memory (< 35%) requirements,
to achieve similar prediction accuracy. In addition to the lower computational
cost, Fourier-MIONet can be trained with only 6 snapshots of time to predict
the PDE solutions for 30 years. The excellent generalizability of
Fourier-MIONet is enabled by its adherence to the physical principle that the
solution to a PDE is continuous over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMI-based Data-Driven Robust Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang Hai Nguyen, Maurice Friedel, Rolf Findeisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive control, which is based on a model of the system to compute the
applied input optimizing the future system behavior, is by now widely used. If
the nominal models are not given or are very uncertain, data-driven model
predictive control approaches can be employed, where the system model or input
is directly obtained from past measured trajectories. Using a data
informativity framework and Finsler's lemma, we propose a data-driven robust
linear matrix inequality-based model predictive control scheme that considers
input and state constraints. Using these data, we formulate the problem as a
semi-definite optimization problem, whose solution provides the matrix gain for
the linear feedback, while the decisive variables are independent of the length
of the measurement data. The designed controller stabilizes the closed-loop
system asymptotically and guarantees constraint satisfaction. Numerical
examples are conducted to illustrate the method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models
  for Image <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Hagemann, Lars Ruthotto, Gabriele Steidl, Nicole Tianjiao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based diffusion models (SBDM) have recently emerged as state-of-the-art
approaches for image generation. Existing SBDMs are typically formulated in a
finite-dimensional setting, where images are considered as tensors of a finite
size. This papers develops SBDMs in the infinite-dimensional setting, that is,
we model the training data as functions supported on a rectangular domain.
Besides the quest for generating images at ever higher resolution our primary
motivation is to create a well-posed infinite-dimensional learning problem so
that we can discretize it consistently on multiple resolution levels. We
thereby hope to obtain diffusion models that generalize across different
resolution levels and improve the efficiency of the training process. We
demonstrate how to overcome two shortcomings of current SBDM approaches in the
infinite-dimensional setting. First, we modify the forward process to ensure
that the latent distribution is well-defined in the infinite-dimensional
setting using the notion of trace class operators. Second, we illustrate that
approximating the score function with an operator network, in our case Fourier
neural operators (FNOs), is beneficial for multilevel training. After deriving
the forward and reverse process in the infinite-dimensional setting, we show
their well-posedness, derive adequate discretizations, and investigate the role
of the latent distributions. We provide first promising numerical results on
two datasets, MNIST and material structures. In particular, we show that
multilevel training is feasible within this framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastFill: Efficient Compatible Model Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Jaeckle, Fartash Faghri, Ali Farhadi, Oncel Tuzel, Hadi Pouransari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many retrieval systems the original high dimensional data (e.g., images)
is mapped to a lower dimensional feature through a learned embedding model. The
task of retrieving the most similar data from a gallery set to a given query
data is performed through a similarity comparison on features. When the
embedding model is updated, it might produce features that are not
comparable/compatible with features already in the gallery computed with the
old model. Subsequently, all features in the gallery need to be re-computed
using the new embedding model -- a computationally expensive process called
backfilling. Recently, compatible representation learning methods have been
proposed to avoid backfilling. Despite their relative success, there is an
inherent trade-off between the new model performance and its compatibility with
the old model. In this work, we introduce FastFill: a compatible model update
process using feature alignment and policy based partial backfilling to
promptly elevate retrieval performance. We show that previous backfilling
strategies suffer from decreased performance and demonstrate the importance of
both the training objective and the ordering in online partial backfilling. We
propose a new training method for feature alignment between old and new
embedding models using uncertainty estimation. Compared to previous works, we
obtain significantly improved backfilling results on a variety of datasets: mAP
on ImageNet (+4.4\%), Places-365 (+2.7\%), and VGG-Face2 (+1.3\%). Further, we
demonstrate that when updating a biased model with FastFill, the minority
subgroup accuracy gap promptly vanishes with a small fraction of partial
backfilling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in The Eleventh International Conference on Learning
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAF: Holistic Compilation for Deep Learning Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cody Hao Yu, Haozheng Fan, Guangtai Huang, Zhen Jia, Yizhi Liu, Jie Wang, Zach Zheng, Yuan Zhou, Haichen Shen, Junru Shao, Mu Li, Yida Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning is pervasive in modern applications, many deep learning
frameworks are presented for deep learning practitioners to develop and train
DNN models rapidly. Meanwhile, as training large deep learning models becomes a
trend in recent years, the training throughput and memory footprint are getting
crucial. Accordingly, optimizing training workloads with compiler optimizations
is inevitable and getting more and more attentions. However, existing deep
learning compilers (DLCs) mainly target inference and do not incorporate
holistic optimizations, such as automatic differentiation and automatic mixed
precision, in training workloads.
  In this paper, we present RAF, a deep learning compiler for training. Unlike
existing DLCs, RAF accepts a forward model and in-house generates a training
graph. Accordingly, RAF is able to systematically consolidate graph
optimizations for performance, memory and distributed training. In addition, to
catch up to the state-of-the-art performance with hand-crafted kernel libraries
as well as tensor compilers, RAF proposes an operator dialect mechanism to
seamlessly integrate all possible kernel implementations. We demonstrate that
by in-house training graph generation and operator dialect mechanism, we are
able to perform holistic optimizations and achieve either better training
throughput or larger batch size against PyTorch (eager and torchscript mode),
XLA, and DeepSpeed for popular transformer models on GPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-learning Control Variates: Variance Reduction with Limited Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Sun, Chris J. Oates, François-Xavier Briol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control variates can be a powerful tool to reduce the variance of Monte Carlo
estimators, but constructing effective control variates can be challenging when
the number of samples is small. In this paper, we show that when a large number
of related integrals need to be computed, it is possible to leverage the
similarity between these integration tasks to improve performance even when the
number of samples per task is very small. Our approach, called meta learning
CVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our
empirical assessment indicates that Meta-CVs can lead to significant variance
reduction in such settings, and our theoretical analysis establishes general
conditions under which Meta-CVs can be successfully trained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Theory of Correct, Incorrect, and Extrinsic Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dian Wang, Xupeng Zhu, Jung Yeon Park, Robert Platt, Robin Walters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although equivariant machine learning has proven effective at many tasks,
success depends heavily on the assumption that the ground truth function is
symmetric over the entire domain matching the symmetry in an equivariant neural
network. A missing piece in the equivariant learning literature is the analysis
of equivariant networks when symmetry exists only partially in the domain. In
this work, we present a general theory for such a situation. We propose
pointwise definitions of correct, incorrect, and extrinsic equivariance, which
allow us to quantify continuously the degree of each type of equivariance a
function displays. We then study the impact of various degrees of incorrect or
extrinsic symmetry on model error. We prove error lower bounds for invariant or
equivariant networks in classification or regression settings with partially
incorrect symmetry. We also analyze the potentially harmful effects of
extrinsic equivariance. Experiments validate these results in three different
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vector Quantized Time Series <span class="highlight-title">Generation</span> with a Bidirectional Prior Model <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daesoo Lee, Sara Malacarne, Erlend Aune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series generation (TSG) studies have mainly focused on the use of
Generative Adversarial Networks (GANs) combined with recurrent neural network
(RNN) variants. However, the fundamental limitations and challenges of training
GANs still remain. In addition, the RNN-family typically has difficulties with
temporal consistency between distant timesteps. Motivated by the successes in
the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our
knowledge, that uses vector quantization (VQ) techniques to address the TSG
problem. Moreover, the priors of the discrete latent spaces are learned with
bidirectional transformer models that can better capture global temporal
consistency. We also propose VQ modeling in a time-frequency domain, separated
into low-frequency (LF) and high-frequency (HF). This allows us to retain
important characteristics of the time series and, in turn, generate new
synthetic signals that are of better quality, with sharper changes in
modularity, than its competing TSG methods. Our experimental evaluation is
conducted on all datasets from the UCR archive, using well-established metrics
in the IMG literature, such as Fr\'echet inception distance and inception
scores. Our implementation on GitHub:
\url{https://github.com/ML4ITS/TimeVQVAE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Direct Convolution using Convolution Slicing Optimization and
  ISA Extensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Ferrari, Rafael Sousa, Marcio Pereira, João P. L. de Carvalho, José Nelson Amaral, José Moreira, Guido Araujo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolution is one of the most computationally intensive operations that must
be performed for machine-learning model inference. A traditional approach to
compute convolutions is known as the Im2Col + BLAS method. This paper proposes
SConv: a direct-convolution algorithm based on a MLIR/LLVM code-generation
toolchain that can be integrated into machine-learning compilers . This
algorithm introduces: (a) Convolution Slicing Analysis (CSA) - a
convolution-specific 3D cache-blocking analysis pass that focuses on tile reuse
over the cache hierarchy; (b) Convolution Slicing Optimization (CSO) - a
code-generation pass that uses CSA to generate a tiled direct-convolution
macro-kernel; and (c) Vector-Based Packing (VBP) - an architecture-specific
optimized input-tensor packing solution based on vector-register shift
instructions for convolutions with unitary stride. Experiments conducted on 393
convolutions from full ONNX-MLIR machine-learning models indicate that the
elimination of the Im2Col transformation and the use of fast packing routines
result in a total packing time reduction, on full model inference, of 2.0x -
3.9x on Intel x86 and 3.6x - 7.2x on IBM POWER10. The speed-up over an Im2Col +
BLAS method based on current BLAS implementations for end-to-end
machine-learning model inference is in the range of 9% - 25% for Intel x86 and
10% - 42% for IBM POWER10 architectures. The total convolution speedup for
model inference is 12% - 27% on Intel x86 and 26% - 46% on IBM POWER10. SConv
also outperforms BLAS GEMM, when computing pointwise convolutions, in more than
83% of the 219 tested instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Risks of Stealing the Decoding Algorithms of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Predictive Control with Gaussian-Process-Supported Dynamical
  Constraints for Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Bethge, Maik Pfefferkorn, Alexander Rose, Jan Peters, Rolf Findeisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a model predictive control approach for autonomous vehicles that
exploits learned Gaussian processes for predicting human driving behavior. The
proposed approach employs the uncertainty about the GP's prediction to achieve
safety. A multi-mode predictive control approach considers the possible
intentions of the human drivers. While the intentions are represented by
different Gaussian processes, their probabilities foreseen in the observed
behaviors are determined by a suitable online classification. Intentions below
a certain probability threshold are neglected to improve performance. The
proposed multi-mode model predictive control approach with Gaussian process
regression support enables repeated feasibility and probabilistic constraint
satisfaction with high probability. The approach is underlined in simulation,
considering real-world measurements for training the Gaussian processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast offset corrected in-memory training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte J. Rasch, Fabio Carta, Omebayode Fagbohungbe, Tayfun Gokmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-memory computing with resistive crossbar arrays has been suggested to
accelerate deep-learning workloads in highly efficient manner. To unleash the
full potential of in-memory computing, it is desirable to accelerate the
training as well as inference for large deep neural networks (DNNs). In the
past, specialized in-memory training algorithms have been proposed that not
only accelerate the forward and backward passes, but also establish tricks to
update the weight in-memory and in parallel. However, the state-of-the-art
algorithm (Tiki-Taka version 2 (TTv2)) still requires near perfect offset
correction and suffers from potential biases that might occur due to
programming and estimation inaccuracies, as well as longer-term instabilities
of the device materials. Here we propose and describe two new and improved
algorithms for in-memory computing (Chopped-TTv2 (c-TTv2) and Analog Gradient
Accumulation with Dynamic reference (AGAD)), that retain the same runtime
complexity but correct for any remaining offsets using choppers. These
algorithms greatly relax the device requirements and thus expanding the scope
of possible materials potentially employed for such fast in-memory DNN
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VOLTA: an Environment-Aware Contrastive Cell Representation Learning for
  Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramin Nakhli, Allen Zhang, Hossein Farahani, Amirali Darbandsari, Elahe Shenasa, Sidney Thiessen, Katy Milne, Jessica McAlpine, Brad Nelson, C Blake Gilks, Ali Bashashati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical practice, many diagnosis tasks rely on the identification of
cells in histopathology images. While supervised machine learning techniques
require labels, providing manual cell annotations is time-consuming due to the
large number of cells. In this paper, we propose a self-supervised framework
(VOLTA) for cell representation learning in histopathology images using a novel
technique that accounts for the cell's mutual relationship with its environment
for improved cell representations. We subjected our model to extensive
experiments on the data collected from multiple institutions around the world
comprising of over 700,000 cells, four cancer types, and cell types ranging
from three to six categories for each dataset. The results show that our model
outperforms the state-of-the-art models in cell representation learning. To
showcase the potential power of our proposed framework, we applied VOLTA to
ovarian and endometrial cancers with very small sample sizes (10-20 samples)
and demonstrated that our cell representations can be utilized to identify the
known histotypes of ovarian cancer and provide novel insights that link
histopathology and molecular subtypes of endometrial cancer. Unlike supervised
deep learning models that require large sample sizes for training, we provide a
framework that can empower new discoveries without any annotation data in
situations where sample sizes are limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A path in regression Random Forest looking for spatial dependence: a
  taxonomy and a systematic <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Patelli, Michela Cameletti, Natalia Golini, Rosaria Ignaccolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Forest (RF) is a well-known data-driven algorithm applied in several
fields thanks to its flexibility in modeling the relationship between the
response variable and the predictors, also in case of strong non-linearities.
In environmental applications, it often occurs that the phenomenon of interest
may present spatial and/or temporal dependence that is not taken explicitly
into account by RF in its standard version. In this work, we propose a taxonomy
to classify strategies according to when (Pre-, In- and/or Post-processing)
they try to include the spatial information into regression RF. Moreover, we
provide a systematic review and classify the most recent strategies adopted to
"adjust" regression RF to spatially dependent data, based on the criteria
provided by the Preferred Reporting Items for Systematic reviews and
Meta-Analysis (PRISMA). The latter consists of a reproducible methodology for
collecting and processing existing literature on a specified topic from
different sources. PRISMA starts with a query and ends with a set of scientific
documents to review: we performed an online query on the 25$^{th}$ October 2022
and, in the end, 32 documents were considered for review. The employed
methodological strategies and the application fields considered in the 32
scientific documents are described and discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Considerations on the Theory of Training Models with Differential
  Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marten van Dijk, Phuong Ha Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning collaborative learning takes place by a set of clients
who each want to remain in control of how their local training data is used, in
particular, how can each client's local training data remain private?
Differential privacy is one method to limit privacy leakage. We provide a
general overview of its framework and provable properties, adopt the more
recent hypothesis based definition called Gaussian DP or $f$-DP, and discuss
Differentially Private Stochastic Gradient Descent (DP-SGD). We stay at a meta
level and attempt intuitive explanations and insights \textit{in this book
chapter}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, a book chapter. arXiv admin note: text overlap with
  arXiv:2212.05796</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Effective Hyperparameter Optimization for Large Language Model
  <span class="highlight-title">Generation</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like GPT-3 have sparked significant interest in
their generative capabilities, leading to the development of various commercial
applications. The high cost of using the models drives application builders to
maximize the value of generation under a limited inference budget. This paper
presents a study of optimizing inference hyperparameters like the number of
responses, temperature and max tokens, which significantly affects the
utility/cost of text generation. We design a framework named EcoOptiGen which
leverages economical hyperparameter optimization and cost-based pruning.
Experiments with the latest GPT-3.5 models on a variety of tasks verify its
effectiveness. EcoOptiGen is implemented in the FLAML library:
https://github.com/microsoft/FLAML, and we provide one example of using it at:
https://microsoft.github.io/FLAML/docs/Examples/Integrate%20-%20OpenAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Probabilistic Logic Programming in Discrete-Continuous Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, Angelika Kimmig, Luc De Readt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic
background knowledge in the form of logic. It has been shown to aid learning in
the limited data regime and to facilitate inference on out-of-distribution
data. Probabilistic NeSy focuses on integrating neural networks with both logic
and probability theory, which additionally allows learning under uncertainty. A
major limitation of current probabilistic NeSy systems, such as DeepProbLog, is
their restriction to finite probability distributions, i.e., discrete random
variables. In contrast, deep probabilistic programming (DPP) excels in
modelling and optimising continuous probability distributions. Hence, we
introduce DeepSeaProbLog, a neural probabilistic logic programming language
that incorporates DPP techniques into NeSy. Doing so results in the support of
inference and learning of both discrete and continuous probability
distributions under logical constraints. Our main contributions are 1) the
semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a
proven asymptotically unbiased learning algorithm, and 3) a series of
experiments that illustrate the versatility of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoliang He, Zak Singh, Eiko Yoneki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rewrite systems [6, 10, 12] have been widely employing equality saturation
[9], which is an optimisation methodology that uses a saturated e-graph to
represent all possible sequences of rewrite simultaneously, and then extracts
the optimal one. As such, optimal results can be achieved by avoiding the
phase-ordering problem. However, we observe that when the e-graph is not
saturated, it cannot represent all possible rewrite opportunities and therefore
the phase-ordering problem is re-introduced during the construction phase of
the e-graph. To address this problem, we propose MCTS-GEB, a domain-general
rewrite system that applies reinforcement learning (RL) to e-graph
construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3]
to efficiently plan for the optimal e-graph construction, and therefore it can
effectively eliminate the phase-ordering problem at the construction phase and
achieve better performance within a reasonable time. Evaluation in two
different domains shows MCTS-GEB can outperform the state-of-the-art rewrite
systems by up to 49x, while the optimisation can generally take less than an
hour, indicating MCTS-GEB is a promising building block for the future
generation of rewrite systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting the movements of Bitcoin prices: an application of machine
  learning algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakan Pabuccu, Serdar Ongan, Ayse Ongan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrencies, such as Bitcoin, are one of the most controversial and
complex technological innovations in today's financial system. This study aims
to forecast the movements of Bitcoin prices at a high degree of accuracy. To
this aim, four different Machine Learning (ML) algorithms are applied, namely,
the Support Vector Machines (SVM), the Artificial Neural Network (ANN), the
Naive Bayes (NB) and the Random Forest (RF) besides the logistic regression
(LR) as a benchmark model. In order to test these algorithms, besides existing
continuous dataset, discrete dataset was also created and used. For the
evaluations of algorithm performances, the F statistic, accuracy statistic, the
Mean Absolute Error (MAE), the Root Mean Square Error (RMSE) and the Root
Absolute Error (RAE) metrics were used. The t test was used to compare the
performances of the SVM, ANN, NB and RF with the performance of the LR.
Empirical findings reveal that, while the RF has the highest forecasting
performance in the continuous dataset, the NB has the lowest. On the other
hand, while the ANN has the highest and the NB the lowest performance in the
discrete dataset. Furthermore, the discrete dataset improves the overall
forecasting performance in all algorithms (models) estimated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures and 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Multimodal Fusion for Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanju Xaviar, Xin Yang, Omid Ardakanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of IoT and mobile devices equipped with heterogeneous
sensors has enabled new applications that rely on the fusion of time-series
data generated by multiple sensors with different modalities. While there are
promising deep neural network architectures for multimodal fusion, their
performance falls apart quickly in the presence of consecutive missing data and
noise across multiple modalities/sensors, the issues that are prevalent in
real-world settings. We propose Centaur, a multimodal fusion model for human
activity recognition (HAR) that is robust to these data quality issues. Centaur
combines a data cleaning module, which is a denoising autoencoder with
convolutional layers, and a multimodal fusion module, which is a deep
convolutional neural network with the self-attention mechanism to capture
cross-sensor correlation. We train Centaur using a stochastic data corruption
scheme and evaluate it on three datasets that contain data generated by
multiple inertial measurement units. Centaur's data cleaning module outperforms
2 state-of-the-art autoencoder-based models and its multimodal fusion module
outperforms 4 strong baselines. Compared to 2 related robust fusion
architectures, Centaur is more robust, achieving 11.59-17.52% higher accuracy
in HAR, especially in the presence of consecutive missing data in multiple
sensor channels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusing Gaussian Mixtures for Generating Categorical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florence Regol, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a categorical distribution comes with its own set of challenges. A
successful approach taken by state-of-the-art works is to cast the problem in a
continuous domain to take advantage of the impressive performance of the
generative models for continuous data. Amongst them are the recently emerging
diffusion probabilistic models, which have the observed advantage of generating
high-quality samples. Recent advances for categorical generative models have
focused on log likelihood improvements. In this work, we propose a generative
model for categorical data based on diffusion models with a focus on
high-quality sample generation, and propose sampled-based evaluation methods.
The efficacy of our method stems from performing diffusion in the continuous
domain while having its parameterization informed by the structure of the
categorical nature of the target distribution. Our method of evaluation
highlights the capabilities and limitations of different generative models for
generating categorical data, and includes experiments on synthetic and
real-world protein datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contribution of clinical course to outcome after traumatic brain injury:
  mining patient trajectories from European intensive care unit data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhayu Bhattacharyay, Pier Francesco Caruso, Cecilia Åkerlund, Lindsay Wilson, Robert D Stevens, David K Menon, Ewout W Steyerberg, David W Nelson, Ari Ercole, the CENTER-TBI investigators/participants
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods to characterise the evolving condition of traumatic brain
injury (TBI) patients in the intensive care unit (ICU) do not capture the
context necessary for individualising treatment. We aimed to develop a
modelling strategy which integrates all data stored in medical records to
produce an interpretable disease course for each TBI patient's ICU stay. From a
prospective, European cohort (n=1,550, 65 centres, 19 countries) of TBI
patients, we extracted all 1,166 variables collected before or during ICU stay
as well as 6-month functional outcome on the Glasgow Outcome Scale-Extended
(GOSE). We trained recurrent neural network models to map a token-embedded time
series representation of all variables (including missing data) to an ordinal
GOSE prognosis every 2 hours. With repeated cross-validation, we evaluated
calibration and the explanation of ordinal variance in GOSE with Somers' Dxy.
Furthermore, we applied TimeSHAP to calculate the contribution of variables and
prior timepoints towards transitions in patient trajectories. Our modelling
strategy achieved calibration at 8 hours, and the full range of variables
explained up to 52% (95% CI: 50-54%) of the variance in ordinal functional
outcome. Up to 91% (90-91%) of this explanation was derived from pre-ICU and
admission information. Information collected in the ICU increased explanation
(by up to 5% [4-6%]), though not enough to counter poorer performance in
longer-stay (>5.75 days) patients. Static variables with the highest
contributions were physician prognoses and certain demographic and CT features.
Among dynamic variables, markers of intracranial hypertension and neurological
function contributed the most. Whilst static information currently accounts for
the majority of functional outcome explanation, our data-driven analysis
highlights investigative avenues to improve dynamic characterisation of
longer-stay patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectional
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avetik Karagulyan, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated sampling algorithms have recently gained great popularity in the
community of machine learning and statistics. This paper studies variants of
such algorithms called Error Feedback Langevin algorithms (ELF). In particular,
we analyze the combinations of EF21 and EF21-P with the federated Langevin
Monte-Carlo. We propose three algorithms: P-ELF, D-ELF, and B-ELF that use,
respectively, primal, dual, and bidirectional compressors. We analyze the
proposed methods under Log-Sobolev inequality and provide non-asymptotic
convergence guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Densely Connected $G$-invariant Deep Neural Networks with Signed
  Permutation Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devanshu Agrawal, James Ostrowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce and investigate, for finite groups $G$, $G$-invariant deep
neural network ($G$-DNN) architectures with ReLU activation that are densely
connected -- i.e., include all possible skip connections. In contrast to other
$G$-invariant architectures in the literature, the preactivations of
the$G$-DNNs presented here are able to transform by \emph{signed} permutation
representations (signed perm-reps) of $G$. Moreover, the individual layers of
the $G$-DNNs are not required to be $G$-equivariant; instead, the
preactivations are constrained to be $G$-equivariant functions of the network
input in a way that couples weights across all layers. The result is a richer
family of $G$-invariant architectures never seen previously. We derive an
efficient implementation of $G$-DNNs after a reparameterization of weights, as
well as necessary and sufficient conditions for an architecture to be
"admissible" -- i.e., nondegenerate and inequivalent to smaller architectures.
We include code that allows a user to build a $G$-DNN interactively
layer-by-layer, with the final architecture guaranteed to be admissible.
Finally, we apply $G$-DNNs to two example problems -- (1) multiplication in
$\{-1, 1\}$ (with theoretical guarantees) and (2) 3D object classification --
finding that the inclusion of signed perm-reps significantly boosts predictive
performance compared to baselines with only ordinary (i.e., unsigned)
perm-reps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 2 figures. For associated code repository see
  https://github.com/dagrawa2/gdnn_code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Descriptive Complexity of Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Grohe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyse the power of graph neural networks (GNNs) in terms of Boolean
circuit complexity and descriptive complexity.
  We prove that the graph queries that can be computed by a polynomial-size
bounded-depth family of GNNs are exactly those definable in the guarded
fragment GFO+C of first-order logic with counting and with built-in relations.
This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN
families may use arbitrary real weights and a wide class of activation
functions that includes the standard ReLU, logistic "sigmoid", and hyperbolic
tangent functions. If the GNNs are allowed to use random initialisation and
global readout (both standard features of GNNs widely used in practice), they
can compute exactly the same queries as bounded depth Boolean circuits with
threshold gates, that is, exactly the queries in TC^0. Moreover, we show that
queries computable by a single GNN with piecewise linear activations and
rational weights are definable in GFO+C without built-in relations. Therefore,
they are contained in uniform TC^0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Privacy Meets Neural Network Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Adamczewski, Mijung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in applying differential privacy to training deep neural
network models is scalability.The widely-used training algorithm,
differentially private stochastic gradient descent (DP-SGD), struggles with
training moderately-sized neural network models for a value of epsilon
corresponding to a high level of privacy protection. In this paper, we explore
the idea of dimensionality reduction inspired by neural network pruning to
improve the scalability of DP-SGD. We study the interplay between neural
network pruning and differential privacy, through the two modes of parameter
updates. We call the first mode, parameter freezing, where we pre-prune the
network and only update the remaining parameters using DP-SGD. We call the
second mode, parameter selection, where we select which parameters to update at
each step of training and update only those selected using DP-SGD. In these
modes, we use public data for freezing or selecting parameters to avoid privacy
loss incurring in these steps. Naturally, the closeness between the private and
public data plays an important role in the success of this paradigm. Our
experimental results demonstrate how decreasing the parameter space improves
differentially private training. Moreover, by studying two popular forms of
pruning which do not rely on gradients and do not incur an additional privacy
loss, we show that random selection performs on par with magnitude-based
selection when it comes to DP-SGD training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of supervised learning models in the Chinese futures market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuquan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on the characteristics of the Chinese futures market, this paper builds
a supervised learning model to predict the trend of futures prices and then
designs a trading strategy based on the prediction results. The Precision,
Recall and F1-score of the classification problem show that our model can meet
the accuracy requirements for the classification of futures price movements in
terms of test data. The backtest results show that our trading system has an
upward trending return curve with low capital retracement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "How to make them stay?" -- Diverse Counterfactual Explanations of
  Employee Attrition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Artelt, Andreas Gregoriades
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employee attrition is an important and complex problem that can directly
affect an organisation's competitiveness and performance. Explaining the
reasons why employees leave an organisation is a key human resource management
challenge due to the high costs and time required to attract and keep talented
employees. Businesses therefore aim to increase employee retention rates to
minimise their costs and maximise their performance. Machine learning (ML) has
been applied in various aspects of human resource management including
attrition prediction to provide businesses with insights on proactive measures
on how to prevent talented employees from quitting. Among these ML methods, the
best performance has been reported by ensemble or deep neural networks, which
by nature constitute black box techniques and thus cannot be easily
interpreted. To enable the understanding of these models' reasoning several
explainability frameworks have been proposed. Counterfactual explanation
methods have attracted considerable attention in recent years since they can be
used to explain and recommend actions to be performed to obtain the desired
outcome. However current counterfactual explanations methods focus on
optimising the changes to be made on individual cases to achieve the desired
outcome. In the attrition problem it is important to be able to foresee what
would be the effect of an organisation's action to a group of employees where
the goal is to prevent them from leaving the company. Therefore, in this paper
we propose the use of counterfactual explanations focusing on multiple
attrition cases from historical data, to identify the optimum interventions
that an organisation needs to make to its practices/policies to prevent or
minimise attrition probability for these cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a short paper at ICEIS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Machine-Learning-supported Model Predictive Force and Motion
  Control in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janine Matschek, Johanna Bethge, Rolf Findeisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many robotic tasks, such as human-robot interactions or the handling of
fragile objects, require tight control and limitation of appearing forces and
moments alongside sensible motion control to achieve safe yet high-performance
operation. We propose a learning-supported model predictive force and motion
control scheme that provides stochastic safety guarantees while adapting to
changing situations. Gaussian processes are used to learn the uncertain
relations that map the robot's states to the forces and moments. The model
predictive controller uses these Gaussian process models to achieve precise
motion and force control under stochastic constraint satisfaction. As the
uncertainty only occurs in the static model parts -- the output equations -- a
computationally efficient stochastic MPC formulation is used. Analysis of
recursive feasibility of the optimal control problem and convergence of the
closed loop system for the static uncertainty case are given. Chance constraint
formulation and back-offs are constructed based on the variance of the Gaussian
process to guarantee safe operation. The approach is illustrated on a
lightweight robot in simulations and experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrapolative Controlled Sequence <span class="highlight-title">Generation</span> via Iterative Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakh Padmakumar, Richard Yuanzhe Pang, He He, Ankur P. Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of extrapolative controlled generation, i.e., generating
sequences with attribute values beyond the range seen in training. This task is
of significant importance in automated design, especially drug discovery, where
the goal is to design novel proteins that are \textit{better} (e.g., more
stable) than existing sequences. Thus, by definition, the target sequences and
their attribute values are out of the training distribution, posing challenges
to existing methods that aim to directly generate the target sequence. Instead,
in this work, we propose Iterative Controlled Extrapolation (ICE) which
iteratively makes local edits to a sequence to enable extrapolation. We train
the model on synthetically generated sequence pairs that demonstrate small
improvement in the attribute value. Results on one natural language task
(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV
fitness) show that ICE considerably outperforms state-of-the-art approaches
despite its simplicity. Our code and models are available at:
https://github.com/vishakhpk/iter-extrapolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Byzantine-Robust Loopless Stochastic Variance-Reduced Gradient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Fedin, Eduard Gorbunov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed optimization with open collaboration is a popular field since it
provides an opportunity for small groups/companies/universities, and
individuals to jointly solve huge-scale problems. However, standard
optimization algorithms are fragile in such settings due to the possible
presence of so-called Byzantine workers -- participants that can send
(intentionally or not) incorrect information instead of the one prescribed by
the protocol (e.g., send anti-gradient instead of stochastic gradients). Thus,
the problem of designing distributed methods with provable robustness to
Byzantine workers has been receiving a lot of attention recently. In
particular, several works consider a very promising way to achieve Byzantine
tolerance via exploiting variance reduction and robust aggregation. The
existing approaches use SAGA- and SARAH-type variance-reduced estimators, while
another popular estimator -- SVRG -- is not studied in the context of
Byzantine-robustness. In this work, we close this gap in the literature and
propose a new method -- Byzantine-Robust Loopless Stochastic Variance Reduced
Gradient (BR-LSVRG). We derive non-asymptotic convergence guarantees for the
new method in the strongly convex case and compare its performance with
existing approaches in numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures. Code: https://github.com/Nikosimus/BR-LSVRG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming Kernel PCA Algorithm With Small Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Deng, Zhao Song, Zifan Wang, Han Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal Component Analysis (PCA) is a widely used technique in machine
learning, data analysis and signal processing. With the increase in the size
and complexity of datasets, it has become important to develop low-space usage
algorithms for PCA. Streaming PCA has gained significant attention in recent
years, as it can handle large datasets efficiently. The kernel method, which is
commonly used in learning algorithms such as Support Vector Machines (SVMs),
has also been applied in PCA algorithms.
  We propose a streaming algorithm for Kernel PCA problems based on the
traditional scheme by Oja. Our algorithm addresses the challenge of reducing
the memory usage of PCA while maintaining its accuracy. We analyze the
performance of our algorithm by studying the conditions under which it
succeeds. Specifically, we show that, when the spectral ratio $R :=
\lambda_1/\lambda_2$ of the target covariance matrix is lower bounded by $C
\cdot \log n\cdot \log d$, the streaming PCA can be solved with $O(d)$ space
cost.
  Our proposed algorithm has several advantages over existing methods. First,
it is a streaming algorithm that can handle large datasets efficiently. Second,
it employs the kernel method, which allows it to capture complex nonlinear
relationships among data points. Third, it has a low-space usage, making it
suitable for applications where memory is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketching with Spherical Designs for Noisy Data Fitting on Spheres 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao-Bo Lin, Di Wang, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a sketching strategy based on spherical designs, which is
applied to the classical spherical basis function approach for massive
spherical data fitting. We conduct theoretical analysis and numerical
verifications to demonstrate the feasibility of the proposed { sketching}
strategy. From the theoretical side, we prove that sketching based on spherical
designs can reduce the computational burden of the spherical basis function
approach without sacrificing its approximation capability. In particular, we
provide upper and lower bounds for the proposed { sketching} strategy to fit
noisy data on spheres. From the experimental side, we numerically illustrate
the feasibility of the sketching strategy by showing its comparable fitting
performance with the spherical basis function approach.
  These interesting findings show that the proposed sketching strategy is
capable of fitting massive and noisy data on spheres.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unimodal Distributions for Ordinal Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaime S. Cardoso, Ricardo Cruz, Tomé Albuquerque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world prediction tasks, class labels contain information about
the relative order between labels that are not captured by commonly used loss
functions such as multicategory cross-entropy. Recently, the preference for
unimodal distributions in the output space has been incorporated into models
and loss functions to account for such ordering information. However, current
approaches rely on heuristics that lack a theoretical foundation. Here, we
propose two new approaches to incorporate the preference for unimodal
distributions into the predictive model. We analyse the set of unimodal
distributions in the probability simplex and establish fundamental properties.
We then propose a new architecture that imposes unimodal distributions and a
new loss term that relies on the notion of projection in a set to promote
unimodality. Experiments show the new architecture achieves top-2 performance,
while the proposed new loss term is very competitive while maintaining high
unimodality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A robust method for reliability updating with equality information using
  sequential adaptive importance sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiong Xiao, Zeyu Wang, Quanwang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliability updating refers to a problem that integrates Bayesian updating
technique with structural reliability analysis and cannot be directly solved by
structural reliability methods (SRMs) when it involves equality information.
The state-of-the-art approaches transform equality information into inequality
information by introducing an auxiliary standard normal parameter. These
methods, however, encounter the loss of computational efficiency due to the
difficulty in finding the maximum of the likelihood function, the large
coefficient of variation (COV) associated with the posterior failure
probability and the inapplicability to dynamic updating problems where new
information is constantly available. To overcome these limitations, this paper
proposes an innovative method called RU-SAIS (reliability updating using
sequential adaptive importance sampling), which combines elements of sequential
importance sampling and K-means clustering to construct a series of important
sampling densities (ISDs) using Gaussian mixture. The last ISD of the sequence
is further adaptively modified through application of the cross entropy method.
The performance of RU-SAIS is demonstrated by three examples. Results show that
RU-SAIS achieves a more accurate and robust estimator of the posterior failure
probability than the existing methods such as subset simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 6 tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magnushammer: A <span class="highlight-title">Transformer</span>-based Approach to Premise Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Mikuła, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Premise selection is a fundamental problem of automated theorem proving.
Previous works often use intricate symbolic methods, rely on domain knowledge,
and require significant engineering effort to solve this task. In this work, we
show that Magnushammer, a neural transformer-based approach, can outperform
traditional symbolic systems by a large margin. Tested on the PISA benchmark,
Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of
Sledgehammer, the most mature and popular symbolic-based solver. Furthermore,
by combining Magnushammer with a neural formal prover based on a language
model, we significantly improve the previous state-of-the-art proof rate from
$57.0\%$ to $71.0\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Together: Using Multi-task Learning to Improve Feature Selection
  within Structural <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. C. Bee, E. Papatheou, M Haywood-Alexander, R. S. Mills, L. A. Bull, K. Worden, N. Dervilis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been recent efforts to move to population-based structural health
monitoring (PBSHM) systems. One area of PBSHM which has been recognised for
potential development is the use of multi-task learning (MTL); algorithms which
differ from traditional independent learning algorithms. Presented here is the
use of the MTL, ''Joint Feature Selection with LASSO'', to provide automatic
feature selection for a structural dataset. The classification task is to
differentiate between the port and starboard side of a tailplane, for samples
from two aircraft of the same model. The independent learner produced perfect
F1 scores but had poor engineering insight; whereas the MTL results were
interpretable, highlighting structural differences as opposed to differences in
experimental set-up.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Digital Biomarkers for Unobtrusive Stress State Screening
  from Multimodal Wearable Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berrenur Saylam, Özlem Durmaz İncel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of wearable technologies, a new kind of healthcare data
has become valuable as medical information. These data provide meaningful
information regarding an individual's physiological and psychological states,
such as activity level, mood, stress, and cognitive health. These biomarkers
are named digital since they are collected from digital devices integrated with
various sensors. In this study, we explore digital biomarkers related to stress
modality by examining data collected from mobile phones and smartwatches. We
utilize machine learning techniques on the Tesserae dataset, precisely Random
Forest, to extract stress biomarkers. Using feature selection techniques, we
utilize weather, activity, heart rate (HR), stress, sleep, and location
(work-home) measurements from wearables to determine the most important
stress-related biomarkers. We believe we contribute to interpreting stress
biomarkers with a high range of features from different devices. In addition,
we classify the $5$ different stress levels with the most important features,
and our results show that we can achieve $85\%$ overall class accuracy by
adjusting class imbalance and adding extra features related to personality
characteristics. We perform similar and even better results in recognizing
stress states with digital biomarkers in a daily-life scenario targeting a
higher number of classes compared to the related studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks Enhanced Smart Contract Vulnerability Detection of
  Educational Blockchain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Wang, Wanxuan Wu, Chunyan Zeng, Jialong Yao, Yang Yang, Hongmin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of blockchain technology, more and more attention has
been paid to the intersection of blockchain and education, and various
educational evaluation systems and E-learning systems are developed based on
blockchain technology. Among them, Ethereum smart contract is favored by
developers for its ``event-triggered" mechanism for building education
intelligent trading systems and intelligent learning platforms. However, due to
the immutability of blockchain, published smart contracts cannot be modified,
so problematic contracts cannot be fixed by modifying the code in the
educational blockchain. In recent years, security incidents due to smart
contract vulnerabilities have caused huge property losses, so the detection of
smart contract vulnerabilities in educational blockchain has become a great
challenge. To solve this problem, this paper proposes a graph neural network
(GNN) based vulnerability detection for smart contracts in educational
blockchains. Firstly, the bytecodes are decompiled to get the opcode. Secondly,
the basic blocks are divided, and the edges between the basic blocks according
to the opcode execution logic are added. Then, the control flow graphs (CFG)
are built. Finally, we designed a GNN-based model for vulnerability detection.
The experimental results show that the proposed method is effective for the
vulnerability detection of smart contracts. Compared with the traditional
approaches, it can get good results with fewer layers of the GCN model, which
shows that the contract bytecode and GCN model are efficient in vulnerability
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RACCER: Towards Reachable and Certain Counterfactual Explanations for
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasmina Gajcin, Ivana Dusparic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While reinforcement learning (RL) algorithms have been successfully applied
to numerous tasks, their reliance on neural networks makes their behavior
difficult to understand and trust. Counterfactual explanations are
human-friendly explanations that offer users actionable advice on how to alter
the model inputs to achieve the desired output from a black-box system.
However, current approaches to generating counterfactuals in RL ignore the
stochastic and sequential nature of RL tasks and can produce counterfactuals
which are difficult to obtain or do not deliver the desired outcome. In this
work, we propose RACCER, the first RL-specific approach to generating
counterfactual explanations for the behaviour of RL agents. We first propose
and implement a set of RL-specific counterfactual properties that ensure easily
reachable counterfactuals with highly-probable desired outcomes. We use a
heuristic tree search of agent's execution trajectories to find the most
suitable counterfactuals based on the defined properties. We evaluate RACCER in
two tasks as well as conduct a user study to show that RL-specific
counterfactuals help users better understand agent's behavior compared to the
current state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grasping Student: semi-supervised learning for robotic manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Krzywicki, Krzysztof Ciebiera, Rafał Michaluk, Inga Maziarz, Marek Cygan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gathering real-world data from the robot quickly becomes a bottleneck when
constructing a robot learning system for grasping. In this work, we design a
semi-supervised grasping system that, on top of a small sample of robot
experience, takes advantage of images of products to be picked, which are
collected without any interactions with the robot. We validate our findings
both in the simulation and in the real world. In the regime of a small number
of robot training samples, taking advantage of the unlabeled data allows us to
achieve performance at the level of 10-fold bigger dataset size used by the
baseline. The code and datasets used in the paper will be released at
https://github.com/nomagiclab/grasping-student.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear Kalman Filtering with Reparametrization Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        San Gultekin, Brendan Kitts, Aaron Flores, John Paisley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel nonlinear Kalman filter that utilizes reparametrization
gradients. The widely used parametric approximation is based on a jointly
Gaussian assumption of the state-space model, which is in turn equivalent to
minimizing an approximation to the Kullback-Leibler divergence. It is possible
to obtain better approximations using the alpha divergence, but the resulting
problem is substantially more complex. In this paper, we introduce an alternate
formulation based on an energy function, which can be optimized instead of the
alpha divergence. The optimization can be carried out using reparametrization
gradients, a technique that has recently been utilized in a number of deep
learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Loss</span>-Curvature Matching for <span class="highlight-title">Dataset</span> Selection and Condensation <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungjae Shin, Heesun Bae, Donghyeok Shin, Weonyoung Joo, Il-Chul Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training neural networks on a large dataset requires substantial
computational costs. Dataset reduction selects or synthesizes data instances
based on the large dataset, while minimizing the degradation in generalization
performance from the full dataset. Existing methods utilize the neural network
during the dataset reduction procedure, so the model parameter becomes
important factor in preserving the performance after reduction. By depending
upon the importance of parameters, this paper introduces a new reduction
objective, coined LCMat, which Matches the Loss Curvatures of the original
dataset and reduced dataset over the model parameter space, more than the
parameter point. This new objective induces a better adaptation of the reduced
dataset on the perturbed parameter region than the exact point matching.
Particularly, we identify the worst case of the loss curvature gap from the
local parameter region, and we derive the implementable upper bound of such
worst-case with theoretical analyses. Our experiments on both coreset selection
and condensation benchmarks illustrate that LCMat shows better generalization
performances than existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26th International Conference on Artificial Intelligence and
  Statistics (AISTATS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MKL-$L_{0/1}$-SVM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhu, Yijie Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formulate the Multiple Kernel Learning (abbreviated as MKL) problem for
the support vector machine with the infamous $(0,1)$-loss function. Some
first-order optimality conditions are given, which could be readily exploited
to develop fast numerical solvers e.g., of the ADMM type.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures. Submitted to the 3rd Chinese Conference on
  Predictive Control and Intelligent Decision (CPCID)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyT-NAS: Hybrid <span class="highlight-title">Transformer</span>s Neural Architecture Search for Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lotfi Abdelkrim Mecharbat, Hadjer Benmeziane, Hamza Ouranoughi, Smail Niar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers have enabled recent attention-based Deep Learning (DL)
architectures to achieve remarkable results in Computer Vision (CV) tasks.
However, due to the extensive computational resources required, these
architectures are rarely implemented on resource-constrained platforms. Current
research investigates hybrid handcrafted convolution-based and attention-based
models for CV tasks such as image classification and object detection. In this
paper, we propose HyT-NAS, an efficient Hardware-aware Neural Architecture
Search (HW-NAS) including hybrid architectures targeting vision tasks on tiny
devices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search space
and enhancing the search strategy as well as the performance predictors. Our
experiments show that HyT-NAS achieves a similar hypervolume with less than ~5x
training evaluations. Our resulting architecture outperforms MLPerf MobileNetV1
by 6.3% accuracy improvement with 3.5x less number of parameters on Visual Wake
Words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CODAI 2022 Workshop - Embedded System Week (ESWeek)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Ferry, Gabriel Laberge, Ulrich Aïvodji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A hybrid model involves the cooperation of an interpretable model and a
complex black box. At inference, any input of the hybrid model is assigned to
either its interpretable or complex component based on a gating mechanism. The
advantages of such models over classical ones are two-fold: 1) They grant users
precise control over the level of transparency of the system and 2) They can
potentially perform better than a standalone black box since redirecting some
of the inputs to an interpretable model implicitly acts as regularization.
Still, despite their high potential, hybrid models remain under-studied in the
interpretability/explainability literature. In this paper, we remedy this fact
by presenting a thorough investigation of such models from three perspectives:
Theory, Taxonomy, and Methods. First, we explore the theory behind the
generalization of hybrid models from the Probably-Approximately-Correct (PAC)
perspective. A consequence of our PAC guarantee is the existence of a sweet
spot for the optimal transparency of the system. When such a sweet spot is
attained, a hybrid model can potentially perform better than a standalone black
box. Secondly, we provide a general taxonomy for the different ways of training
hybrid models: the Post-Black-Box and Pre-Black-Box paradigms. These approaches
differ in the order in which the interpretable and complex components are
trained. We show where the state-of-the-art hybrid models Hybrid-Rule-Set and
Companion-Rule-List fall in this taxonomy. Thirdly, we implement the two
paradigms in a single method: HybridCORELS, which extends the CORELS algorithm
to hybrid modeling. By leveraging CORELS, HybridCORELS provides a certificate
of optimality of its interpretable component and precise control over
transparency. We finally show empirically that HybridCORELS is competitive with
existing hybrid models, and performs just as well as a standalone black box (or
even better) while being partly transparent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparison of rational and neural network based approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinesha Peiris, Reinier Diaz Millan, Nadezda Sukhorukova, Julien Ugon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rational and neural network based approximations are efficient tools in
modern approximation. These approaches are able to produce accurate
approximations to nonsmooth and non-Lipschitz functions, including multivariate
domain functions. In this paper we compare the efficiency of function
approximation using rational approximation, neural network and their
combinations. It was found that rational approximation is superior to neural
network based approaches with the same number of decision variables. Our
numerical experiments demonstrate the efficiency of rational approximation,
even when the number of approximation parameters (that is, the dimension of the
corresponding optimisation problems) is small. Another important contribution
of this paper lies in the improvement of rational approximation algorithms.
Namely, the optimisation based algorithms for rational approximation can be
adjusted to in such a way that the conditioning number of the constraint
matrices are controlled. This simple adjustment enables us to work with high
dimension optimisation problems and improve the design of the neural network.
The main strength of neural networks is in their ability to handle models with
a large number of variables: complex models are decomposed in several simple
optimisation problems. Therefore the the large number of decision variables is
in the nature of neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Message Passing Perspective on Learning Dynamics of Contrastive
  Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Wang, Qi Zhang, Tianqi Du, Jiansheng Yang, Zhouchen Lin, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, contrastive learning achieves impressive results on
self-supervised visual representation learning, but there still lacks a
rigorous understanding of its learning dynamics. In this paper, we show that if
we cast a contrastive objective equivalently into the feature space, then its
learning dynamics admits an interpretable form. Specifically, we show that its
gradient descent corresponds to a specific message passing scheme on the
corresponding augmentation graph. Based on this perspective, we theoretically
characterize how contrastive learning gradually learns discriminative features
with the alignment update and the uniformity update. Meanwhile, this
perspective also establishes an intriguing connection between contrastive
learning and Message Passing Graph Neural Networks (MP-GNNs). This connection
not only provides a unified understanding of many techniques independently
developed in each community, but also enables us to borrow techniques from
MP-GNNs to design new contrastive learning variants, such as graph attention,
graph rewiring, jumpy knowledge techniques, etc. We believe that our message
passing perspective not only provides a new theoretical understanding of
contrastive learning dynamics, but also bridges the two seemingly independent
areas together, which could inspire more interleaving studies to benefit from
each other. The code is available at
https://github.com/PKU-ML/Message-Passing-Contrastive-Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FUSQA: Fetal Ultrasound Segmentation Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sevim Cengiz, Ibrahim Almakk, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have been effective for various fetal ultrasound
segmentation tasks. However, generalization to new unseen data has raised
questions about their effectiveness for clinical adoption. Normally, a
transition to new unseen data requires time-consuming and costly quality
assurance processes to validate the segmentation performance post-transition.
Segmentation quality assessment efforts have focused on natural images, where
the problem has been typically formulated as a dice score regression task. In
this paper, we propose a simplified Fetal Ultrasound Segmentation Quality
Assessment (FUSQA) model to tackle the segmentation quality assessment when no
masks exist to compare with. We formulate the segmentation quality assessment
process as an automated classification task to distinguish between good and
poor-quality segmentation masks for more accurate gestational age estimation.
We validate the performance of our proposed approach on two datasets we collect
from two hospitals using different ultrasound machines. We compare different
architectures, with our best-performing architecture achieving over 90%
classification accuracy on distinguishing between good and poor-quality
segmentation masks from an unseen dataset. Additionally, there was only a
1.45-day difference between the gestational age reported by doctors and
estimated based on CRL measurements using well-segmented masks. On the other
hand, this difference increased and reached up to 7.73 days when we calculated
CRL from the poorly segmented masks. As a result, AI-based approaches can
potentially aid fetal ultrasound segmentation quality assessment and might
detect poor segmentation in real-time screening in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference on Optimal Dynamic Policies via Softmax Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhao Chen, Morgane Austern, Vasilis Syrgkanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating optimal dynamic policies from offline data is a fundamental
problem in dynamic decision making. In the context of causal inference, the
problem is known as estimating the optimal dynamic treatment regime. Even
though there exists a plethora of methods for estimation, constructing
confidence intervals for the value of the optimal regime and structural
parameters associated with it is inherently harder, as it involves non-linear
and non-differentiable functionals of un-known quantities that need to be
estimated. Prior work resorted to sub-sample approaches that can deteriorate
the quality of the estimate. We show that a simple soft-max approximation to
the optimal treatment regime, for an appropriately fast growing temperature
parameter, can achieve valid inference on the truly optimal regime. We
illustrate our result for a two-period optimal dynamic regime, though our
approach should directly extend to the finite horizon case. Our work combines
techniques from semi-parametric inference and $g$-estimation, together with an
appropriate triangular array central limit theorem, as well as a novel analysis
of the asymptotic influence and asymptotic bias of softmax approximations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Lie-Group Bayesian Learning Rule <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eren Mehmet Kıral, Thomas Möllenhoff, Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bayesian Learning Rule provides a framework for generic algorithm design
but can be difficult to use for three reasons. First, it requires a specific
parameterization of exponential family. Second, it uses gradients which can be
difficult to compute. Third, its update may not always stay on the manifold. We
address these difficulties by proposing an extension based on Lie-groups where
posteriors are parametrized through transformations of an arbitrary base
distribution and updated via the group's exponential map. This simplifies all
three difficulties for many cases, providing flexible parametrizations through
group's action, simple gradient computation through reparameterization, and
updates that always stay on the manifold. We use the new learning rule to
derive a new algorithm for deep learning with desirable biologically-plausible
attributes to learn sparse features. Our work opens a new frontier for the
design of new algorithms by exploiting Lie-group structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep-Learning-Based Neural Decoding Framework for Emotional
  Brain-Computer Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinming Wu, Ji Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading emotions precisely from segments of neural activity is crucial for
the development of emotional brain-computer interfaces. Among all neural
decoding algorithms, deep learning (DL) holds the potential to become the most
promising one, yet progress has been limited in recent years. One possible
reason is that the efficacy of DL strongly relies on training samples, yet the
neural data used for training are often from non-human primates and mixed with
plenty of noise, which in turn mislead the training of DL models. Given it is
difficult to accurately determine animals' emotions from humans' perspective,
we assume the dominant noise in neural data representing different emotions is
the labeling error. Here, we report the development and application of a neural
decoding framework called Emo-Net that consists of a confidence learning (CL)
component and a DL component. The framework is fully data-driven and is capable
of decoding emotions from multiple datasets obtained from behaving monkeys. In
addition to improving the decoding ability, Emo-Net significantly improves the
performance of the base DL models, making emotion recognition in animal models
possible. In summary, this framework may inspire novel understandings of the
neural basis of emotion and drive the realization of close-loop emotional
brain-computer interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Mirror Descent Inherently Explores Action Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Guanghui Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing computationally efficient exploration strategies for on-policy
first-order methods that attain optimal $\mathcal{O}(1/\epsilon^2)$ sample
complexity remains open for solving Markov decision processes (MDP). This
manuscript provides an answer to this question from a perspective of
simplicity, by showing that whenever exploration over the state space is
implied by the MDP structure, there seems to be little need for sophisticated
exploration strategies. We revisit a stochastic policy gradient method, named
stochastic policy mirror descent, applied to the infinite horizon, discounted
MDP with finite state and action spaces. Accompanying SPMD we present two
on-policy evaluation operators, both simply following the policy for trajectory
collection with no explicit exploration, or any form of intervention. SPMD with
the first evaluation operator, named value-based estimation, tailors to the
Kullback-Leibler (KL) divergence. Provided the Markov chains on the state space
of generated policies are uniformly mixing with non-diminishing minimal
visitation measure, an $\tilde{\mathcal{O}}( 1 / \epsilon^2)$ sample complexity
is obtained with a linear dependence on the size of the action space. SPMD with
the second evaluation operator, named truncated on-policy Monte Carlo, attains
an $\tilde{\mathcal{O}}(\mathcal{H}_{\mathcal{D}} / \epsilon^2)$ sample
complexity, with the same assumption on the state chains of generated policies.
We characterize $\mathcal{H}_{\mathcal{D}}$ as a divergence-dependent function
of the effective horizon and the size of the action space, which leads to an
exponential dependence of the latter two quantities for the KL divergence, and
a polynomial dependence for the divergence induced by negative Tsallis entropy.
These obtained sample complexities seem to be new among on-policy stochastic
policy gradient methods without explicit explorations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Auditing Large Language Models via Discrete Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auditing large language models for unexpected behaviors is critical to
preempt catastrophic deployments, yet remains challenging. In this work, we
cast auditing as an optimization problem, where we automatically search for
input-output pairs that match a desired target behavior. For example, we might
aim to find a non-toxic input that starts with "Barack Obama" that a model maps
to a toxic output. This optimization problem is difficult to solve as the set
of feasible points is sparse, the space is discrete, and the language models we
audit are non-linear and high-dimensional. To combat these challenges, we
introduce a discrete optimization algorithm, ARCA, that jointly and efficiently
optimizes over inputs and outputs. Our approach automatically uncovers
derogatory completions about celebrities (e.g. "Barack Obama is a legalized
unborn" -> "child murderer"), produces French inputs that complete to English
outputs, and finds inputs that generate a specific name. Our work offers a
promising new tool to uncover models' failure-modes before deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HappyMap: A Generalized Multi-calibration Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhun Deng, Cynthia Dwork, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-calibration is a powerful and evolving concept originating in the field
of algorithmic fairness. For a predictor $f$ that estimates the outcome $y$
given covariates $x$, and for a function class $\mathcal{C}$, multi-calibration
requires that the predictor $f(x)$ and outcome $y$ are indistinguishable under
the class of auditors in $\mathcal{C}$. Fairness is captured by incorporating
demographic subgroups into the class of functions~$\mathcal{C}$. Recent work
has shown that, by enriching the class $\mathcal{C}$ to incorporate appropriate
propensity re-weighting functions, multi-calibration also yields
target-independent learning, wherein a model trained on a source domain
performs well on unseen, future, target domains(approximately) captured by the
re-weightings.
  Formally, multi-calibration with respect to $\mathcal{C}$ bounds
$\big|\mathbb{E}_{(x,y)\sim \mathcal{D}}[c(f(x),x)\cdot(f(x)-y)]\big|$ for all
$c \in \mathcal{C}$. In this work, we view the term $(f(x)-y)$ as just one
specific mapping, and explore the power of an enriched class of mappings. We
propose \textit{HappyMap}, a generalization of multi-calibration, which yields
a wide range of new applications, including a new fairness notion for
uncertainty quantification (conformal prediction), a novel technique for
conformal prediction under covariate shift, and a different approach to
analyzing missing data, while also yielding a unified understanding of several
existing seemingly disparate algorithmic fairness notions and
target-independent learning approaches.
  We give a single \textit{HappyMap} meta-algorithm that captures all these
results, together with a sufficiency condition for its success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at ITCS 2023 (submitted on Sept. 8th, 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically <span class="highlight-title">Consist</span>ent Multi-view Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Zhou, Qinghai Zheng, Shunshun Bai, Jihua Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we devote ourselves to the challenging task of Unsupervised
Multi-view Representation Learning (UMRL), which requires learning a unified
feature representation from multiple views in an unsupervised manner. Existing
UMRL methods mainly concentrate on the learning process in the feature space
while ignoring the valuable semantic information hidden in different views. To
address this issue, we propose a novel Semantically Consistent Multi-view
Representation Learning (SCMRL), which makes efforts to excavate underlying
multi-view semantic consensus information and utilize the information to guide
the unified feature representation learning. Specifically, SCMRL consists of a
within-view reconstruction module and a unified feature representation learning
module, which are elegantly integrated by the contrastive learning strategy to
simultaneously align semantic labels of both view-specific feature
representations and the learned unified feature representation. In this way,
the consensus information in the semantic space can be effectively exploited to
constrain the learning process of unified feature representation. Compared with
several state-of-the-art algorithms, extensive experiments demonstrate its
superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Scenario Representation Learning for Motion Forecasting with
  Heterogeneous Graph Convolutional Recurrent Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Gao, Xiaogang Jia, Yikang Li, Hongkai Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the complex and changing interactions in dynamic scenarios, motion
forecasting is a challenging problem in autonomous driving. Most existing works
exploit static road graphs to characterize scenarios and are limited in
modeling evolving spatio-temporal dependencies in dynamic scenarios. In this
paper, we resort to dynamic heterogeneous graphs to model the scenario. Various
scenario components including vehicles (agents) and lanes, multi-type
interactions, and their changes over time are jointly encoded. Furthermore, we
design a novel heterogeneous graph convolutional recurrent network, aggregating
diverse interaction information and capturing their evolution, to learn to
exploit intrinsic spatio-temporal dependencies in dynamic graphs and obtain
effective representations of dynamic scenarios. Finally, with a motion
forecasting decoder, our model predicts realistic and multi-modal future
trajectories of agents and outperforms state-of-the-art published works on
several motion forecasting benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Synthetic Data <span class="highlight-title">Generation</span> of LLMs Help Clinical Text Mining? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have led to the
development of highly potent models like OpenAI's ChatGPT. These models have
exhibited exceptional performance in a variety of tasks, such as question
answering, essay composition, and code generation. However, their effectiveness
in the healthcare sector remains uncertain. In this study, we seek to
investigate the potential of ChatGPT to aid in clinical text mining by
examining its ability to extract structured information from unstructured
healthcare texts, with a focus on biological named entity recognition and
relation extraction. However, our preliminary results indicate that employing
ChatGPT directly for these tasks resulted in poor performance and raised
privacy concerns associated with uploading patients' information to the ChatGPT
API. To overcome these limitations, we propose a new training paradigm that
involves generating a vast quantity of high-quality synthetic data with labels
utilizing ChatGPT and fine-tuning a local model for the downstream task. Our
method has resulted in significant improvements in the performance of
downstream tasks, improving the F1-score from 23.37% to 63.99% for the named
entity recognition task and from 75.86% to 83.59% for the relation extraction
task. Furthermore, generating data using ChatGPT can significantly reduce the
time and effort required for data collection and labeling, as well as mitigate
data privacy concerns. In summary, the proposed framework presents a promising
solution to enhance the applicability of LLM models to clinical text mining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Actor-Critic Algorithm with Truly Inequality Constraint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taisuke Kobayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft actor-critic (SAC) in reinforcement learning is expected to be one of
the next-generation robot control schemes. Its ability to maximize policy
entropy would make a robotic controller robust to noise and perturbation, which
is useful for real-world robot applications. However, the priority of
maximizing the policy entropy is automatically tuned in the current
implementation, the rule of which can be interpreted as one for equality
constraint, binding the policy entropy into its specified target value. The
current SAC is therefore no longer maximize the policy entropy, contrary to our
expectation. To resolve this issue in SAC, this paper improves its
implementation with a slack variable for appropriately handling the inequality
constraint to maximize the policy entropy. In Mujoco and Pybullet simulators,
the modified SAC achieved the higher robustness and the more stable learning
than before while regularizing the norm of action. In addition, a real-robot
variable impedance task was demonstrated for showing the applicability of the
modified SAC to real-world robot control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning via Variational Bayesian Inference: <span class="highlight-title">Persona</span>lization,
  Sparsity and Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Zhang, Wenpeng Li, Yunfeng Shao, Yinchuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a promising framework that models distributed
machine learning while protecting the privacy of clients. However, FL suffers
performance degradation from heterogeneous and limited data. To alleviate the
degradation, we present a novel personalized Bayesian FL approach named
pFedBayes. By using the trained global distribution from the server as the
prior distribution of each client, each client adjusts its own distribution by
minimizing the sum of the reconstruction error over its personalized data and
the KL divergence with the downloaded global distribution. Then, we propose a
sparse personalized Bayesian FL approach named sFedBayes. To overcome the
extreme heterogeneity in non-i.i.d. data, we propose a clustered Bayesian FL
model named cFedbayes by learning different prior distributions for different
clients. Theoretical analysis gives the generalization error bound of three
approaches and shows that the generalization error convergence rates of the
proposed approaches achieve minimax optimality up to a logarithmic factor.
Moreover, the analysis presents that cFedbayes has a tighter generalization
error rate than pFedBayes. Numerous experiments are provided to demonstrate
that the proposed approaches have better performance than other advanced
personalized methods on private models in the presence of heterogeneous and
limited data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-preserving and Uncertainty-aware Federated Trajectory Prediction
  for Connected Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzi Peng, Jiangwei Wang, Dongjin Song, Fei Miao, Lili Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is the method of choice for trajectory prediction for
autonomous vehicles. Unfortunately, its data-hungry nature implicitly requires
the availability of sufficiently rich and high-quality centralized datasets,
which easily leads to privacy leakage. Besides, uncertainty-awareness becomes
increasingly important for safety-crucial cyber physical systems whose
prediction module heavily relies on machine learning tools. In this paper, we
relax the data collection requirement and enhance uncertainty-awareness by
using Federated Learning on Connected Autonomous Vehicles with an
uncertainty-aware global objective. We name our algorithm as FLTP. We further
introduce ALFLTP which boosts FLTP via using active learning techniques in
adaptatively selecting participating clients. We consider both negative
log-likelihood (NLL) and aleatoric uncertainty (AU) as client selection
metrics. Experiments on Argoverse dataset show that FLTP significantly
outperforms the model trained on local data. In addition, ALFLTP-AU converges
faster in training regression loss and performs better in terms of NLL, minADE
and MR than FLTP in most rounds, and has more stable round-wise performance
than ALFLTP-NLL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Finer Things: Bayesian Structure Learning at the
  Instantiation Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chase Yakaboski, Eugene Santos Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful machine learning methods require a trade-off between memorization
and generalization. Too much memorization and the model cannot generalize to
unobserved examples. Too much over-generalization and we risk under-fitting the
data. While we commonly measure their performance through cross validation and
accuracy metrics, how should these algorithms cope in domains that are
extremely under-determined where accuracy is always unsatisfactory? We present
a novel probabilistic graphical model structure learning approach that can
learn, generalize and explain in these elusive domains by operating at the
random variable instantiation level. Using Minimum Description Length (MDL)
analysis, we propose a new decomposition of the learning problem over all
training exemplars, fusing together minimal entropy inferences to construct a
final knowledge base. By leveraging Bayesian Knowledge Bases (BKBs), a
framework that operates at the instantiation level and inherently subsumes
Bayesian Networks (BNs), we develop both a theoretical MDL score and associated
structure learning algorithm that demonstrates significant improvements over
learned BNs on 40 benchmark datasets. Further, our algorithm incorporates
recent off-the-shelf DAG learning techniques enabling tractable results even on
large problems. We then demonstrate the utility of our approach in a
significantly under-determined domain by learning gene regulatory networks on
breast cancer gene mutational data available from The Cancer Genome Atlas
(TCGA).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Pathways: Learning Multiple Tasks over Multiple Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcong Li, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing useful representations across a large number of tasks is a key
requirement for sample-efficient intelligent systems. A traditional idea in
multitask learning (MTL) is building a shared representation across tasks which
can then be adapted to new tasks by tuning last layers. A desirable refinement
of using a shared one-fits-all representation is to construct task-specific
representations. To this end, recent PathNet/muNet architectures represent
individual tasks as pathways within a larger supernet. The subnetworks induced
by pathways can be viewed as task-specific representations that are composition
of modules within supernet's computation graph. This work explores the pathways
proposal from the lens of statistical learning: We first develop novel
generalization bounds for empirical risk minimization problems learning
multiple tasks over multiple paths (Multipath MTL). In conjunction, we
formalize the benefits of resulting multipath representation when adapting to
new downstream tasks. Our bounds are expressed in terms of Gaussian complexity,
lead to tangible guarantees for the class of linear representations, and
provide novel insights into the quality and benefits of a multipath
representation. When computation graph is a tree, Multipath MTL hierarchically
clusters the tasks and builds cluster-specific representations. We provide
further discussion and experiments for hierarchical MTL and rigorously identify
the conditions under which Multipath MTL is provably superior to traditional
MTL approaches with shallow supernets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster
  Inference on Mobile Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Berger, Manik Dhingra, Antoine Mercier, Yashesh Savani, Sunny Panchal, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present QuickSRNet, an efficient super-resolution
architecture for real-time applications on mobile platforms. Super-resolution
clarifies, sharpens, and upscales an image to higher resolution. Applications
such as gaming and video playback along with the ever-improving display
capabilities of TVs, smartphones, and VR headsets are driving the need for
efficient upscaling solutions. While existing deep learning-based
super-resolution approaches achieve impressive results in terms of visual
quality, enabling real-time DL-based super-resolution on mobile devices with
compute, thermal, and power constraints is challenging. To address these
challenges, we propose QuickSRNet, a simple yet effective architecture that
provides better accuracy-to-latency trade-offs than existing neural
architectures for single-image super resolution. We present training tricks to
speed up existing residual-based super-resolution architectures while
maintaining robustness to quantization. Our proposed architecture produces
1080p outputs via 2x upscaling in 2.2 ms on a modern smartphone, making it
ideal for high-fps real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Learning to Rank with Biased Continuous Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Hongyan Tang, Siwen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a well-known challenge to learn an unbiased ranker with biased
feedback. Unbiased learning-to-rank(LTR) algorithms, which are verified to
model the relative relevance accurately based on noisy feedback, are appealing
candidates and have already been applied in many applications with single
categorical labels, such as user click signals. Nevertheless, the existing
unbiased LTR methods cannot properly handle continuous feedback, which are
essential for many industrial applications, such as content recommender
systems.
  To provide personalized high-quality recommendation results, recommender
systems need model both categorical and continuous biased feedback, such as
click and dwell time. Accordingly, we design a novel unbiased LTR algorithm to
tackle the challenges, which innovatively models position bias in the pairwise
fashion and introduces the pairwise trust bias to separate the position bias,
trust bias, and user relevance explicitly and can work for both continuous and
categorical feedback. Experiment results on public benchmark datasets and
internal live traffic of a large-scale recommender system at Tencent News show
superior results for continuous labels and also competitive performance for
categorical labels of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. arXiv admin note: substantial text overlap with
  arXiv:2111.12929</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Aware Delivery Planning for Last-Mile Logistics <span class="chip">AAMAS-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing delivery routes for last-mile logistics service is challenging and
has attracted the attention of many researchers. These problems are usually
modeled and solved as variants of vehicle routing problems (VRPs) with
challenging real-world constraints (e.g., time windows, precedence). However,
despite many decades of solid research on solving these VRP instances, we still
see significant gaps between optimized routes and the routes that are actually
preferred by the practitioners. Most of these gaps are due to the difference
between what's being optimized, and what the practitioners actually care about,
which is hard to be defined exactly in many instances. In this paper, we
propose a novel hierarchical route optimizer with learnable parameters that
combines the strength of both the optimization and machine learning approaches.
Our hierarchical router first solves a zone-level Traveling Salesman Problem
with learnable weights on various zone-level features; with the zone visit
sequence fixed, we then solve the stop-level vehicle routing problem as a
Shortest Hamiltonian Path problem. The Bayesian optimization approach is then
introduced to allow us to adjust the weights to be assigned to different zone
features used in solving the zone-level Traveling Salesman Problem. By using a
real-world delivery dataset provided by the Amazon Last Mile Routing Research
Challenge, we demonstrate the importance of having both the optimization and
the machine learning components. We also demonstrate how we can use
route-related features to identify instances that we might have difficulty
with. This paves ways to further research on how we can tackle these difficult
instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 22nd International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS-23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Novel Adaptive Fractional Order Gradient Decent Algorithms Design
  via Robust Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Liu, Song Chen, Shengze Cai, Chao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vanilla fractional order gradient descent may oscillatively converge to a
region around the global minimum instead of converging to the exact minimum
point, or even diverge, in the case where the objective function is strongly
convex. To address this problem, a novel adaptive fractional order gradient
descent (AFOGD) method and a novel adaptive fractional order accelerated
gradient descent (AFOAGD) method are proposed in this paper. Inspired by the
quadratic constraints and Lyapunov stability analysis from robust control
theory, we establish a linear matrix inequality to analyse the convergence of
our proposed algorithms. We prove that the proposed algorithms can achieve
R-linear convergence when the objective function is $\textbf{L-}$smooth and
$\textbf{m-}$strongly-convex. Several numerical simulations are demonstrated to
verify the effectiveness and superiority of our proposed algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Memory-Based Learning to Solve Tasks with State-Action Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinal Verghese, Chris Atkeson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks where the set of possible actions depend discontinuously on the state
pose a significant challenge for current reinforcement learning algorithms. For
example, a locked door must be first unlocked, and then the handle turned
before the door can be opened. The sequential nature of these tasks makes
obtaining final rewards difficult, and transferring information between task
variants using continuous learned values such as weights rather than discrete
symbols can be inefficient. Our key insight is that agents that act and think
symbolically are often more effective in dealing with these tasks. We propose a
memory-based learning approach that leverages the symbolic nature of
constraints and temporal ordering of actions in these tasks to quickly acquire
and transfer high-level information. We evaluate the performance of
memory-based learning on both real and simulated tasks with approximately
discontinuous constraints between states and actions, and show our method
learns to solve these tasks an order of magnitude faster than both model-based
and model-free deep reinforcement learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, accepted to the International Conference on
  Robotics and Automation 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Environment-Aware Control Barrier Functions for Safe and
  Feasible Multi-Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Gao, Guang Yang, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control Barrier Functions (CBFs) have been applied to provide safety
guarantees for robot navigation. Traditional approaches consider fixed CBFs
during navigation and hand-tune the underlying parameters apriori. Such
approaches are inefficient and vulnerable to changes in the environment. The
goal of this paper is to learn CBFs for multi-robot navigation based on what
robots perceive about their environment. In order to guarantee the feasibility
of the navigation task, while ensuring robot safety, we pursue a trade-off
between conservativeness and aggressiveness in robot behavior by defining
dynamic environment-aware CBF constraints. Since the explicit relationship
between CBF constraints and navigation performance is challenging to model, we
leverage reinforcement learning to learn time-varying CBFs in a model-free
manner. We parameterize the CBF policy with graph neural networks (GNNs), and
design GNNs that are translation invariant and permutation equivariant, to
synthesize decentralized policies that generalize across environments. The
proposed approach maintains safety guarantees (due to the underlying CBFs),
while optimizing navigation performance (due to the reward-based learning). We
perform simulations that compare the proposed approach with fixed CBFs tuned by
exhaustive grid-search. The results show that environment-aware CBFs are
capable of adapting to robot movements and obstacle changes, yielding improved
navigation performance and robust generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Sparse Recovery with Decision Stumps <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiarash Banihashem, MohammadTaghi Hajiaghayi, Max Springer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision trees are widely used for their low computational cost, good
predictive performance, and ability to assess the importance of features.
Though often used in practice for feature selection, the theoretical guarantees
of these methods are not well understood. We here obtain a tight finite sample
bound for the feature selection problem in linear regression using single-depth
decision trees. We examine the statistical properties of these "decision
stumps" for the recovery of the $s$ active features from $p$ total features,
where $s \ll p$. Our analysis provides tight sample performance guarantees on
high-dimensional sparse systems which align with the finite sample bound of
$O(s \log p)$ as obtained by Lasso, improving upon previous bounds for both the
median and optimal splitting criteria. Our results extend to the non-linear
regime as well as arbitrary sub-Gaussian distributions, demonstrating that tree
based methods attain strong feature selection properties under a wide variety
of settings and further shedding light on the success of these methods in
practice. As a byproduct of our analysis, we show that we can provably
guarantee recovery even when the number of active features $s$ is unknown. We
further validate our theoretical results and proof methodology using
computational experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATM Fraud Detection using Streaming Data Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yelleti Vivek, Vadlamani Ravi, Abhay Anand Mane, Laveti Ramesh Naidu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaining the trust and confidence of customers is the essence of the growth
and success of financial institutions and organizations. Of late, the financial
industry is significantly impacted by numerous instances of fraudulent
activities. Further, owing to the generation of large voluminous datasets, it
is highly essential that underlying framework is scalable and meet real time
needs. To address this issue, in the study, we proposed ATM fraud detection in
static and streaming contexts respectively. In the static context, we
investigated a parallel and scalable machine learning algorithms for ATM fraud
detection that is built on Spark and trained with a variety of machine learning
(ML) models including Naive Bayes (NB), Logistic Regression (LR), Support
Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), Gradient Boosting
Tree (GBT), and Multi-layer perceptron (MLP). We also employed several
balancing techniques like Synthetic Minority Oversampling Technique (SMOTE) and
its variants, Generative Adversarial Networks (GAN), to address the rarity in
the dataset. In addition, we proposed a streaming based ATM fraud detection in
the streaming context. Our sliding window based method collects ATM
transactions that are performed within a specified time interval and then
utilizes to train several ML models, including NB, RF, DT, and K-Nearest
Neighbour (KNN). We selected these models based on their less model complexity
and quicker response time. In both contexts, RF turned out to be the best
model. RF obtained the best mean AUC of 0.975 in the static context and mean
AUC of 0.910 in the streaming context. RF is also empirically proven to be
statistically significant than the next-best performing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures, 10 tables. arXiv admin note: text overlap with
  arXiv:2211.10595</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Benefits of Biophysical Synapses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Lemmel, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The approximation capability of ANNs and their RNN instantiations, is
strongly correlated with the number of parameters packed into these networks.
However, the complexity barrier for human understanding, is arguably related to
the number of neurons and synapses in the networks, and to the associated
nonlinear transformations. In this paper we show that the use of biophysical
synapses, as found in LTCs, have two main benefits. First, they allow to pack
more parameters for a given number of neurons and synapses. Second, they allow
to formulate the nonlinear-network transformation, as a linear system with
state-dependent coefficients. Both increase interpretability, as for a given
task, they allow to learn a system linear in its input features, that is
smaller in size compared to the state of the art. We substantiate the above
claims on various time-series prediction tasks, but we believe that our results
are applicable to any feedforward or recurrent ANN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of Variable-Role-based Feature Enrichment in Neural Models of
  Code <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aftab Hussain, Md Rafiqul Islam Rabin, Bowen Xu, David Lo, Mohammad Amin Alipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep neural models substantially reduce the overhead of feature
engineering, the features readily available in the inputs might significantly
impact training cost and the performance of the models. In this paper, we
explore the impact of an unsuperivsed feature enrichment approach based on
variable roles on the performance of neural models of code. The notion of
variable roles (as introduced in the works of Sajaniemi et al. [Refs. 1,2]) has
been found to help students' abilities in programming. In this paper, we
investigate if this notion would improve the performance of neural models of
code. To the best of our knowledge, this is the first work to investigate how
Sajaniemi et al.'s concept of variable roles can affect neural models of code.
In particular, we enrich a source code dataset by adding the role of individual
variables in the dataset programs, and thereby conduct a study on the impact of
variable role enrichment in training the Code2Seq model. In addition, we shed
light on some challenges and opportunities in feature enrichment for neural
code intelligence models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 1st International Workshop on Interpretability and
  Robustness in Neural Software Engineering (InteNSE'23), Co-located with ICSE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Multi-User Surface Recognition with the Kernel Two-Sample
  Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnam Khojasteh, Friedrich Solowjow, Sebastian Trimpe, Katherine J. Kuchenbecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning and deep learning have been used extensively to classify
physical surfaces through images and time-series contact data. However, these
methods rely on human expertise and entail the time-consuming processes of data
and parameter tuning. To overcome these challenges, we propose an easily
implemented framework that can directly handle heterogeneous data sources for
classification tasks. Our data-versus-data approach automatically quantifies
distinctive differences in distributions in a high-dimensional space via kernel
two-sample testing between two sets extracted from multimodal data (e.g.,
images, sounds, haptic signals). We demonstrate the effectiveness of our
technique by benchmarking against expertly engineered classifiers for
visual-audio-haptic surface recognition due to the industrial relevance,
difficulty, and competitive baselines of this application; ablation studies
confirm the utility of key components of our pipeline. As shown in our
open-source code, we achieve 97.2% accuracy on a standard multi-user dataset
with 108 surface classes, outperforming the state-of-the-art machine-learning
algorithm by 6% on a more difficult version of the task. The fact that our
classifier obtains this performance with minimal data processing in the
standard algorithm setting reinforces the powerful nature of kernel methods for
learning to recognize complex patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Active Learning of Relational State Abstractions for Bilevel
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amber Li, Tom Silver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State abstraction is an effective technique for planning in robotics
environments with continuous states and actions, long task horizons, and sparse
feedback. In object-oriented environments, predicates are a particularly useful
form of state abstraction because of their compatibility with symbolic planners
and their capacity for relational generalization. However, to plan with
predicates, the agent must be able to interpret them in continuous environment
states (i.e., ground the symbols). Manually programming predicate
interpretations can be difficult, so we would instead like to learn them from
data. We propose an embodied active learning paradigm where the agent learns
predicate interpretations through online interaction with an expert. For
example, after taking actions in a block stacking environment, the agent may
ask the expert: "Is On(block1, block2) true?" From this experience, the agent
learns to plan: it learns neural predicate interpretations, symbolic planning
operators, and neural samplers that can be used for bilevel planning. During
exploration, the agent plans to learn: it uses its current models to select
actions towards generating informative expert queries. We learn predicate
interpretations as ensembles of neural networks and use their entropy to
measure the informativeness of potential queries. We evaluate this approach in
three robotic environments and find that it consistently outperforms six
baselines while exhibiting sample efficiency in two key metrics: number of
environment interactions, and number of queries to the expert. Code:
https://tinyurl.com/active-predicates
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Engineering Breast MRIs: Predicting Acquisition Parameters
  Directly from Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Konz, Maciej A. Mazurowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The image acquisition parameters (IAPs) used to create MRI scans are central
to defining the appearance of the images. Deep learning models trained on data
acquired using certain parameters might not generalize well to images acquired
with different parameters. Being able to recover such parameters directly from
an image could help determine whether a deep learning model is applicable, and
could assist with data harmonization and/or domain adaptation. Here, we
introduce a neural network model that can predict many complex IAPs used to
generate an MR image with high accuracy solely using the image, with a single
forward pass. These predicted parameters include field strength, echo and
repetition times, acquisition matrix, scanner model, scan options, and others.
Even challenging parameters such as contrast agent type can be predicted with
good accuracy. We perform a variety of experiments and analyses of our model's
ability to predict IAPs on many MRI scans of new patients, and demonstrate its
usage in a realistic application. Predicting IAPs from the images is an
important step toward better understanding the relationship between image
appearance and IAPs. This in turn will advance the understanding of many
concepts related to the generalizability of neural network models on medical
images, including domain shift, domain adaptation, and data harmonization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at MIDL 2023. Code available at
  https://github.com/mazurowski-lab/MRI-IAP-prediction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Baldur: Whole-Proof <span class="highlight-title">Generation</span> and Repair with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily First, Markus N. Rabe, Talia Ringer, Yuriy Brun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formally verifying software properties is a highly desirable but
labor-intensive task. Recent work has developed methods to automate formal
verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by
training a model to predict one proof step at a time, and using that model to
search through the space of possible proofs. This paper introduces a new method
to automate formal verification: We use large language models, trained on
natural language text and code and fine-tuned on proofs, to generate whole
proofs for theorems at once, rather than one step at a time. We combine this
proof generation model with a fine-tuned repair model to repair generated
proofs, further increasing proving power. As its main contributions, this paper
demonstrates for the first time that: (1) Whole-proof generation using
transformers is possible and is as effective as search-based techniques without
requiring costly search. (2) Giving the learned model additional context, such
as a prior failed proof attempt and the ensuing error message, results in proof
repair and further improves automated proof generation. (3) We establish a new
state of the art for fully automated proof synthesis. We reify our method in a
prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL
theorems and their proofs. In addition to empirically showing the effectiveness
of whole-proof generation, repair, and added context, we show that Baldur
improves on the state-of-the-art tool, Thor, by automatically generating proofs
for an additional 8.7% of the theorems. Together, Baldur and Thor can prove
65.7% of the theorems fully automatically. This paper paves the way for new
research into using large language models for automating formal verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Agnostic Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Mittone, Walter Riviera, Iacopo Colonnelli, Robert Birke, Marco Aldinucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since its debut in 2016, Federated Learning (FL) has been tied to the inner
workings of Deep Neural Networks (DNNs). On the one hand, this allowed its
development and widespread use as DNNs proliferated. On the other hand, it
neglected all those scenarios in which using DNNs is not possible or
advantageous. The fact that most current FL frameworks only allow training DNNs
reinforces this problem. To address the lack of FL solutions for non-DNN-based
use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a
model-agnostic FL algorithm, AdaBoost.F, with an open industry-grade FL
framework: Intel OpenFL. MAFL is the first FL system not tied to any specific
type of machine learning model, allowing exploration of FL scenarios beyond
DNNs and trees. We test MAFL from multiple points of view, assessing its
correctness, flexibility and scaling properties up to 64 nodes. We optimised
the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is
compatible with x86-64, ARM-v8, Power and RISC-V.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Only Crash Once: Improved Object Detection for Real-Time,
  Sim-to-Real Hazardous Terrain Detection and Classification for Autonomous
  Planetary Landings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Chris Gnam, John Crassidis, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of hazardous terrain during the planetary landing of spacecraft
plays a critical role in assuring vehicle safety and mission success. A cheap
and effective way of detecting hazardous terrain is through the use of visual
cameras, which ensure operational ability from atmospheric entry through
touchdown. Plagued by resource constraints and limited computational power,
traditional techniques for visual hazardous terrain detection focus on template
matching and registration to pre-built hazard maps. Although successful on
previous missions, this approach is restricted to the specificity of the
templates and limited by the fidelity of the underlying hazard map, which both
require extensive pre-flight cost and effort to obtain and develop. Terrestrial
systems that perform a similar task in applications such as autonomous driving
utilize state-of-the-art deep learning techniques to successfully localize and
classify navigation hazards. Advancements in spacecraft co-processors aimed at
accelerating deep learning inference enable the application of these methods in
space for the first time. In this work, we introduce You Only Crash Once
(YOCO), a deep learning-based visual hazardous terrain detection and
classification technique for autonomous spacecraft planetary landings. Through
the use of unsupervised domain adaptation we tailor YOCO for training by
simulation, removing the need for real-world annotated data and expensive
mission surveying phases. We further improve the transfer of representative
terrain knowledge between simulation and the real world through visual
similarity clustering. We demonstrate the utility of YOCO through a series of
terrestrial and extraterrestrial simulation-to-real experiments and show
substantial improvements toward the ability to both detect and accurately
classify instances of planetary terrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of AAS/AIAA Astrodynamics Specialist
  Conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-adaptive Depth-wise Heterogenous Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhang, Yutong Dai, Hongyi Wang, Eric Xing, Xun Chen, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a promising paradigm that allows multiple clients to
collaboratively train a model without sharing the local data. However, the
presence of heterogeneous devices in federated learning, such as mobile phones
and IoT devices with varying memory capabilities, would limit the scale and
hence the performance of the model could be trained. The mainstream approaches
to address memory limitations focus on width-slimming techniques, where
different clients train subnetworks with reduced widths locally and then the
server aggregates the subnetworks. The global model produced from these methods
suffers from performance degradation due to the negative impact of the actions
taken to handle the varying subnetwork widths in the aggregation phase. In this
paper, we introduce a memory-adaptive depth-wise learning solution in FL called
FeDepth, which adaptively decomposes the full model into blocks according to
the memory budgets of each client and trains blocks sequentially to obtain a
full inference model. Our method outperforms state-of-the-art approaches,
achieving 5% and more than 10% improvements in top-1 accuracy on CIFAR-10 and
CIFAR-100, respectively. We also demonstrate the effectiveness of depth-wise
fine-tuning on ViT. Our findings highlight the importance of memory-aware
techniques for federated learning with heterogeneous devices and the success of
depth-wise training strategy in improving the global model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, Lionel Briand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are widely used in various application domains
such as image processing, speech recognition, and natural language processing.
However, testing DNN models may be challenging due to the complexity and size
of their input domain. Particularly, testing DNN models often requires
generating or exploring large unlabeled datasets. In practice, DNN test
oracles, which identify the correct outputs for inputs, often require expensive
manual effort to label test data, possibly involving multiple experts to ensure
labeling correctness.
  In this paper, we propose DeepGD, a black-box multi-objective test selection
approach for DNN models. It reduces the cost of labeling by prioritizing the
selection of test inputs with high fault revealing power from large unlabeled
datasets. DeepGD not only selects test inputs with high uncertainty scores to
trigger as many mispredicted inputs as possible but also maximizes the
probability of revealing distinct faults in the DNN model by selecting diverse
mispredicted inputs.
  The experimental results conducted on four widely used datasets and five DNN
models show that in terms of fault-revealing ability: (1) White-box,
coverage-based approaches fare poorly, (2) DeepGD outperforms existing
black-box test selection approaches in terms of fault detection, and (3) DeepGD
also leads to better guidance for DNN model retraining when using selected
inputs to augment the training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extensible Machine Learning for Encrypted Network Traffic Application
  Labeling via Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.05628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.05628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Jorgensen, John Holodnak, Jensen Dempsey, Karla de Souza, Ananditha Raghunath, Vernon Rivet, Noah DeMoes, Andrés Alejos, Allan Wollaber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence of encrypted network traffic, cyber security
analysts have been turning to machine learning (ML) techniques to elucidate the
traffic on their networks. However, ML models can become stale as new traffic
emerges that is outside of the distribution of the training set. In order to
reliably adapt in this dynamic environment, ML models must additionally provide
contextualized uncertainty quantification to their predictions, which has
received little attention in the cyber security domain. Uncertainty
quantification is necessary both to signal when the model is uncertain about
which class to choose in its label assignment and when the traffic is not
likely to belong to any pre-trained classes.
  We present a new, public dataset of network traffic that includes labeled,
Virtual Private Network (VPN)-encrypted network traffic generated by 10
applications and corresponding to 5 application categories. We also present an
ML framework that is designed to rapidly train with modest data requirements
and provide both calibrated, predictive probabilities as well as an
interpretable "out-of-distribution" (OOD) score to flag novel traffic samples.
We describe calibrating OOD scores using p-values of the relative Mahalanobis
distance.
  We demonstrate that our framework achieves an F1 score of 0.98 on our dataset
and that it can extend to an enterprise network by testing the model: (1) on
data from similar applications, (2) on dissimilar application traffic from an
existing category, and (3) on application traffic from a new category. The
model correctly flags uncertain traffic and, upon retraining, accurately
incorporates the new data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper is 15 pages and has 10 figures. Published in IEEE Transactions
  on Artificial Intelligence (https://doi.org/10.1109/TAI.2023.3244168). For
  associated dataset, see
  https://www.ll.mit.edu/r-d/datasets/vpnnonvpn-network-application-traffic-dataset-vnat</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tensor Network Quantum States to Tensorial Recurrent Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dian Wu, Riccardo Rossi, Filippo Vicentini, Giuseppe Carleo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that any matrix product state (MPS) can be exactly represented by a
recurrent neural network (RNN) with a linear memory update. We generalize this
RNN architecture to 2D lattices using a multilinear memory update. It supports
perfect sampling and wave function evaluation in polynomial time, and can
represent an area law of entanglement entropy. Numerical evidence shows that it
can encode the wave function using a bond dimension lower by orders of
magnitude when compared to MPS, with an accuracy that can be systematically
improved by increasing the bond dimension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Closed-Loop Failures of Vision-Based Controllers via
  Reachability Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustav Chakraborty, Somil Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning driven image-based controllers allow robotic systems to take
intelligent actions based on the visual feedback from their environment.
Understanding when these controllers might lead to system safety violations is
important for their integration in safety-critical applications and engineering
corrective safety measures for the system. Existing methods leverage
simulation-based testing (or falsification) to find the failures of
vision-based controllers, i.e., the visual inputs that lead to closed-loop
safety violations. However, these techniques do not scale well to the scenarios
involving high-dimensional and complex visual inputs, such as RGB images. In
this work, we cast the problem of finding closed-loop vision failures as a
Hamilton-Jacobi (HJ) reachability problem. Our approach blends simulation-based
analysis with HJ reachability methods to compute an approximation of the
backward reachable tube (BRT) of the system, i.e., the set of unsafe states for
the system under vision-based controllers. Utilizing the BRT, we can tractably
and systematically find the system states and corresponding visual inputs that
lead to closed-loop failures. These visual inputs can be subsequently analyzed
to find the input characteristics that might have caused the failure. Besides
its scalability to high-dimensional visual inputs, an explicit computation of
BRT allows the proposed approach to capture non-trivial system failures that
are difficult to expose via random simulations. We demonstrate our framework on
two case studies involving an RGB image-based neural network controller for (a)
autonomous indoor navigation, and (b) autonomous aircraft taxiing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Predictions on OOD Images via Nearest Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.08485v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.08485v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao-Yuan Yang, Cyrus Rashtchian, Ruslan Salakhutdinov, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study out-of-distribution (OOD) prediction behavior of neural networks
when they classify images from unseen classes or corrupted images. To probe the
OOD behavior, we introduce a new measure, nearest category generalization
(NCG), where we compute the fraction of OOD inputs that are classified with the
same label as their nearest neighbor in the training set. Our motivation stems
from understanding the prediction patterns of adversarially robust networks,
since previous work has identified unexpected consequences of training to be
robust to norm-bounded perturbations. We find that robust networks have
consistently higher NCG accuracy than natural training, even when the OOD data
is much farther away than the robustness radius. This implies that the local
regularization of robust training has a significant impact on the network's
decision regions. We replicate our findings using many datasets, comparing new
and existing training methods. Overall, adversarially robust networks resemble
a nearest neighbor classifier when it comes to OOD data. Code available at
https://github.com/yangarbiter/nearest-category-generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Transactions on Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PASHA: Efficient HPO and NAS with Progressive Resource Allocation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondrej Bohdal, Lukas Balles, Martin Wistuba, Beyza Ermis, Cédric Archambeau, Giovanni Zappella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperparameter optimization (HPO) and neural architecture search (NAS) are
methods of choice to obtain the best-in-class machine learning models, but in
practice they can be costly to run. When models are trained on large datasets,
tuning them with HPO or NAS rapidly becomes prohibitively expensive for
practitioners, even when efficient multi-fidelity methods are employed. We
propose an approach to tackle the challenge of tuning machine learning models
trained on large datasets with limited computational resources. Our approach,
named PASHA, extends ASHA and is able to dynamically allocate maximum resources
for the tuning procedure depending on the need. The experimental comparison
shows that PASHA identifies well-performing hyperparameter configurations and
architectures while consuming significantly fewer computational resources than
ASHA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The autoregressive neural network architecture of the Boltzmann
  distribution of pairwise interacting spins systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indaco Biazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Autoregressive Neural Networks (ARNN) have recently demonstrated
exceptional results in image and language generation tasks, contributing to the
growing popularity of generative models in both scientific and commercial
applications. This work presents a physical interpretation of the ARNNs by
reformulating the Boltzmann distribution of binary pairwise interacting systems
into autoregressive form. The resulting ARNN architecture has weights and
biases of its first layer corresponding to the Hamiltonian's couplings and
external fields, featuring widely used structures like the residual connections
and a recurrent architecture with clear physical meanings. However, the
exponential growth, with system size, of the number of parameters of the hidden
layers makes its direct application unfeasible. Nevertheless, its
architecture's explicit formulation allows using statistical physics techniques
to derive new ARNNs for specific systems. As examples, new effective ARNN
architectures are derived from two well-known mean-field systems, the
Curie-Weiss and Sherrington-Kirkpatrick models, showing superior performances
in approximating the Boltzmann distributions of the corresponding physics model
compared to other commonly used ARNN architectures. The connection established
between the physics of the system and the ARNN architecture provides a way to
derive new neural network architectures for different interacting systems and
interpret existing ones from a physical perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figure plus the Supplementary Information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Train for Global Optimization Problems in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhan Shetty, Teguh Lembono, Tobias Loew, Sylvain Calinon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convergence of many numerical optimization techniques is highly dependent
on the initial guess given to the solver. To address this issue, we propose a
novel approach that utilizes tensor methods to initialize existing optimization
solvers near global optima. Our method does not require access to a database of
good solutions. We first transform the cost function, which depends on both
task parameters and optimization variables, into a probability density
function. The joint probability distribution of the task parameters and
optimization variables is approximated using the Tensor Train model which
enables efficient conditioning and sampling. Unlike existing methods, we treat
the task parameters as random variables and for a given task we generate
samples for decision variables from the conditional distribution to initialize
the optimization solver. Our method can produce multiple solutions for a given
task from different modes when they exist. We first evaluate the approach on
benchmark functions for numerical optimization that are hard to solve using
gradient-based optimization solvers with a naive initialization. The results
show that the proposed method can generate samples close to global optima and
from multiple modes. We then demonstrate the generality and relevance of our
framework to robotics by applying it to inverse kinematics with obstacles and
motion planning problems with a 7-DoF manipulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Hyper Label Model for Programmatic Weak Supervision <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.13545v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.13545v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renzhi Wu, Shen-En Chen, Jieyu Zhang, Xu Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the human annotation efforts, the programmatic weak supervision
(PWS) paradigm abstracts weak supervision sources as labeling functions (LFs)
and involves a label model to aggregate the output of multiple LFs to produce
training labels. Most existing label models require a parameter learning step
for each dataset. In this work, we present a hyper label model that (once
learned) infers the ground-truth labels for each dataset in a single forward
pass without dataset-specific parameter learning. The hyper label model
approximates an optimal analytical (yet computationally intractable) solution
of the ground-truth labels. We train the model on synthetic data generated in
the way that ensures the model approximates the analytical optimal solution,
and build the model upon Graph Neural Network (GNN) to ensure the model
prediction being invariant (or equivariant) to the permutation of LFs (or data
points). On 14 real-world datasets, our hyper label model outperforms the best
existing methods in both accuracy (by 1.4 points on average) and efficiency (by
six times on average). Our code is available at
https://github.com/wurenzhi/hyper_label_model
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Variable Metric Proximal Gradient with variance reduction for
  non-convex composite optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gersende Fort, Eric Moulines
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel algorithm, the Perturbed Proximal
Preconditioned SPIDER algorithm (3P-SPIDER), designed to solve finite sum
non-convex composite optimization. It is a stochastic Variable Metric
Forward-Backward algorithm, which allows approximate preconditioned forward
operator and uses a variable metric proximity operator as the backward
operator; it also proposes a mini-batch strategy with variance reduction to
address the finite sum setting. We show that 3P-SPIDER extends some Stochastic
preconditioned Gradient Descent-based algorithms and some Incremental
Expectation Maximization algorithms to composite optimization and to the case
the forward operator can not be computed in closed form. We also provide an
explicit control of convergence in expectation of 3P-SPIDER, and study its
complexity in order to satisfy the epsilon-approximate stationary condition.
Our results are the first to combine the composite non-convex optimization
setting, a variance reduction technique to tackle the finite sum setting by
using a minibatch strategy and, to allow deterministic or random approximations
of the preconditioned forward operator. Finally, through an application to
inference in a logistic regression model with random effects, we numerically
compare 3P-SPIDER to other stochastic forward-backward algorithms and discuss
the role of some design parameters of 3P-SPIDER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Statistics and Computing, In press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SHIFT15M: Fashion-specific <span class="highlight-title">dataset</span> for set-to-set matching with several
  distribution shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.12992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.12992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Kimura, Takuma Nakamura, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of set-to-set matching, which involves
matching two different sets of items based on some criteria, especially in the
case of high-dimensional items like images. Although neural networks have been
applied to solve this problem, most machine learning-based approaches assume
that the training and test data follow the same distribution, which is not
always true in real-world scenarios. To address this limitation, we introduce
SHIFT15M, a dataset that can be used to evaluate set-to-set matching models
when the distribution of data changes between training and testing. We conduct
benchmark experiments that demonstrate the performance drop of naive methods
due to distribution shift. Additionally, we provide software to handle the
SHIFT15M dataset in a simple manner, with the URL for the software to be made
available after publication of this manuscript. We believe proposed SHIFT15M
dataset provide a valuable resource for evaluating set-to-set matching models
under the distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Covid19 Reproduction Number: Credibility Intervals by Blockwise Proximal
  Monte Carlo Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09142v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09142v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gersende Fort, Barbara Pascal, Patrice Abry, Nelly Pustelnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring the Covid19 pandemic constitutes a critical societal stake that
received considerable research efforts. The intensity of the pandemic on a
given territory is efficiently measured by the reproduction number, quantifying
the rate of growth of daily new infections. Recently, estimates for the time
evolution of the reproduction number were produced using an inverse problem
formulation with a nonsmooth functional minimization. While it was designed to
be robust to the limited quality of the Covid19 data (outliers, missing
counts), the procedure lacks the ability to output credibility interval based
estimates. This remains a severe limitation for practical use in actual
pandemic monitoring by epidemiologists that the present work aims to overcome
by use of Monte Carlo sampling. After interpretation of the nonsmooth
functional into a Bayesian framework, several sampling schemes are tailored to
adjust the nonsmooth nature of the resulting posterior distribution. The
originality of the devised algorithms stems from combining a Langevin Monte
Carlo sampling scheme with Proximal operators. Performance of the new
algorithms in producing relevant credibility intervals for the reproduction
number estimates and denoised counts are compared. Assessment is conducted on
real daily new infection counts made available by the Johns Hopkins University.
The interest of the devised monitoring tools are illustrated on Covid19 data
from several different countries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihua Zhou, Ruibin Li, Song Guo, Peiran Dong, Yi Liu, Jingcai Guo, Zhenda Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the dramatic growth of Internet video traffic,
where the video bitstreams are often compressed and delivered in low quality to
fit the streamer's uplink bandwidth. To alleviate the quality degradation, it
comes the rise of Neural-enhanced Video Streaming (NVS), which shows great
prospects for recovering low-quality videos by mostly deploying neural
super-resolution (SR) on the media server. Despite its benefit, we reveal that
current mainstream works with SR enhancement have not achieved the desired
rate-distortion trade-off between bitrate saving and quality restoration, due
to: (1) overemphasizing the enhancement on the decoder side while omitting the
co-design of encoder, (2) limited generative capacity to recover high-fidelity
perceptual details, and (3) optimizing the compression-and-restoration pipeline
from the resolution perspective solely, without considering color bit-depth.
Aiming at overcoming these limitations, we are the first to conduct an
encoder-decoder (i.e., codec) synergy by leveraging the inherent
visual-generative property of diffusion models. Specifically, we present the
Codec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly
reduce streaming delivery bitrates while holding pretty higher restoration
capacity over existing methods. First, CaDM improves the encoder's compression
efficiency by simultaneously reducing resolution and color bit-depth of video
frames. Second, CaDM empowers the decoder with high-quality enhancement by
making the denoising diffusion restoration aware of encoder's resolution-color
conditions. Evaluation on public cloud services with OpenMMLab benchmarks shows
that CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common
video standards and achieves much better recovery quality (e.g., FID of 0.61)
over state-of-the-art neural-enhancing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison of semi-supervised deep learning algorithms for audio
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.08183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.08183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léo Cances, Etienne Labbé, Thomas Pellegrini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we adapted five recent SSL methods to the task of audio
classification. The first two methods, namely Deep Co-Training (DCT) and Mean
Teacher (MT), involve two collaborative neural networks. The three other
algorithms, called MixMatch (MM), ReMixMatch (RMM), and FixMatch (FM), are
single-model methods that rely primarily on data augmentation strategies. Using
the Wide-ResNet-28-2 architecture in all our experiments, 10% of labeled data
and the remaining 90% as unlabeled data for training, we first compare the
error rates of the five methods on three standard benchmark audio datasets:
Environmental Sound Classification (ESC-10), UrbanSound8K (UBS8K), and Google
Speech Commands (GSC). In all but one cases, MM, RMM, and FM outperformed MT
and DCT significantly, MM and RMM being the best methods in most experiments.
On UBS8K and GSC, MM achieved 18.02% and 3.25% error rate (ER), respectively,
outperforming models trained with 100% of the available labeled data, which
reached 23.29% and 4.94%, respectively. RMM achieved the best results on ESC-10
(12.00% ER), followed by FM which reached 13.33%. Second, we explored adding
the mixup augmentation, used in MM and RMM, to DCT, MT, and FM. In almost all
cases, mixup brought consistent gains. For instance, on GSC, FM reached 4.44%
and 3.31% ER without and with mixup. Our PyTorch code will be made available
upon paper acceptance at https:// github. com/ Labbe ti/ SSLH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 5 tables. This is the version 3 of the paper.
  Contains minor fixes compared to the EURASIP one (which is the version 2 of
  the paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLCC: A General Framework for Graph-Level Clustering <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11879v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11879v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ju, Yiyang Gu, Binqi Chen, Gongbo Sun, Yifang Qin, Xingyuming Liu, Xiao Luo, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of graph-level clustering, which is a novel
yet challenging task. This problem is critical in a variety of real-world
applications such as protein clustering and genome analysis in bioinformatics.
Recent years have witnessed the success of deep clustering coupled with graph
neural networks (GNNs). However, existing methods focus on clustering among
nodes given a single graph, while exploring clustering on multiple graphs is
still under-explored. In this paper, we propose a general graph-level
clustering framework named Graph-Level Contrastive Clustering (GLCC) given
multiple graphs. Specifically, GLCC first constructs an adaptive affinity graph
to explore instance- and cluster-level contrastive learning (CL).
Instance-level CL leverages graph Laplacian based contrastive loss to learn
clustering-friendly representations while cluster-level CL captures
discriminative cluster representations incorporating neighbor information of
each sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the
optimization of representation learning. The two steps can be alternatively
trained to collaborate and benefit each other. Experiments on a range of
well-known datasets demonstrate the superiority of our proposed GLCC over
competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker
  Assumptions and Communication Compression as a Cherry on the Top <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Gorbunov, Samuel Horváth, Peter Richtárik, Gauthier Gidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byzantine-robustness has been gaining a lot of attention due to the growth of
the interest in collaborative and federated learning. However, many fruitful
directions, such as the usage of variance reduction for achieving robustness
and communication compression for reducing communication costs, remain weakly
explored in the field. This work addresses this gap and proposes Byz-VR-MARINA
- a new Byzantine-tolerant method with variance reduction and compression. A
key message of our paper is that variance reduction is key to fighting
Byzantine workers more effectively. At the same time, communication compression
is a bonus that makes the process more communication efficient. We derive
theoretical convergence guarantees for Byz-VR-MARINA outperforming previous
state-of-the-art for general non-convex and Polyak-Lojasiewicz loss functions.
Unlike the concurrent Byzantine-robust methods with variance reduction and/or
compression, our complexity results are tight and do not rely on restrictive
assumptions such as boundedness of the gradients or limited compression.
Moreover, we provide the first analysis of a Byzantine-tolerant method
supporting non-uniform sampling of stochastic gradients. Numerical experiments
corroborate our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. 42 pages, 8 figures. Changes in v2: few typos and
  inaccuracies were fixed, more clarifications were added. Changes in v3: ICLR
  formatting was applied, additional experiments were added (Appendix B.4-B.5)
  and extra discussion of the results was added to Appendix E.5. Code:
  https://github.com/SamuelHorvath/VR_Byzantine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Research on Efficient Fuzzy Clustering Method Based on Local Fuzzy
  Granular balls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Xie, Qiao Deng, Shuyin Xia, Yangzhou Zhao, Guoyin Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the problem of fuzzy clustering has been widely concerned.
The membership iteration of existing methods is mostly considered globally,
which has considerable problems in noisy environments, and iterative
calculations for clusters with a large number of different sample sizes are not
accurate and efficient. In this paper, starting from the strategy of
large-scale priority, the data is fuzzy iterated using granular-balls, and the
membership degree of data only considers the two granular-balls where it is
located, thus improving the efficiency of iteration. The formed fuzzy
granular-balls set can use more processing methods in the face of different
data scenarios, which enhances the practicability of fuzzy clustering
calculations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient
  Methods <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, Nicolas Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic Gradient Descent-Ascent (SGDA) is one of the most prominent
algorithms for solving min-max optimization and variational inequalities
problems (VIP) appearing in various machine learning tasks. The success of the
method led to several advanced extensions of the classical SGDA, including
variants with arbitrary sampling, variance reduction, coordinate randomization,
and distributed variants with compression, which were extensively studied in
the literature, especially during the last few years. In this paper, we propose
a unified convergence analysis that covers a large variety of stochastic
gradient descent-ascent methods, which so far have required different
intuitions, have different applications and have been developed separately in
various communities. A key to our unified framework is a parametric assumption
on the stochastic estimates. Via our general theoretical framework, we either
recover the sharpest known rates for the known special cases or tighten them.
Moreover, to illustrate the flexibility of our approach we develop several new
variants of SGDA such as a new variance-reduced method (L-SVRGDA), new
distributed methods with compression (QSGDA, DIANA-SGDA, VR-DIANA-SGDA), and a
new method with coordinate randomization (SEGA-SGDA). Although variants of the
new methods are known for solving minimization problems, they were never
considered or analyzed for solving min-max problems and VIPs. We also
demonstrate the most important properties of the new methods through extensive
numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2023. 65 pages, 5 figures, 3 tables. Changes in v2: new
  results were added (Theorem 2.5 and its corollaries), few typos were fixed,
  more clarifications were added. Changes in v3: AISTATS formatting was
  applied, small clarifications were added. Code:
  https://github.com/hugobb/sgda</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Generation</span> of non-stationary stochastic fields using Generative
  Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.05469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.05469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alhasan Abdellatif, Ahmed H. Elsheikh, Daniel Busby, Philippe Berthet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of generating geological facies conditioned on observed data,
samples corresponding to all possible conditions are not generally available in
the training set and hence the generation of these realizations depends primary
on the generalization capability of the trained generative model. The problem
becomes more complex when applied on non-stationary fields. In this work, we
investigate the problem of using Generative Adversarial Networks (GANs) models
to generate non-stationary geological channelized patterns and examine the
models generalization capability at new spatial modes that were never seen in
the given training set. The developed training method based on
spatial-conditioning allowed for effective learning of the correlation between
the spatial conditions (i.e. non-stationary maps) and the realizations
implicitly without using additional loss terms or solving optimization problems
for every new given data after training. In addition, our models can be trained
on 2D and 3D samples. The results on real and artificial datasets show that we
were able to generate geologically-plausible realizations beyond the training
samples and with a strong correlation with the target maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Inference Despite Limited Global Confounding via Mixture Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11602v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11602v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer L. Gordon, Bijan Mazaheri, Yuval Rabani, Leonard J. Schulman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random
variables (the vertices); a Bayesian Network Distribution (BND) is a
probability distribution on the random variables that is Markovian on the
graph. A finite $k$-mixture of such models is graphically represented by a
larger graph which has an additional "hidden" (or "latent") random variable
$U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other
vertex. Models of this type are fundamental to causal inference, where $U$
models an unobserved confounding effect of multiple populations, obscuring the
causal relationships in the observable DAG. By solving the mixture problem and
recovering the joint probability distribution on $U$, traditionally
unidentifiable causal relationships become identifiable. Using a reduction to
the more well-studied "product" case on empty graphs, we give the first
algorithm to learn mixtures of non-empty DAGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper to appear in CLEAR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptation of <span class="highlight-title">Transformer</span>-Based Models using Unlabeled Data for
  Relevance and Polarity Classification of German Customer Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Idrissi-Yaghir, Henning Schäfer, Nadja Bauer, Christoph M. Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding customer feedback is becoming a necessity for companies to
identify problems and improve their products and services. Text classification
and sentiment analysis can play a major role in analyzing this data by using a
variety of machine and deep learning approaches. In this work, different
transformer-based models are utilized to explore how efficient these models are
when working with a German customer feedback dataset. In addition, these
pre-trained models are further analyzed to determine if adapting them to a
specific domain using unlabeled data can yield better results than
off-the-shelf pre-trained models. To evaluate the models, two downstream tasks
from the GermEval 2017 are considered. The experimental results show that
transformer-based models can reach significant improvements compared to a
fastText baseline and outperform the published scores and previous models. For
the subtask Relevance Classification, the best models achieve a micro-averaged
$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a
score of 85.1 % and 85.3 % for the subtask Polarity Classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Complete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Imbalanced Data with Vision <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengzhuo Xu, Ruikang Liu, Shuo Yang, Zenghao Chai, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real-world data tends to be heavily imbalanced and severely skew the
data-driven deep neural networks, which makes Long-Tailed Recognition (LTR) a
massive challenging task. Existing LTR methods seldom train Vision Transformers
(ViTs) with Long-Tailed (LT) data, while the off-the-shelf pretrain weight of
ViTs always leads to unfair comparisons. In this paper, we systematically
investigate the ViTs' performance in LTR and propose LiVT to train ViTs from
scratch only with LT data. With the observation that ViTs suffer more severe
LTR problems, we conduct Masked Generative Pretraining (MGP) to learn
generalized features. With ample and solid evidence, we show that MGP is more
robust than supervised manners. In addition, Binary Cross Entropy (BCE) loss,
which shows conspicuous performance with ViTs, encounters predicaments in LTR.
We further propose the balanced BCE to ameliorate it with strong theoretical
groundings. Specially, we derive the unbiased extension of Sigmoid and
compensate extra logit margins to deploy it. Our Bal-BCE contributes to the
quick convergence of ViTs in just a few epochs. Extensive experiments
demonstrate that with MGP and Bal-BCE, LiVT successfully trains ViTs well
without any additional data and outperforms comparable state-of-the-art methods
significantly, e.g., our ViT-B achieves 81.0% Top-1 accuracy in iNaturalist
2018 without bells and whistles. Code is available at
https://github.com/XuZhengzhuo/LiVT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023, camera-ready version; Code:
  https://github.com/XuZhengzhuo/LiVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Inferential Reproducibility of Machine Learning Research <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hagmann, Philipp Meier, Stefan Riezler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliability of machine learning evaluation -- the consistency of observed
evaluation scores across replicated model training runs -- is affected by
several sources of nondeterminism which can be regarded as measurement noise.
Current tendencies to remove noise in order to enforce reproducibility of
research results neglect inherent nondeterminism at the implementation level
and disregard crucial interaction effects between algorithmic noise factors and
data properties. This limits the scope of conclusions that can be drawn from
such experiments. Instead of removing noise, we propose to incorporate several
sources of variance, including their interaction with data properties, into an
analysis of significance and reliability of machine learning evaluation, with
the aim to draw inferences beyond particular instances of trained models. We
show how to use linear mixed effects models (LMEMs) to analyze performance
evaluation scores, and to conduct statistical inference with a generalized
likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources
of noise like meta-parameter variations into statistical significance testing,
and to assess performance differences conditional on data properties.
Furthermore, a variance component analysis (VCA) enables the analysis of the
contribution of noise sources to overall variance and the computation of a
reliability coefficient by the ratio of substantial to total variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023 (see https://openreview.net/pdf?id=li4GQCQWkv)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Selection for Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakan Pabuccu, Adrian Barbu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the importance of feature selection for improving the
forecasting performance of machine learning algorithms for financial data.
Artificial neural networks (ANN), convolutional neural networks (CNN),
long-short term memory (LSTM) networks, as well as linear models were applied
for forecasting purposes. The Feature Selection with Annealing (FSA) algorithm
was used to select the features from about 1000 possible predictors obtained
from 26 technical indicators with specific periods and their lags. In addition
to this, the Boruta feature selection algorithm was applied as a baseline
feature selection method. The dependent variables consisted of daily
logarithmic returns and daily trends of ten financial data sets, including
cryptocurrency and different stocks. Experiments indicate that the FSA
algorithm increased the performance of ML models regardless of the problem
type. The FSA hybrid machine learning models showed better performance in 10
out of 10 data sets for regression and 8 out of 10 data sets for
classification. None of the hybrid Boruta models outperformed the hybrid FSA
models. However, the BORCNN model performance was comparable to the best model
for 4 out of 10 data sets for regression estimates. BOR-LR and BOR-CNN models
showed comparable performance with the best hybrid FSA models in 2 out of 10
datasets for classification. FSA was observed to improve the model performance
in both better performance metrics as well as a decreased computation time by
providing a lower dimensional input feature space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 figures and 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpReME: Sparse Regression for Multi-Environment Dynamic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MoonJeong Park, Youngbin Choi, Namhoon Lee, Dongwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dynamical systems is a promising avenue for scientific discoveries.
However, capturing the governing dynamics in multiple environments still
remains a challenge: model-based approaches rely on the fidelity of assumptions
made for a single environment, whereas data-driven approaches based on neural
networks are often fragile on extrapolating into the future. In this work, we
develop a method of sparse regression dubbed SpReME to discover the major
dynamics that underlie multiple environments. Specifically, SpReME shares a
sparse structure of ordinary differential equation (ODE) across different
environments in common while allowing each environment to keep the coefficients
of ODE terms independently. We demonstrate that the proposed model captures the
correct dynamics from multiple environments over four different dynamic systems
with improved prediction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at https://github.com/ml-postech/SpReME</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extending DNN-based Multiplicative Masking to Deep Subband Filtering for
  Improved Dereverberation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Marie Lemercier, Julian Tobergte, Timo Gerkmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a scheme for extending deep neural network-based
multiplicative maskers to deep subband filters for speech restoration in the
time-frequency domain. The resulting method can be generically applied to any
deep neural network providing masks in the time-frequency domain, while
requiring only few more trainable parameters and a computational overhead that
is negligible for state-of-the-art neural networks. We demonstrate that the
resulting deep subband filtering scheme outperforms multiplicative masking for
dereverberation, while leaving the denoising performance virtually the same. We
argue that this is because deep subband filtering in the time-frequency domain
fits the subband approximation often assumed in the dereverberation literature,
whereas multiplicative masking corresponds to the narrowband approximation
generally employed in denoising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Heteroscedastic Uncertainty in Learning Complex Spectral
  Mapping for Single-channel Speech Enhancement <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08624v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08624v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Lin Chen, Daniel D. E. Wong, Ke Tan, Buye Xu, Anurag Kumar, Vamsi Krishna Ithapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most speech enhancement (SE) models learn a point estimate and do not make
use of uncertainty estimation in the learning process. In this paper, we show
that modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian
negative log-likelihood (NLL) improves SE performance at no extra cost. During
training, our approach augments a model learning complex spectral mapping with
a temporary submodel to predict the covariance of the enhancement error at each
time-frequency bin. Due to unrestricted heteroscedastic uncertainty, the
covariance introduces an undersampling effect, detrimental to SE performance.
To mitigate undersampling, our approach inflates the uncertainty lower bound
and weights each loss component with their uncertainty, effectively
compensating severely undersampled components with more penalties. Our
multivariate setting reveals common covariance assumptions such as scalar and
diagonal matrices. By weakening these assumptions, we show that the NLL
achieves superior performance compared to popular loss functions including the
mean squared error (MSE), mean absolute error (MAE), and scale-invariant
signal-to-distortion ratio (SI-SDR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding Language with Visual Affordances over Unstructured Data <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oier Mees, Jessica Borja-Diaz, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that Large Language Models (LLMs) can be applied to
ground natural language to a wide variety of robot skills. However, in
practice, learning multi-task, language-conditioned robotic skills typically
requires large-scale data collection and frequent human intervention to reset
the environment or help correcting the current policies. In this work, we
propose a novel approach to efficiently learn general-purpose
language-conditioned robot skills from unstructured, offline and reset-free
data in the real world by exploiting a self-supervised visuo-lingual affordance
model, which requires annotating as little as 1% of the total data with
language. We evaluate our method in extensive experiments both in simulated and
real-world robotic tasks, achieving state-of-the-art performance on the
challenging CALVIN benchmark and learning over 25 distinct visuomotor
manipulation tasks with a single policy in the real world. We find that when
paired with LLMs to break down abstract natural language instructions into
subgoals via few-shot prompting, our method is capable of completing
long-horizon, multi-tier tasks in the real world, while requiring an order of
magnitude less data than previous approaches. Code and videos are available at
http://hulc2.cs.uni-freiburg.de
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE International Conference on Robotics and
  Automation (ICRA). Project website: http://hulc2.cs.uni-freiburg.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Language Maps for Robot Navigation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding language to the visual observations of a navigating agent can be
performed using off-the-shelf visual-language models pretrained on
Internet-scale data (e.g., image captions). While this is useful for matching
images to natural language descriptions of object goals, it remains disjoint
from the process of mapping the environment, so that it lacks the spatial
precision of classic geometric maps. To address this problem, we propose
VLMaps, a spatial map representation that directly fuses pretrained
visual-language features with a 3D reconstruction of the physical world. VLMaps
can be autonomously built from video feed on robots using standard exploration
approaches and enables natural language indexing of the map without additional
labeled data. Specifically, when combined with large language models (LLMs),
VLMaps can be used to (i) translate natural language commands into a sequence
of open-vocabulary navigation goals (which, beyond prior work, can be spatial
by construction, e.g., "in between the sofa and TV" or "three meters to the
right of the chair") directly localized in the map, and (ii) can be shared
among multiple robots with different embodiments to generate new obstacle maps
on-the-fly (by using a list of obstacle categories). Extensive experiments
carried out in simulated and real world environments show that VLMaps enable
navigation according to more complex language instructions than existing
methods. Videos are available at https://vlmaps.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE International Conference on Robotics and
  Automation (ICRA). Project page: https://vlmaps.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-Conditioned Q-Learning as <span class="highlight-title">Knowledge</span> Distillation <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13298v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13298v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Levine, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications of reinforcement learning can be formalized as
goal-conditioned environments, where, in each episode, there is a "goal" that
affects the rewards obtained during that episode but does not affect the
dynamics. Various techniques have been proposed to improve performance in
goal-conditioned environments, such as automatic curriculum generation and goal
relabeling. In this work, we explore a connection between off-policy
reinforcement learning in goal-conditioned settings and knowledge distillation.
In particular: the current Q-value function and the target Q-value estimate are
both functions of the goal, and we would like to train the Q-value function to
match its target for all goals. We therefore apply Gradient-Based Attention
Transfer (Zagoruyko and Komodakis 2017), a knowledge distillation technique, to
the Q-function update. We empirically show that this can improve the
performance of goal-conditioned off-policy reinforcement learning when the
space of goals is high-dimensional. We also show that this technique can be
adapted to allow for efficient learning in the case of multiple simultaneous
sparse goals, where the agent can attain a reward by achieving any one of a
large set of objectives, all specified at test time. Finally, to provide
theoretical support, we give examples of classes of environments where (under
some assumptions) standard off-policy algorithms such as DDPG require at least
O(d^2) replay buffer transitions to learn an optimal policy, while our proposed
technique requires only O(d) transitions, where d is the dimensionality of the
goal and state space. Code is available at
https://github.com/alevine0/ReenGAGE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023 Accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoteng Ma, Shuai Ma, Li Xia, Qianchuan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keeping risk under control is often more crucial than maximizing expected
rewards in real-world decision-making situations, such as finance, robotics,
autonomous driving, etc. The most natural choice of risk measures is variance,
which penalizes the upside volatility as much as the downside part. Instead,
the (downside) semivariance, which captures the negative deviation of a random
variable under its mean, is more suitable for risk-averse proposes. This paper
aims at optimizing the mean-semivariance (MSV) criterion in reinforcement
learning w.r.t. steady reward distribution. Since semivariance is
time-inconsistent and does not satisfy the standard Bellman equation, the
traditional dynamic programming methods are inapplicable to MSV problems
directly. To tackle this challenge, we resort to Perturbation Analysis (PA)
theory and establish the performance difference formula for MSV. We reveal that
the MSV problem can be solved by iteratively solving a sequence of RL problems
with a policy-dependent reward function. Further, we propose two on-policy
algorithms based on the policy gradient theory and the trust region method.
Finally, we conduct diverse experiments from simple bandit problems to
continuous control tasks in MuJoCo, which demonstrate the effectiveness of our
proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accecpted by Journal of Artificial Intelligence Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">ChatGPT</span>: Beginning of an End of Manual Linguistic Data Annotation? Use
  Case of Automatic Genre Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taja Kuzman, Igor Mozetič, Nikola Ljubešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has shown strong capabilities in natural language generation tasks,
which naturally leads researchers to explore where its abilities end. In this
paper, we examine whether ChatGPT can be used for zero-shot text
classification, more specifically, automatic genre identification. We compare
ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on
datasets, manually annotated with genres. The models are compared on test sets
in two languages: English and Slovenian. Results show that ChatGPT outperforms
the fine-tuned model when applied to the dataset which was not seen before by
either of the models. Even when applied on Slovenian language as an
under-resourced language, ChatGPT's performance is no worse than when applied
to English. However, if the model is fully prompted in Slovenian, the
performance drops significantly, showing the current limitations of ChatGPT
usage on smaller languages. The presented results lead us to questioning
whether this is the beginning of an end of laborious manual annotation
campaigns even for smaller languages, such as Slovenian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Configurable calorimeter simulation for AI applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Armando Di Bello, Anton Charkin-Gorbulin, Kyle Cranmer, Etienne Dreyer, Sanmay Ganguly, Eilam Gross, Lukas Heinrich, Lorenzo Santi, Marumi Kado, Nilotpal Kakati, Patrick Rieck, Matteo Tusoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A configurable calorimeter simulation for AI (COCOA) applications is
presented, based on the Geant4 toolkit and interfaced with the Pythia event
generator. This open-source project is aimed to support the development of
machine learning algorithms in high energy physics that rely on realistic
particle shower descriptions, such as reconstruction, fast simulation, and
low-level analysis. Specifications such as the granularity and material of its
nearly hermetic geometry are user-configurable. The tool is supplemented with
simple event processing including topological clustering, jet algorithms, and a
nearest-neighbors graph construction. Formatting is also provided to visualise
events using the Phoenix event display software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimension-reduced KRnet maps for high-dimensional Bayesian inverse
  problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yani Feng, Kejun Tang, Xiaoliang Wan, Qifeng Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dimension-reduced KRnet map approach (DR-KRnet) for
high-dimensional Bayesian inverse problems, which is based on an explicit
construction of a map that pushes forward the prior measure to the posterior
measure in the latent space. Our approach consists of two main components:
data-driven VAE prior and density approximation of the posterior of the latent
variable. In reality, it may not be trivial to initialize a prior distribution
that is consistent with available prior data; in other words, the complex prior
information is often beyond simple hand-crafted priors. We employ variational
autoencoder (VAE) to approximate the underlying distribution of the prior
dataset, which is achieved through a latent variable and a decoder. Using the
decoder provided by the VAE prior, we reformulate the problem in a
low-dimensional latent space. In particular, we seek an invertible transport
map given by KRnet to approximate the posterior distribution of the latent
variable. Moreover, an efficient physics-constrained surrogate model without
any labeled data is constructed to reduce the computational cost of solving
both forward and adjoint problems involved in likelihood computation. With
numerical experiments, we demonstrate the accuracy and efficiency of DR-KRnet
for high-dimensional Bayesian inverse problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Covariance-Regularized Discriminant Analysis for EHR-based
  Predictive Analytics of Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1610.05446v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1610.05446v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Yang, Haoyi Xiong, Kaibo Xu, Licheng Wang, Jiang Bian, Zeyi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Discriminant Analysis (LDA) is a well-known technique for feature
extraction and dimension reduction. The performance of classical LDA, however,
significantly degrades on the High Dimension Low Sample Size (HDLSS) data for
the ill-posed inverse problem. Existing approaches for HDLSS data
classification typically assume the data in question are with Gaussian
distribution and deal the HDLSS classification problem with regularization.
However, these assumptions are too strict to hold in many emerging real-life
applications, such as enabling personalized predictive analysis using
Electronic Health Records (EHRs) data collected from an extremely limited
number of patients who have been diagnosed with or without the target disease
for prediction. In this paper, we revised the problem of predictive analysis of
disease using personal EHR data and LDA classifier. To fill the gap, in this
paper, we first studied an analytical model that understands the accuracy of
LDA for classifying data with arbitrary distribution. The model gives a
theoretical upper bound of LDA error rate that is controlled by two factors:
(1) the statistical convergence rate of (inverse) covariance matrix estimators
and (2) the divergence of the training/testing datasets to fitted
distributions. To this end, we could lower the error rate by balancing the two
factors for better classification performance. Hereby, we further proposed a
novel LDA classifier De-Sparse that leverages De-sparsified Graphical Lasso to
improve the estimation of LDA, which outperforms state-of-the-art LDA
approaches developed for HDLSS data. Such advances and effectiveness are
further demonstrated by both theoretical analysis and extensive experiments on
EHR datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sijia Yang wrote the manuscript into to the current version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqin Tan, Pihe Hu, Ling Pan, Jiatai Huang, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep reinforcement learning (DRL) models usually requires high
computation costs. Therefore, compressing DRL models possesses immense
potential for training acceleration and model deployment. However, existing
methods that generate small models mainly adopt the knowledge
distillation-based approach by iteratively training a dense network. As a
result, the training process still demands massive computing resources. Indeed,
sparse training from scratch in DRL has not been well explored and is
particularly challenging due to non-stationarity in bootstrap training. In this
work, we propose a novel sparse DRL training framework, "the Rigged
Reinforcement Learning Lottery" (RLx2), which builds upon gradient-based
topology evolution and is capable of training a sparse DRL model based entirely
on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target
mechanism with a dynamic-capacity replay buffer to achieve robust value
learning and efficient topology exploration in sparse models. It also reaches
state-of-the-art sparse training performance in several tasks, showing
7.5\times-20\times model compression with less than 3% performance degradation
and up to 20\times and 50\times FLOPs reduction for training and inference,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI
  using Deep Pixel Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mevan Ekanayake, Kamlesh Pawar, Gary Egan, Zhaolin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models are capable of successfully exploiting latent
representations in MR data and have become state-of-the-art for accelerated MRI
reconstruction. However, undersampling the measurements in k-space as well as
the over- or under-parameterized and non-transparent nature of DL make these
models exposed to uncertainty. Consequently, uncertainty estimation has become
a major issue in DL MRI reconstruction. To estimate uncertainty, Monte Carlo
(MC) inference techniques have become a common practice where multiple
reconstructions are utilized to compute the variance in reconstruction as a
measurement of uncertainty. However, these methods demand high computational
costs as they require multiple inferences through the DL model. To this end, we
introduce a method to estimate uncertainty during MRI reconstruction using a
pixel classification framework. The proposed method, PixCUE (stands for Pixel
Classification Uncertainty Estimation) produces the reconstructed image along
with an uncertainty map during a single forward pass through the DL model. We
demonstrate that this approach generates uncertainty maps that highly correlate
with the reconstruction errors with respect to various MR imaging sequences and
under numerous adversarial conditions. We also show that the estimated
uncertainties are correlated to that of the conventional MC method. We further
provide an empirical relationship between the uncertainty estimations using
PixCUE and well-established reconstruction metrics such as NMSE, PSNR, and
SSIM. We conclude that PixCUE is capable of reliably estimating the uncertainty
in MRI reconstruction with a minimum additional computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal contrastive pretraining has been used to train multimodal
representation models, such as CLIP, on large amounts of paired image-text
data. However, previous studies have revealed that such models are vulnerable
to backdoor attacks. Specifically, when trained on backdoored examples, CLIP
learns spurious correlations between the embedded backdoor trigger and the
target label, aligning their representations in the joint embedding space.
Injecting even a small number of poisoned examples, such as 75 examples in 3
million pretraining data, can significantly manipulate the model's behavior,
making it difficult to detect or unlearn such correlations. To address this
issue, we propose CleanCLIP, a finetuning framework that weakens the learned
spurious associations introduced by backdoor attacks by independently
re-aligning the representations for individual modalities. We demonstrate that
unsupervised finetuning using a combination of multimodal contrastive and
unimodal self-supervised objectives for individual modalities can significantly
reduce the impact of the backdoor attack. Additionally, we show that supervised
finetuning on task-specific labeled image data removes the backdoor trigger
from the CLIP vision encoder. We show empirically that CleanCLIP maintains
model performance on benign examples while erasing a range of backdoor attacks
on multimodal contrastive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Positional Encoding via Random Feature Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Eliasof, Fabrizio Frasca, Beatrice Bevilacqua, Eran Treister, Gal Chechik, Haggai Maron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two main families of node feature augmentation schemes have been explored for
enhancing GNNs: random features and spectral positional encoding. Surprisingly,
however, there is still no clear understanding of the relation between these
two augmentation schemes. Here we propose a novel family of positional encoding
schemes which draws a link between the above two approaches and improves over
both. The new approach, named Random Feature Propagation (RFP), is inspired by
the power iteration method and its generalizations. It concatenates several
intermediate steps of an iterative algorithm for computing the dominant
eigenvectors of a propagation matrix, starting from random node features.
Notably, these propagation steps are based on graph-dependent propagation
operators that can be either predefined or learned. We explore the theoretical
and empirical benefits of RFP. First, we provide theoretical justifications for
using random features, for incorporating early propagation steps, and for using
multiple random initializations. Then, we empirically demonstrate that RFP
significantly outperforms both spectral PE and random features in multiple node
classification and graph classification benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Switchable Representation Learning Framework with Self-compatibility <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengsen Wu, Yan Bai, Yihang Lou, Xiongkun Linghu, Jianzhong He, Ling-Yu Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world visual search systems involve deployments on multiple platforms
with different computing and storage resources. Deploying a unified model that
suits the minimal-constrain platforms leads to limited accuracy. It is expected
to deploy models with different capacities adapting to the resource
constraints, which requires features extracted by these models to be aligned in
the metric space. The method to achieve feature alignments is called
``compatible learning''. Existing research mainly focuses on the one-to-one
compatible paradigm, which is limited in learning compatibility among multiple
models. We propose a Switchable representation learning Framework with
Self-Compatibility (SFSC). SFSC generates a series of compatible sub-models
with different capacities through one training process. The optimization of
sub-models faces gradients conflict, and we mitigate this problem from the
perspective of the magnitude and direction. We adjust the priorities of
sub-models dynamically through uncertainty estimation to co-optimize sub-models
properly. Besides, the gradients with conflicting directions are projected to
avoid mutual interference. SFSC achieves state-of-the-art performance on the
evaluated datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drugs Resistance Analysis from Scarce Health Records via Multi-task
  Graph Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Shu, Pei Gao, Lingwei Zhu, Zheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinicians prescribe antibiotics by looking at the patient's health record
with an experienced eye. However, the therapy might be rendered futile if the
patient has drug resistance. Determining drug resistance requires
time-consuming laboratory-level testing while applying clinicians' heuristics
in an automated way is difficult due to the categorical or binary medical
events that constitute health records. In this paper, we propose a novel
framework for rapid clinical intervention by viewing health records as graphs
whose nodes are mapped from medical events and edges as correspondence between
events in given a time window. A novel graph-based model is then proposed to
extract informative features and yield automated drug resistance analysis from
those high-dimensional and scarce graphs. The proposed method integrates
multi-task learning into a common feature extracting graph encoder for
simultaneous analyses of multiple drugs as well as stabilizing learning. On a
massive dataset comprising over 110,000 patients with urinary tract infections,
we verify the proposed method is capable of attaining superior performance on
the drug resistance prediction problem. Furthermore, automated drug
recommendations resemblant to laboratory-level testing can also be made based
on the model resistance analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoFR: Automated Filter Rule <span class="highlight-title">Generation</span> for Adblocking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Le, Salma Elmalaki, Athina Markopoulou, Zubair Shafiq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adblocking relies on filter lists, which are manually curated and maintained
by a community of filter list authors. Filter list curation is a laborious
process that does not scale well to a large number of sites or over time. In
this paper, we introduce AutoFR, a reinforcement learning framework to fully
automate the process of filter rule creation and evaluation for sites of
interest. We design an algorithm based on multi-arm bandits to generate filter
rules that block ads while controlling the trade-off between blocking ads and
avoiding visual breakage. We test AutoFR on thousands of sites and we show that
it is efficient: it takes only a few minutes to generate filter rules for a
site of interest. AutoFR is effective: it generates filter rules that can block
86% of the ads, as compared to 87% by EasyList, while achieving comparable
visual breakage. Furthermore, AutoFR generates filter rules that generalize
well to new sites. We envision that AutoFR can assist the adblocking community
in filter rule generation at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages with 13 figures, 3 tables, 1 algorithm. 3.5 pages of
  references. Appendices include 10 pages of appendices with 11 figures and 3
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RETEXO: Scalable Neural Network Training over Distributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashish Kolluri, Sarthak Choudhary, Bryan Hooi, Prateek Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks offer a promising approach to supervised learning over
graph data. Graph data, especially when it is privacy-sensitive or too large to
train on centrally, is often stored partitioned across disparate processing
units (clients) which want to minimize the communication costs during
collaborative training. The fully-distributed setup takes such partitioning to
its extreme, wherein features of only a single node and its adjacent edges are
kept locally with one client processor. Existing GNNs are not architected for
training in such setups and incur prohibitive costs therein. We propose RETEXO,
a novel transformation of existing GNNs that improves the communication
efficiency during training in the fully-distributed setup. We experimentally
confirm that RETEXO offers up to 6 orders of magnitude better communication
efficiency even when training shallow GNNs, with a minimal trade-off in
accuracy for supervised node classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Membership Inferencing be Refuted? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Kong, Amrita Roy Chowdhury, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference (MI) attack is currently the most popular test for
measuring privacy leakage in machine learning models. Given a machine learning
model, a data point and some auxiliary information, the goal of an MI attack is
to determine whether the data point was used to train the model. In this work,
we study the reliability of membership inference attacks in practice.
Specifically, we show that a model owner can plausibly refute the result of a
membership inference test on a data point $x$ by constructing a proof of
repudiation that proves that the model was trained without $x$. We design
efficient algorithms to construct proofs of repudiation for all data points of
the training dataset. Our empirical evaluation demonstrates the practical
feasibility of our algorithm by constructing proofs of repudiation for popular
machine learning models on MNIST and CIFAR-10. Consequently, our results call
for a re-evaluation of the implications of membership inference attacks in
practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Line Graph Contrastive Learning for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Zhang, Shilin Sun, Guixiang Ma, Caiming Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction tasks focus on predicting possible future connections. Most
existing researches measure the likelihood of links by different similarity
scores on node pairs and predict links between nodes. However, the
similarity-based approaches have some challenges in information loss on nodes
and generalization ability on similarity indexes. To address the above issues,
we propose a Line Graph Contrastive Learning(LGCL) method to obtain rich
information with multiple perspectives. LGCL obtains a subgraph view by h-hop
subgraph sampling with target node pairs. After transforming the sampled
subgraph into a line graph, the link prediction task is converted into a node
classification task, which graph convolution progress can learn edge embeddings
from graphs more effectively. Then we design a novel cross-scale contrastive
learning framework on the line graph and the subgraph to maximize the mutual
information of them, so that fuses the structure and feature information. The
experimental results demonstrate that the proposed LGCL outperforms the
state-of-the-art methods and has better performance on generalization and
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Optimization for Cascade-type Multi-stage Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08330v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08330v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunya Kusakawa, Shion Takeno, Yu Inatsu, Kentaro Kutsukake, Shogo Iwazaki, Takashi Nakano, Toru Ujihara, Masayuki Karasuyama, Ichiro Takeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex processes in science and engineering are often formulated as
multistage decision-making problems. In this paper, we consider a type of
multistage decision-making process called a cascade process. A cascade process
is a multistage process in which the output of one stage is used as an input
for the subsequent stage. When the cost of each stage is expensive, it is
difficult to search for the optimal controllable parameters for each stage
exhaustively. To address this problem, we formulate the optimization of the
cascade process as an extension of the Bayesian optimization framework and
propose two types of acquisition functions based on credible intervals and
expected improvement. We investigate the theoretical properties of the proposed
acquisition functions and demonstrate their effectiveness through numerical
experiments. In addition, we consider an extension called suspension setting in
which we are allowed to suspend the cascade process at the middle of the
multistage decision-making process that often arises in practical problems. We
apply the proposed method in a test problem involving a solar cell simulator,
which was the motivation for this study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>70pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirGNNs: Graph Neural Networks over the Air 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Gao, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are information processing architectures that
model representations from networked data and allow for decentralized
implementation through localized communications. Existing GNN architectures
often assume ideal communication links and ignore channel effects, such as
fading and noise, leading to performance degradation in real-world
implementation. This paper proposes graph neural networks over the air
(AirGNNs), a novel GNN architecture that incorporates the communication model
into the architecture. AirGNN modifies the graph convolutional operation that
shifts graph signals over random communication graphs to take into account
channel fading and noise when aggregating features from neighbors, thus,
improving the architecture robustness to channel impairments during testing. We
propose a stochastic gradient descent based method to train the AirGNN, and
show that the training procedure converges to a stationary solution. Numerical
simulations on decentralized source localization and multi-robot flocking
corroborate theoretical findings and show superior performance of the AirGNN
over wireless communication channels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Limits of Indiscriminate Data Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Lu, Gautam Kamath, Yaoliang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indiscriminate data poisoning attacks aim to decrease a model's test accuracy
by injecting a small amount of corrupted training data. Despite significant
interest, existing attacks remain relatively ineffective against modern machine
learning (ML) architectures. In this work, we introduce the notion of model
poisonability as a technical tool to explore the intrinsic limits of data
poisoning attacks. We derive an easily computable threshold to establish and
quantify a surprising phase transition phenomenon among popular ML models: data
poisoning attacks become effective only when the poisoning ratio exceeds our
threshold. Building on existing parameter corruption attacks and refining the
Gradient Canceling attack, we perform extensive experiments to confirm our
theoretical findings, test the predictability of our transition threshold, and
significantly improve existing data poisoning baselines over a range of
datasets and models. Our work highlights the critical role played by the
poisoning ratio, and sheds new insights on existing empirical results, attacks
and mitigation strategies in data poisoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bort: Towards Explainable Neural Networks with Bounded Orthogonal
  Constraint <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has revolutionized human society, yet the black-box nature of
deep neural networks hinders further application to reliability-demanded
industries. In the attempt to unpack them, many works observe or impact
internal variables to improve the comprehensibility and invertibility of the
black-box models. However, existing methods rely on intuitive assumptions and
lack mathematical guarantees. To bridge this gap, we introduce Bort, an
optimizer for improving model explainability with boundedness and orthogonality
constraints on model parameters, derived from the sufficient conditions of
model comprehensibility and invertibility. We perform reconstruction and
backtracking on the model representations optimized by Bort and observe a clear
improvement in model explainability. Based on Bort, we are able to synthesize
explainable adversarial samples without additional parameters and training.
Surprisingly, we find Bort constantly improves the classification accuracy of
various architectures including ResNet and DeiT on MNIST, CIFAR-10, and
ImageNet. Code: https://github.com/zbr17/Bort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling Attacks on Meta Reinforcement Learning: A Minimax Formulation
  and Complexity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Li, Haozhe Lei, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta RL), as a combination of meta-learning
ideas and reinforcement learning (RL), enables the agent to adapt to different
tasks using a few samples. However, this sampling-based adaptation also makes
meta RL vulnerable to adversarial attacks. By manipulating the reward feedback
from sampling processes in meta RL, an attacker can mislead the agent into
building wrong knowledge from training experience, which deteriorates the
agent's performance when dealing with different tasks after adaptation. This
paper provides a game-theoretical underpinning for understanding this type of
security risk. In particular, we formally define the sampling attack model as a
Stackelberg game between the attacker and the agent, which yields a minimax
formulation. It leads to two online attack schemes: Intermittent Attack and
Persistent Attack, which enable the attacker to learn an optimal sampling
attack, defined by an $\epsilon$-first-order stationary point, within
$\mathcal{O}(\epsilon^{-2})$ iterations. These attack schemes freeride the
learning progress concurrently without extra interactions with the environment.
By corroborating the convergence results with numerical experiments, we observe
that a minor effort of the attacker can significantly deteriorate the learning
performance, and the minimax approach can also help robustify the meta RL
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updates: github repo posted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An ODE Model for Dynamic Matching in Heterogeneous Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowu Dai, Hengzhi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of dynamic matching in heterogeneous networks, where
agents are subject to compatibility restrictions and stochastic arrival and
departure times. In particular, we consider networks with one type of
easy-to-match agents and multiple types of hard-to-match agents, each subject
to its own compatibility constraints. Such a setting arises in many real-world
applications, including kidney exchange programs and carpooling platforms. We
introduce a novel approach to modeling dynamic matching by establishing the
ordinary differential equation (ODE) model, which offers a new perspective for
evaluating various matching algorithms. We study two algorithms, namely the
Greedy and Patient Algorithms, where both algorithms prioritize matching
compatible hard-to-match agents over easy-to-match agents in heterogeneous
networks. Our results demonstrate the trade-off between the conflicting goals
of matching agents quickly and optimally, offering insights into the design of
real-world dynamic matching systems. We provide simulations and a real-world
case study using data from the Organ Procurement and Transplantation Network to
validate theoretical predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Commitment with Signaling under Double-sided Information Asymmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Li, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information asymmetry in games enables players with the information advantage
to manipulate others' beliefs by strategically revealing information to other
players. This work considers a double-sided information asymmetry in a Bayesian
Stackelberg game, where the leader's realized action, sampled from the mixed
strategy commitment, is hidden from the follower. In contrast, the follower
holds private information about his payoff. Given asymmetric information on
both sides, an important question arises: \emph{Does the leader's information
advantage outweigh the follower's?} We answer this question affirmatively in
this work, where we demonstrate that by adequately designing a signaling device
that reveals partial information regarding the leader's realized action to the
follower, the leader can achieve a higher expected utility than that without
signaling. Moreover, unlike previous works on the Bayesian Stackelberg game
where mathematical programming tools are utilized, we interpret the leader's
commitment as a probability measure over the belief space. Such a probabilistic
language greatly simplifies the analysis and allows an indirect signaling
scheme, leading to a geometric characterization of the equilibrium under the
proposed game model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working paper; 18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference and FDR Control for Simulated Markov Random Fields in
  High-dimension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wei, Xiaoyu Lei, Huiming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the consistency and statistical inference of simulated
Markov random fields (MRFs) in a high dimensional background. Our estimators
are based on the Markov chain Monte Carlo maximum likelihood estimation
(MCMC-MLE) method, penalized by the Elastic-net. Under mild conditions that
ensure a specific convergence rate of the MCMC method, the $\ell_{1}$
consistency of Elastic-net-penalized MCMC-MLE is obtained. We further propose a
decorrelated score test based on the decorrelated score function and prove the
asymptotic normality of the score function without the influence of many
nuisance parameters under the assumption that it accelerates the convergence of
the MCMC method. The one-step estimator for a single parameter of interest is
constructed by linearizing the decorrelated score function to solve its root,
and the normality and confidence interval for the true value, is established.
We use different algorithms to control the false discovery rate (FDR) for
multiple testing problems via classic p-values and novel e-values. Finally, we
empirically validate the asymptotic theories and demonstrate both FDR control
procedures in our article have good performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Multi-Task Denoising for joint SDR and PESQ Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1901.09146v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1901.09146v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Kim, Mostafa El-Khamy, Jungwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised learning based on a deep neural network recently has achieved
substantial improvement on speech enhancement. Denoising networks learn mapping
from noisy speech to clean one directly, or to a spectrum mask which is the
ratio between clean and noisy spectra. In either case, the network is optimized
by minimizing mean square error (MSE) between ground-truth labels and
time-domain or spectrum output. However, existing schemes have either of two
critical issues: spectrum and metric mismatches. The spectrum mismatch is a
well known issue that any spectrum modification after short-time Fourier
transform (STFT), in general, cannot be fully recovered after inverse
short-time Fourier transform (ISTFT). The metric mismatch is that a
conventional MSE metric is sub-optimal to maximize our target metrics,
signal-to-distortion ratio (SDR) and perceptual evaluation of speech quality
(PESQ). This paper presents a new end-to-end denoising framework with the goal
of joint SDR and PESQ optimization. First, the network optimization is
performed on the time-domain signals after ISTFT to avoid spectrum mismatch.
Second, two loss functions which have improved correlations with SDR and PESQ
metrics are proposed to minimize metric mismatch. The experimental result
showed that the proposed denoising scheme significantly improved both SDR and
PESQ performance over the existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness in Forecasting of Observations of Linear Dynamical Systems <span class="chip">AAAI 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05274v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05274v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhou, Jakub Marecek, Robert N. Shorten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning, training data often capture the behaviour of multiple
subgroups of some underlying human population. When the nature of training data
for subgroups are not controlled carefully, under-representation bias arises.
To counter this effect we introduce two natural notions of subgroup fairness
and instantaneous fairness to address such under-representation bias in
time-series forecasting problems. Here we show globally convergent methods for
the fairness-constrained learning problems using hierarchies of
convexifications of non-commutative polynomial optimisation problems. Our
empirical results on a biased data set motivated by insurance applications and
the well-known COMPAS data set demonstrate the efficacy of our methods. We also
show that by exploiting sparsity in the convexifications, we can reduce the run
time of our methods considerably.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal version of Zhou et al. [arXiv:2006.07315, AAAI 2021]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling representations in Restricted Boltzmann Machines without
  adversaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11600v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11600v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Fernandez-de-Cossio-Diaz, Simona Cocco, Remi Monasson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A goal of unsupervised machine learning is to build representations of
complex high-dimensional data, with simple relations to their properties. Such
disentangled representations make easier to interpret the significant latent
factors of variation in the data, as well as to generate new data with
desirable features. Methods for disentangling representations often rely on an
adversarial scheme, in which representations are tuned to avoid discriminators
from being able to reconstruct information about the data properties (labels).
Unfortunately adversarial training is generally difficult to implement in
practice. Here we propose a simple, effective way of disentangling
representations without any need to train adversarial discriminators, and apply
our approach to Restricted Boltzmann Machines (RBM), one of the simplest
representation-based generative models. Our approach relies on the introduction
of adequate constraints on the weights during training, which allows us to
concentrate information about labels on a small subset of latent variables. The
effectiveness of the approach is illustrated with four examples: the CelebA
dataset of facial images, the two-dimensional Ising model, the MNIST dataset of
handwritten digits, and the taxonomy of protein families. In addition, we show
how our framework allows for analytically computing the cost, in terms of
log-likelihood of the data, associated to the disentanglement of their
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor corrections. Accepted for publication in Physical Review X</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Defense of the Unitary Scalarization for Deep Multi-Task Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04122v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04122v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, M. Pawan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multi-task learning research argues against unitary scalarization,
where training simply minimizes the sum of the task losses. Several ad-hoc
multi-task optimization algorithms have instead been proposed, inspired by
various hypotheses about what makes multi-task settings difficult. The majority
of these optimizers require per-task gradients, and introduce significant
memory, runtime, and implementation overhead. We show that unitary
scalarization, coupled with standard regularization and stabilization
techniques from single-task learning, matches or improves upon the performance
of complex multi-task optimizers in popular supervised and reinforcement
learning settings. We then present an analysis suggesting that many specialized
multi-task optimizers can be partly interpreted as forms of regularization,
potentially explaining our surprising results. We believe our results call for
a critical reevaluation of recent research in the area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera-ready version, fixed training loss y axis scale</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Fairness of Medical Image Classification with Multiple Sensitive
  Attributes via Learning Orthogonal Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01481v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01481v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Deng, Yuan Zhong, Qi Dou, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the discrimination of machine learning models has gained
increasing attention in medical image analysis. However, rare works focus on
fair treatments for patients with multiple sensitive demographic ones, which is
a crucial yet challenging problem for real-world clinical applications. In this
paper, we propose a novel method for fair representation learning with respect
to multi-sensitive attributes. We pursue the independence between target and
multi-sensitive representations by achieving orthogonality in the
representation space. Concretely, we enforce the column space orthogonality by
keeping target information on the complement of a low-rank sensitive space.
Furthermore, in the row space, we encourage feature dimensions between target
and sensitive representations to be orthogonal. The effectiveness of the
proposed method is demonstrated with extensive experiments on the CheXpert
dataset. To our best knowledge, this is the first work to mitigate unfairness
with respect to multiple sensitive attributes in the field of medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven reduced-order modelling for blood flow simulations with
  geometry-informed snapshots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwei Ye, Valeria Krzhizhanovskaya, Alfons G. Hoekstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational fluid dynamics is a common tool in cardiovascular science and
engineering to simulate, predict and study hemodynamics in arteries. However,
owing to the complexity and scale of cardiovascular flow problems, the
evaluation of the model could be computationally expensive, especially in those
cases where a large number of evaluations are required, such as uncertainty
quantification and design optimisation. In such scenarios, the model may have
to be repeatedly evaluated due to the changes or distinctions of simulation
domains. In this work, a data-driven surrogate model is proposed for the
efficient prediction of blood flow simulations on similar but distinct domains.
The proposed surrogate model leverages surface registration to parameterise
those similar but distinct shapes and formulate corresponding hemodynamics
information into geometry-informed snapshots by the diffeomorphism constructed
between the reference domain and target domain. A non-intrusive reduced-order
model for geometrical parameters is subsequently constructed using proper
orthogonal decomposition, and a radial basis function interpolator is trained
for predicting the reduced coefficients of the reduced-order model based on
reduced coefficients of geometrical parameters of the shape. Two examples of
blood flowing through a stenosis and a bifurcation are presented and analysed.
The proposed surrogate model demonstrates its accuracy and efficiency in
hemodynamics prediction and shows its potential application toward real-time
simulation or uncertainty quantification for complex patient-specific
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning and Bayesian Optimization: a Unified Perspective to
  Learn with a Goal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Fiore, Michela Nardelli, Laura Mainini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both Bayesian optimization and active learning realize an adaptive sampling
scheme to achieve a specific learning goal. However, while the two fields have
seen an exponential growth in popularity in the past decade, their dualism has
received relatively little attention. In this paper, we argue for an original
unified perspective of Bayesian optimization and active learning based on the
synergy between the principles driving the sampling policies. This symbiotic
relationship is demonstrated through the substantial analogy between the infill
criteria of Bayesian optimization and the learning criteria in active learning,
and is formalized for the case of single information source and when multiple
sources at different levels of fidelity are available. We further investigate
the capabilities of each infill criteria both individually and in combination
on a variety of analytical benchmark problems, to highlight benefits and
limitations over mathematical properties that characterize real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why do networks have inhibitory/negative connections? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03211v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03211v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Wang, Michael A. Powell, Ali Geisa, Eric Bridgeford, Carey E. Priebe, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why do brains have inhibitory connections? Why do deep networks have negative
weights? We believe representing functions is the primary role of both (i) the
brain in natural intelligence, and (ii) deep networks in artificial
intelligence. Our answer to why there are inhibitory/negative weights is: to
learn more functions. We prove that, in the absence of negative weights, neural
networks with non-decreasing activation functions are not universal
approximators. While this may be an intuitive result to some, to the best of
our knowledge, there is no formal theory, in either machine learning or
neuroscience, that demonstrates why negative weights are crucial in the context
of representation capacity. Further, we provide insights on the geometric
properties of the representation space that non-negative deep networks cannot
represent. We expect these insights will yield a deeper understanding of more
sophisticated inductive priors imposed on the distribution of weights that lead
to more efficient biological and machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Concept-Based Interpretability for Graph Neural Networks via
  Neuron Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Xuanyuan, Pietro Barbiero, Dobrik Georgiev, Lucie Charlotte Magister, Pietro Lió
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are highly effective on a variety of
graph-related tasks; however, they lack interpretability and transparency.
Current explainability approaches are typically local and treat GNNs as
black-boxes. They do not look inside the model, inhibiting human trust in the
model and explanations. Motivated by the ability of neurons to detect
high-level semantic concepts in vision models, we perform a novel analysis on
the behaviour of individual GNN neurons to answer questions about GNN
interpretability, and propose new metrics for evaluating the interpretability
of GNN neurons. We propose a novel approach for producing global explanations
for GNNs using neuron-level concepts to enable practitioners to have a
high-level view of the model. Specifically, (i) to the best of our knowledge,
this is the first work which shows that GNN neurons act as concept detectors
and have strong alignment with concepts formulated as logical compositions of
node degree and neighbourhood properties; (ii) we quantitatively assess the
importance of detected concepts, and identify a trade-off between training
duration and neuron-level interpretability; (iii) we demonstrate that our
global explainability approach has advantages over the current state-of-the-art
-- we can disentangle the explanation into individual interpretable concepts
backed by logical descriptions, which reduces potential for bias and improves
user-friendliness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional optimization of quantum circuits for quantum kernels of
  support vector machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elham Torabian, Roman V. Krems
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While quantum machine learning (ML) has been proposed to be one of the most
promising applications of quantum computing, how to build quantum ML models
that outperform classical ML remains a major open question. Here, we
demonstrate a Bayesian algorithm for constructing quantum kernels for support
vector machines that adapts quantum gate sequences to data. The algorithm
increases the complexity of quantum circuits incrementally by appending quantum
gates selected with Bayesian information criterion as circuit selection metric
and Bayesian optimization of the parameters of the locally optimal quantum
circuits identified. The goal is to build quantum kernels for SVM that can
solve classification problems with as little training data as possible. The
performance of the resulting quantum models for the classification problems
considered here significantly exceeds that of optimized classical models with
conventional kernels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Convolutions Cause an Implicit Bias towards High Frequency
  Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.11440v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.11440v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josue Ortega Caro, Yilong Ju, Ryan Pyle, Sourav Dey, Wieland Brendel, Fabio Anselmi, Ankit Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Attacks are still a significant challenge for neural networks.
Recent work has shown that adversarial perturbations typically contain
high-frequency features, but the root cause of this phenomenon remains unknown.
Inspired by theoretical work on linear full-width convolutional models, we
hypothesize that the local (i.e. bounded-width) convolutional operations
commonly used in current neural networks are implicitly biased to learn high
frequency features, and that this is one of the root causes of high frequency
adversarial examples. To test this hypothesis, we analyzed the impact of
different choices of linear and nonlinear architectures on the implicit bias of
the learned features and the adversarial perturbations, in both spatial and
frequency domains. We find that the high-frequency adversarial perturbations
are critically dependent on the convolution operation because the
spatially-limited nature of local convolutions induces an implicit bias towards
high frequency features. The explanation for the latter involves the Fourier
Uncertainty Principle: a spatially-limited (local in the space domain) filter
cannot also be frequency-limited (local in the frequency domain). Furthermore,
using larger convolution kernel sizes or avoiding convolutions (e.g. by using
Vision Transformers architecture) significantly reduces this high frequency
bias, but not the overall susceptibility to attacks. Looking forward, our work
strongly suggests that understanding and controlling the implicit bias of
architectures will be essential for achieving adversarial robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimistic Whittle Index Policy: Online Learning for Restless Bandits <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15372v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15372v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang*, Lily Xu, Aparna Taneja, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Restless multi-armed bandits (RMABs) extend multi-armed bandits to allow for
stateful arms, where the state of each arm evolves restlessly with different
transitions depending on whether that arm is pulled. Solving RMABs requires
information on transition dynamics, which are often unknown upfront. To plan in
RMAB settings with unknown transitions, we propose the first online learning
algorithm based on the Whittle index policy, using an upper confidence bound
(UCB) approach to learn transition dynamics. Specifically, we estimate
confidence bounds of the transition probabilities and formulate a bilinear
program to compute optimistic Whittle indices using these estimates. Our
algorithm, UCWhittle, achieves sublinear $O(H \sqrt{T \log T})$ frequentist
regret to solve RMABs with unknown transitions in $T$ episodes with a constant
horizon $H$. Empirically, we demonstrate that UCWhittle leverages the structure
of RMABs and the Whittle index policy solution to achieve better performance
than existing online learning baselines across three domains, including one
constructed from a real-world maternal and childcare dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2023. 7 page paper, 2 page references, 9 page
  appendix. Code available. Proceedings of the Thirty-Seventh AAAI Conference
  on Artificial Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-MEC Cooperation Based VR Video Transmission and Cache using
  K-Shortest Paths Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwen Xia, Luyao Chen, Yong Tang, Ting Yang, Wenyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent network architectures, multi-MEC cooperative caching has been
introduced to reduce the transmission latency of VR videos, in which MEC
servers' computing and caching capability are utilized to optimize the
transmission process. However, many solutions that use the computing capability
of MEC servers ignore the additional arithmetic power consumed by the codec
process, thus making them infeasible. Besides, the minimum cache unit is
usually the entire VR video, which makes caching inefficient.
  To address these challenges, we split VR videos into tile files for caching
based on the current popular network architecture and provide a reliable
transmission mechanism and an effective caching strategy. Since the number of
different tile files N is too large, the current cooperative caching algorithms
do not cope with such large-scale input data. We further analyze the problem
and propose an optimized k-shortest paths (OKSP) algorithm with an upper bound
time complexity of O((K * M + N) * M * logN)), and suitable for shortest paths
with restricted number of edges, where K is the total number of tiles that all
M MEC servers can cache in the collaboration domain. And we prove the OKSP
algorithm can compute the caching scheme with the lowest average latency in any
case, which means the solution given is the exact solution. The simulation
results show that the OKSP algorithm has excellent speed for solving
large-scale data and consistently outperforms other caching algorithms in the
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihua Zhou, Ruibin Li, Song Guo, Peiran Dong, Yi Liu, Jingcai Guo, Zhenda Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the dramatic growth of Internet video traffic,
where the video bitstreams are often compressed and delivered in low quality to
fit the streamer's uplink bandwidth. To alleviate the quality degradation, it
comes the rise of Neural-enhanced Video Streaming (NVS), which shows great
prospects for recovering low-quality videos by mostly deploying neural
super-resolution (SR) on the media server. Despite its benefit, we reveal that
current mainstream works with SR enhancement have not achieved the desired
rate-distortion trade-off between bitrate saving and quality restoration, due
to: (1) overemphasizing the enhancement on the decoder side while omitting the
co-design of encoder, (2) limited generative capacity to recover high-fidelity
perceptual details, and (3) optimizing the compression-and-restoration pipeline
from the resolution perspective solely, without considering color bit-depth.
Aiming at overcoming these limitations, we are the first to conduct an
encoder-decoder (i.e., codec) synergy by leveraging the inherent
visual-generative property of diffusion models. Specifically, we present the
Codec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly
reduce streaming delivery bitrates while holding pretty higher restoration
capacity over existing methods. First, CaDM improves the encoder's compression
efficiency by simultaneously reducing resolution and color bit-depth of video
frames. Second, CaDM empowers the decoder with high-quality enhancement by
making the denoising diffusion restoration aware of encoder's resolution-color
conditions. Evaluation on public cloud services with OpenMMLab benchmarks shows
that CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common
video standards and achieves much better recovery quality (e.g., FID of 0.61)
over state-of-the-art neural-enhancing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Question Answering Using CLIP-Guided Visual-Text Attention <span class="chip">ICIP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Ye, Weikai Kong, Chenglin Yao, Jianfeng Ren, Xudong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal learning of video and text plays a key role in Video Question
Answering (VideoQA). In this paper, we propose a visual-text attention
mechanism to utilize the Contrastive Language-Image Pre-training (CLIP) trained
on lots of general domain language-image pairs to guide the cross-modal
learning for VideoQA. Specifically, we first extract video features using a
TimeSformer and text features using a BERT from the target application domain,
and utilize CLIP to extract a pair of visual-text features from the
general-knowledge domain through the domain-specific learning. We then propose
a Cross-domain Learning to extract the attention information between visual and
linguistic features across the target domain and general domain. The set of
CLIP-guided visual-text features are integrated to predict the answer. The
proposed method is evaluated on MSVD-QA and MSRVTT-QA datasets, and outperforms
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE International Conference on Image
  Processing (ICIP 2023)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-07T00:00:00Z">2023-03-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Architecture for Out of Domain Intent Detection and Intent
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masoud Akbari, Ali Mohades, M. Hassan Shirali-Shahreza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent Detection is one of the tasks of the Natural Language Understanding
(NLU) unit in task-oriented dialogue systems. Out of Scope (OOS) and Out of
Domain (OOD) inputs may run these systems into a problem. On the other side, a
labeled dataset is needed to train a model for Intent Detection in
task-oriented dialogue systems. The creation of a labeled dataset is
time-consuming and needs human resources. The purpose of this article is to
address mentioned problems. The task of identifying OOD/OOS inputs is named
OOD/OOS Intent Detection. Also, discovering new intents and pseudo-labeling of
OOD inputs is well known by Intent Discovery. In OOD intent detection part, we
make use of a Variational Autoencoder to distinguish between known and unknown
intents independent of input data distribution. After that, an unsupervised
clustering method is used to discover different unknown intents underlying
OOD/OOS inputs. We also apply a non-linear dimensionality reduction on OOD/OOS
representations to make distances between representations more meaning full for
clustering. Our results show that the proposed model for both OOD/OOS Intent
Detection and Intent Discovery achieves great results and passes baselines in
English and Persian languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Asymmetry for Synthetic Training Data <span class="highlight-title">Generation</span>: SynthIE and
  the Case of Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show great potential for synthetic data
generation. This work shows that useful data can be synthetically generated
even for tasks that cannot be solved directly by the LLM: we show that, for
problems with structured outputs, it is possible to prompt an LLM to perform
the task in the opposite direction, to generate plausible text for the target
structure. Leveraging the asymmetry in task difficulty makes it possible to
produce large-scale, high-quality data for complex tasks. We demonstrate the
effectiveness of this approach on closed information extraction, where
collecting ground-truth data is challenging, and no satisfactory dataset exists
to date. We synthetically generate a dataset of 1.8M data points, demonstrate
its superior quality compared to existing datasets in a human evaluation and
use it to finetune small models (220M and 770M parameters). The models we
introduce, SynthIE, outperform existing baselines of comparable size with a
substantial gap of 57 and 79 absolute points in micro and macro F1,
respectively. Code, data, and models are available at
https://github.com/epfl-dlab/SynthIE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Marpa and nullable symbols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Kegler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Marpa parser was intended to make the best results in the academic
literature on Earley's algorithm available as a practical general parser.
Earley-based parsers have had issues handling nullable symbols. Initially, we
dealt with nullable symbols by following the approach in Aycock and Horspool's
2002 paper. This paper reports our experience with the approach in that paper,
and the approach to handling nullables that we settled on in reaction to that
experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CroCoSum: A Benchmark <span class="highlight-title">Dataset</span> for Cross-Lingual Code-Switched
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Zhang, Carsten Eickhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual summarization (CLS) has attracted increasing interest in recent
years due to the availability of large-scale web-mined datasets and the
advancements of multilingual language models. However, given the rareness of
naturally occurring CLS resources, the majority of datasets are forced to rely
on translation which can contain overly literal artifacts. This restricts our
ability to observe naturally occurring CLS pairs that capture organic diction,
including instances of code-switching. This alteration between languages in
mid-message is a common phenomenon in multilingual settings yet has been
largely overlooked in cross-lingual contexts due to data scarcity. To address
this gap, we introduce CroCoSum, a dataset of cross-lingual code-switched
summarization of technology news. It consists of over 24,000 English source
articles and 18,000 human-curated Chinese news summaries, with more than 92% of
the summaries containing code-switched phrases. For reference, we evaluate the
performance of existing approaches including pipeline, end-to-end, and
zero-shot methods. We show that leveraging existing resources as a pretraining
step does not improve performance on CroCoSum, indicating the limited
generalizability of existing resources. Finally, we discuss the challenges of
evaluating cross-lingual summarizers on code-switched generation through
qualitative error analyses. Our collection and code can be accessed at
https://github.com/RosenZhang/CroCoSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Abstraction and <span class="highlight-title">Reasoning</span> through Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Camposampiero, Loic Houmard, Benjamin Estermann, Joël Mathys, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Artificial Intelligence (AI) models have achieved human or even
superhuman performance in narrowly defined applications, they still struggle to
show signs of broader and more flexible intelligence. The Abstraction and
Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how
close AI systems are to human-like cognitive abilities. Most current approaches
rely on carefully handcrafted domain-specific languages (DSLs), which are used
to brute-force solutions to the tasks present in ARC. In this work, we propose
a general framework for solving ARC based on natural language descriptions of
the tasks. While not yet beating state-of-the-art DSL models on ARC, we
demonstrate the immense potential of our approach hinted at by the ability to
solve previously unsolved tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors have contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Describe me an Aucklet: Generating Grounded Perceptual Category
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bill Noble, Nikolai Ilinykh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language users can generate descriptions of perceptual concepts beyond
instance-level representations and also use such descriptions to learn
provisional class-level representations. However, the ability of computational
models to learn and operate with class representations is under-investigated in
the language-and-vision field. In this paper, we train separate neural networks
to generate and interpret class-level descriptions. We then use the zero-shot
classification performance of the interpretation model as a measure of
communicative success and class-level conceptual grounding. We investigate the
performance of prototype- and exemplar-based neural representations grounded
category description. Finally, we show that communicative success reveals
performance issues in the generation model that are not captured by traditional
intrinsic NLG evaluation metrics, and argue that these issues can be traced to
a failure to properly ground language in vision at the class level. We observe
that the interpretation model performs better with descriptions that are low in
diversity on the class level, possibly indicating a strong reliance on
frequently occurring features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is <span class="highlight-title">ChatGPT</span> a Good NLG Evaluator? A Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the emergence of ChatGPT has attracted wide attention from the
computational linguistics community. Many prior studies have shown that ChatGPT
achieves remarkable performance on various NLP tasks in terms of automatic
evaluation metrics. However, the ability of ChatGPT to serve as an evaluation
metric is still underexplored. Considering assessing the quality of NLG models
is an arduous task and previous statistical metrics notoriously show their poor
correlation with human judgments, we wonder whether ChatGPT is a good NLG
evaluation metric. In this report, we provide a preliminary meta-evaluation on
ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT
as a human evaluator and give task-specific (e.g., summarization) and
aspect-specific (e.g., relevance) instruction to prompt ChatGPT to score the
generation of NLG models. We conduct experiments on three widely-used NLG
meta-evaluation datasets (including summarization, story generation and
data-to-text tasks). Experimental results show that compared with previous
automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation
with golden human judgments. We hope our preliminary study could prompt the
emergence of a general-purposed reliable NLG metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELODIN: Naming Concepts in Embedding Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mello, Filipe Calegario, Geber Ramalho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements, the field of text-to-image synthesis still
suffers from lack of fine-grained control. Using only text, it remains
challenging to deal with issues such as concept coherence and concept
contamination. We propose a method to enhance control by generating specific
concepts that can be reused throughout multiple images, effectively expanding
natural language with new words that can be combined much like a painter's
palette. Unlike previous contributions, our method does not copy visuals from
input data and can generate concepts through text alone. We perform a set of
comparisons that finds our method to be a significant improvement over
text-only prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GATE: A Challenge Set for Gender-Ambiguous Translation Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Rarrick, Ranjita Naik, Varun Mathur, Sundar Poudel, Vishal Chowdhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although recent years have brought significant progress in improving
translation of unambiguously gendered sentences, translation of ambiguously
gendered input remains relatively unexplored. When source gender is ambiguous,
machine translation models typically default to stereotypical gender roles,
perpetuating harmful bias. Recent work has led to the development of "gender
rewriters" that generate alternative gender translations on such ambiguous
inputs, but such systems are plagued by poor linguistic coverage. To encourage
better performance on this task we present and release GATE, a linguistically
diverse corpus of gender-ambiguous source sentences along with multiple
alternative target language translations. We also provide tools for evaluation
and system analysis when using GATE and use them to evaluate our translation
rewriter system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">ChatGPT</span>: Beginning of an End of Manual Annotation? Use Case of Automatic
  Genre Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taja Kuzman, Nikola Ljubešić, Igor Mozetič
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has shown strong capabilities in natural language generation tasks,
which naturally leads researchers to explore where its abilities end. In this
paper, we examine whether ChatGPT can be used for zero-shot text
classification, more specifically, automatic genre identification. We compare
ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on
datasets, manually annotated with genres. The models are compared on test sets
in two languages: English and Slovenian. Results show that ChatGPT outperforms
the fine-tuned model when applied to the dataset which was not seen before by
either of the models. Even when applied on Slovenian language as an
under-resourced language, ChatGPT's performance is no worse than when applied
to English. However, if the model is fully prompted in Slovenian, the
performance drops significantly, showing the current limitations of ChatGPT
usage on smaller languages. The presented results lead us to questioning
whether this is the beginning of an end of laborious manual annotation
campaigns even for smaller languages, such as Slovenian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Meta-Evaluation of <span class="highlight-title">Faith</span>fulness Metrics for Long-Form Hospital-Course
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Griffin Adams, Jason Zucker, Noémie Elhadad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form clinical summarization of hospital admissions has real-world
significance because of its potential to help both clinicians and patients. The
faithfulness of summaries is critical to their safe usage in clinical settings.
To better understand the limitations of abstractive systems, as well as the
suitability of existing evaluation metrics, we benchmark faithfulness metrics
against fine-grained human annotations for model-generated summaries of a
patient's Brief Hospital Course. We create a corpus of patient hospital
admissions and summaries for a cohort of HIV patients, each with complex
medical histories. Annotators are presented with summaries and source notes,
and asked to categorize manually highlighted summary elements (clinical
entities like conditions and medications as well as actions like "following
up") into one of three categories: ``Incorrect,'' ``Missing,'' and ``Not in
Notes.'' We meta-evaluate a broad set of proposed faithfulness metrics and,
across metrics, explore the importance of domain adaptation (e.g. the impact of
in-domain pre-training and metric fine-tuning), the use of source-summary
alignments, and the effects of distilling a single metric from an ensemble of
pre-existing metrics. Off-the-shelf metrics with no exposure to clinical text
correlate well yet overly rely on summary extractiveness. As a practical guide
to long-form clinical narrative summarization, we find that most metrics
correlate best to human judgments when provided with one summary sentence at a
time and a minimal set of relevant source context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec
  Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a cross-lingual neural codec language model, VALL-E X, for
cross-lingual speech synthesis. Specifically, we extend VALL-E and train a
multi-lingual conditional codec language model to predict the acoustic token
sequences of the target language speech by using both the source language
speech and the target language text as prompts. VALL-E X inherits strong
in-context learning capabilities and can be applied for zero-shot cross-lingual
text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
Experimental results show that it can generate high-quality speech in the
target language via just one speech utterance in the source language as a
prompt while preserving the unseen speaker's voice, emotion, and acoustic
environment. Moreover, VALL-E X effectively alleviates the foreign accent
problems, which can be controlled by a language ID. Audio samples are available
at \url{https://aka.ms/vallex}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We encourage readers to listen to the audio samples on our demo page:
  \url{https://aka.ms/vallex}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual <span class="highlight-title">Dataset</span> <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Muñoz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, Yacine Jernite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models grow ever larger, the need for large-scale high-quality
text datasets has never been more pressing, especially in multilingual
settings. The BigScience workshop, a 1-year international and multidisciplinary
initiative, was formed with the goal of researching and training large language
models as a values-driven undertaking, putting issues of ethics, harm, and
governance in the foreground. This paper documents the data creation and
curation efforts undertaken by BigScience to assemble the Responsible
Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset
spanning 59 languages that was used to train the 176-billion-parameter
BigScience Large Open-science Open-access Multilingual (BLOOM) language model.
We further release a large initial subset of the corpus and analyses thereof,
and hope to empower large-scale monolingual and multilingual modeling projects
with both the data and the processing tools, as well as stimulate research
around this large multilingual corpus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document-level Relation Extraction with Cross-sentence <span class="highlight-title">Reasoning</span> Graph <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfei Liu, Zhao Kang, Lizong Zhang, Ling Tian, Fujun Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) has recently moved from the sentence-level to
document-level, which requires aggregating document information and using
entities and mentions for reasoning. Existing works put entity nodes and
mention nodes with similar representations in a document-level graph, whose
complex edges may incur redundant information. Furthermore, existing studies
only focus on entity-level reasoning paths without considering global
interactions among entities cross-sentence. To these ends, we propose a novel
document-level RE model with a GRaph information Aggregation and Cross-sentence
Reasoning network (GRACR). Specifically, a simplified document-level graph is
constructed to model the semantic information of all mentions and sentences in
a document, and an entity-level graph is designed to explore relations of
long-distance cross-sentence entity pairs. Experimental results show that GRACR
achieves excellent performance on two public datasets of document-level RE. It
is especially effective in extracting potential relations of cross-sentence
entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by PAKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Larger language models do in-context learning differently 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how in-context learning (ICL) in language models is affected by
semantic priors versus input-label mappings. We investigate two setups-ICL with
flipped labels and ICL with semantically-unrelated labels-across various model
families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments
on ICL with flipped labels show that overriding semantic priors is an emergent
ability of model scale. While small language models ignore flipped labels
presented in-context and thus rely primarily on semantic priors from
pretraining, large models can override semantic priors when presented with
in-context exemplars that contradict priors, despite the stronger semantic
priors that larger models may hold. We next study semantically-unrelated label
ICL (SUL-ICL), in which labels are semantically unrelated to their inputs
(e.g., foo/bar instead of negative/positive), thereby forcing language models
to learn the input-label mappings shown in in-context exemplars in order to
perform the task. The ability to do SUL-ICL also emerges primarily with scale,
and large-enough language models can even perform linear classification in a
SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that
instruction tuning strengthens both the use of semantic priors and the capacity
to learn input-label mappings, but more of the former.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Challenging Benchmark for <span class="highlight-title">Low-Resource</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Wang, Chang Ma, Qingxiu Dong, Lingpeng Kong, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With promising yet saturated results in high-resource settings, low-resource
datasets have gradually become popular benchmarks for evaluating the learning
ability of advanced neural networks (e.g., BigBench, superGLUE). Some models
even surpass humans according to benchmark test results. However, we find that
there exists a set of hard examples in low-resource settings that challenge
neural networks but are not well evaluated, which causes over-estimated
performance. We first give a theoretical analysis on which factors bring the
difficulty of low-resource learning. It then motivate us to propose a
challenging benchmark hardBench to better evaluate the learning ability, which
covers 11 datasets, including 3 computer vision (CV) datasets and 8 natural
language process (NLP) datasets. Experiments on a wide range of models show
that neural networks, even pre-trained language models, have sharp performance
drops on our benchmark, demonstrating the effectiveness on evaluating the
weaknesses of neural networks. On NLP tasks, we surprisingly find that despite
better results on traditional low-resource benchmarks, pre-trained networks,
does not show performance improvements on our benchmarks. These results
demonstrate that there are still a large robustness gap between existing models
and human-level performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Feasibility of <span class="highlight-title">ChatGPT</span> for Event Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Gao, Huan Zhao, Changlong Yu, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction is a fundamental task in natural language processing that
involves identifying and extracting information about events mentioned in text.
However, it is a challenging task due to the lack of annotated data, which is
expensive and time-consuming to obtain. The emergence of large language models
(LLMs) such as ChatGPT provides an opportunity to solve language tasks with
simple prompts without the need for task-specific datasets and fine-tuning.
While ChatGPT has demonstrated impressive results in tasks like machine
translation, text summarization, and question answering, it presents challenges
when used for complex tasks like event extraction. Unlike other tasks, event
extraction requires the model to be provided with a complex set of instructions
defining all event types and their schemas. To explore the feasibility of
ChatGPT for event extraction and the challenges it poses, we conducted a series
of experiments. Our results show that ChatGPT has, on average, only 51.04% of
the performance of a task-specific model such as EEQA in long-tail and complex
scenarios. Our usability testing experiments indicate that ChatGPT is not
robust enough, and continuous refinement of the prompt does not lead to stable
performance improvements, which can result in a poor user experience. Besides,
ChatGPT is highly sensitive to different prompt styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preparing the Vuk'uzenzele and ZA-gov-multilingual South African
  multilingual corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Lastrucci, Isheanesu Dzingirai, Jenalea Rajab, Andani Madodonga, Matimba Shingange, Daniel Njini, Vukosi Marivate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces two multilingual government themed corpora in various
South African languages. The corpora were collected by gathering the South
African Government newspaper (Vuk'uzenzele), as well as South African
government speeches (ZA-gov-multilingual), that are translated into all 11
South African official languages. The corpora can be used for a myriad of
downstream NLP tasks. The corpora were created to allow researchers to study
the language used in South African government publications, with a focus on
understanding how South African government officials communicate with their
constituents.
  In this paper we highlight the process of gathering, cleaning and making
available the corpora. We create parallel sentence corpora for Neural Machine
Translation (NMT) tasks using Language-Agnostic Sentence Representations
(LASER) embeddings. With these aligned sentences we then provide NMT benchmarks
for 9 indigenous languages by fine-tuning a massively multilingual pre-trained
language model. \end{abstra
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal resources for quantum computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Sheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unravelling the source of quantum computing power has been a major goal in
the field of quantum information science. In recent years, the quantum resource
theory (QRT) has been established to characterize various quantum resources,
yet their roles in quantum computing tasks still require investigation. The
so-called universal quantum computing model (UQCM), e.g., the circuit model,
has been the main framework to guide the design of quantum algorithms, creation
of real quantum computers etc. In this work, we combine the study of UQCM
together with QRT. We find on one hand, using QRT can provide a
resource-theoretic characterization of a UQCM, the relation among models and
inspire new ones, and on the other hand, using UQCM offers a framework to apply
resources, study relation among resources and classify them.
  We develop the theory of universal resources in the setting of UQCM, and find
a rich spectrum of UQCMs and the corresponding universal resources. Depending
on a hierarchical structure of resource theories, we find models can be
classified into families. In this work, we study three natural families of
UQCMs in details: the amplitude family, the quasi-probability family, and the
Hamiltonian family. They include some well known models, like the
measurement-based model and adiabatic model, and also inspire new models such
as the contextual model we introduce. Each family contains at least a triplet
of models, and such a succinct structure of families of UQCMs offers a unifying
picture to investigate resources and design models. It also provides a rigorous
framework to resolve puzzles, such as the role of entanglement vs.
interference, and unravel resource-theoretic features of quantum algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying Text-Based Conspiracy Tweets related to COVID-19 using
  Contextualized Word Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Rehman, Rabeeh Ayaz Abbasi, Irfan ul Haq Qureshi, Akmal Saeed Khattak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The FakeNews task in MediaEval 2022 investigates the challenge of finding
accurate and high-performance models for the classification of conspiracy
tweets related to COVID-19. In this paper, we used BERT, ELMO, and their
combination for feature extraction and RandomForest as classifier. The results
show that ELMO performs slightly better than BERT, however their combination at
feature level reduces the performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Multimedia Benchmark Workshop 2022, Bergen, Norway and
  Online, 12-13 January 2023: https://2022.multimediaeval.com/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylometric Detection of AI-<span class="highlight-title">Generate</span>d Text in Twitter Timelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee, Kirill Trapeznikov, Scott Ruston, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in pre-trained language models have enabled convenient
methods for generating human-like text at a large scale. Though these
generation capabilities hold great potential for breakthrough applications, it
can also be a tool for an adversary to generate misinformation. In particular,
social media platforms like Twitter are highly susceptible to AI-generated
misinformation. A potential threat scenario is when an adversary hijacks a
credible user account and incorporates a natural language generator to generate
misinformation. Such threats necessitate automated detectors for AI-generated
tweets in a given user's Twitter timeline. However, tweets are inherently
short, thus making it difficult for current state-of-the-art pre-trained
language model-based detectors to accurately detect at what point the AI starts
to generate tweets in a given Twitter timeline. In this paper, we present a
novel algorithm using stylometric signals to aid detecting AI-generated tweets.
We propose models corresponding to quantifying stylistic changes in human and
AI tweets in two related tasks: Task 1 - discriminate between human and
AI-generated tweets, and Task 2 - detect if and when an AI starts to generate
tweets in a given Twitter timeline. Our extensive experiments demonstrate that
the stylometric features are effective in augmenting the state-of-the-art
AI-generated text detectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoTEVer: Chain of Thought <span class="highlight-title">Prompt</span>ing Annotation Toolkit for Explanation
  Verification <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting enables large language models (LLMs) to
solve complex reasoning tasks by generating an explanation before the final
prediction. Despite it's promising ability, a critical downside of CoT
prompting is that the performance is greatly affected by the factuality of the
generated explanation. To improve the correctness of the explanations,
fine-tuning language models with explanation data is needed. However, there
exists only a few datasets that can be used for such approaches, and no data
collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for
annotating the factual correctness of generated explanations and collecting
revision data of wrong explanations. Furthermore, we suggest several use cases
where the data collected with CoTEVer can be utilized for enhancing the
faithfulness of explanations. Our toolkit is publicly available at
https://github.com/SeungoneKim/CoTEVer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023 Demo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Interpretable and Efficient Automatic Reference-Based
  Summarization Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Alexander R. Fabbri, Yilun Zhao, Pengfei Liu, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability and efficiency are two important considerations for the
adoption of neural automatic metrics. In this work, we develop
strong-performing automatic metrics for reference-based summarization
evaluation, based on a two-stage evaluation pipeline that first extracts basic
information units from one text sequence and then checks the extracted units in
another sequence. The metrics we developed include two-stage metrics that can
provide high interpretability at both the fine-grained unit level and summary
level, and one-stage metrics that achieve a balance between efficiency and
interoperability. We make the developed tools publicly available through a
Python package and GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub Repo: https://github.com/Yale-LILY/AutoACU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Knowledge</span> Distillation between Text and Speech <span class="highlight-title">Pre-train</span>ed
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Ni, Yukun Ma, Wen Wang, Qian Chen, Dianwen Ng, Han Lei, Trung Hieu Nguyen, Chong Zhang, Bin Ma, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning on a massive amount of speech corpus leads to the recent success of
many self-supervised speech models. With knowledge distillation, these models
may also benefit from the knowledge encoded by language models that are
pre-trained on rich sources of texts. The distillation process, however, is
challenging due to the modal disparity between textual and speech embedding
spaces. This paper studies metric-based distillation to align the embedding
space of text and speech with only a small amount of data without modifying the
model structure. Since the semantic and granularity gap between text and speech
has been omitted in literature, which impairs the distillation, we propose the
Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages
text/speech units of variable granularity and prior distributions to achieve
better global and local alignments between text and speech pre-trained models.
We evaluate on three spoken language understanding benchmarks to show that PAD
is more effective in transferring linguistic knowledge than other metric-based
distillation approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADELT: Transpilation Between Deep Learning Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyuan Gong, Jiayi Wang, Alvin Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Adversarial DEep Learning Transpiler (ADELT) for source-to-source
transpilation between deep learning frameworks. Unlike prior approaches, we
decouple the transpilation of code skeletons and the mapping of API keywords
(an API function name or a parameter name). ADELT transpile code skeletons
using few-shot prompting on big language models. Based on contextual embeddings
extracted by a BERT for code, we train aligned API embeddings in a
domain-adversarial setup, upon which we generate a dictionary for keyword
translation. The model is trained on our unlabeled DL corpus from web crawl
data, without using any hand-crafted rules and parallel data. Our method
outperforms state-of-the-art transpilers on multiple transpilation pairs
including PyTorch-Keras and PyTorch-MXNet by 15.9pts and 12.0pts in exact match
scores respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Prosody Transfer Models Transfer Prosody? <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atli Thor Sigurgeirsson, Simon King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some recent models for Text-to-Speech synthesis aim to transfer the prosody
of a reference utterance to the generated target synthetic speech. This is done
by using a learned embedding of the reference utterance, which is used to
condition speech generation. During training, the reference utterance is
identical to the target utterance. Yet, during synthesis, these models are
often used to transfer prosody from a reference that differs from the text or
speaker being synthesized.
  To address this inconsistency, we propose to use a different, but
prosodically-related, utterance during training too. We believe this should
encourage the model to learn to transfer only those characteristics that the
reference and target have in common. If prosody transfer methods do indeed
transfer prosody they should be able to be trained in the way we propose.
However, results show that a model trained under these conditions performs
significantly worse than one trained using the target utterance as a reference.
To explain this, we hypothesize that prosody transfer models do not learn a
transferable representation of prosody, but rather an utterance-level
representation which is highly dependent on both the reference speaker and
reference text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2023, 5 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do <span class="highlight-title">Transformer</span>s Learn Topic Structure: Towards a Mechanistic
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Yuanzhi Li, Andrej Risteski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the successes of transformers across many domains are indisputable,
accurate understanding of the learning mechanics is still largely lacking.
Their capabilities have been probed on benchmarks which include a variety of
structured and reasoning tasks -- but mathematical understanding is lagging
substantially behind. Recent lines of work have begun studying representational
aspects of this question: that is, the size/depth/complexity of attention-based
networks to perform certain tasks. However, there is no guarantee the learning
dynamics will converge to the constructions proposed. In our paper, we provide
fine-grained mechanistic understanding of how transformers learn "semantic
structure", understood as capturing co-occurrence structure of words.
Precisely, we show, through a combination of experiments on synthetic data
modeled by Latent Dirichlet Allocation (LDA), Wikipedia data, and mathematical
analysis that the embedding layer and the self-attention layer encode the
topical structure. In the former case, this manifests as higher average inner
product of embeddings between same-topic words. In the latter, it manifests as
higher average pairwise attention between same-topic words. The mathematical
results involve several assumptions to make the analysis tractable, which we
verify on data, and might be of independent interest as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> of AI-<span class="highlight-title">Generate</span>d Content (AIGC): A History of
  Generative AI from GAN to <span class="highlight-title">ChatGPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant
attention from society. As a result, many individuals have become interested in
related resources and are seeking to uncover the background and secrets behind
its impressive performance. In fact, ChatGPT and other Generative AI (GAI)
techniques belong to the category of Artificial Intelligence Generated Content
(AIGC), which involves the creation of digital content, such as images, music,
and natural language, through AI models. The goal of AIGC is to make the
content creation process more efficient and accessible, allowing for the
production of high-quality content at a faster pace. AIGC is achieved by
extracting and understanding intent information from instructions provided by
human, and generating the content according to its knowledge and the intent
information. In recent years, large-scale models have become increasingly
important in AIGC as they provide better intent extraction and thus, improved
generation results. With the growth of data and the size of the models, the
distribution that the model can learn becomes more comprehensive and closer to
reality, leading to more realistic and high-quality content generation. This
survey provides a comprehensive review on the history of generative models, and
basic components, recent advances in AIGC from unimodal interaction and
multimodal interaction. From the perspective of unimodality, we introduce the
generation tasks and relative models of text and image. From the perspective of
multimodality, we introduce the cross-application between the modalities
mentioned above. Finally, we discuss the existing open problems and future
challenges in AIGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemEval-2023 Task 10: Explainable Detection of Online Sexism <span class="chip">SemEval-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Rose Kirk, Wenjie Yin, Bertie Vidgen, Paul Röttger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online sexism is a widespread and harmful phenomenon. Automated tools can
assist the detection of sexism at scale. Binary detection, however, disregards
the diversity of sexist content, and fails to provide clear explanations for
why something is sexist. To address this issue, we introduce SemEval Task 10 on
the Explainable Detection of Online Sexism (EDOS). We make three main
contributions: i) a novel hierarchical taxonomy of sexist content, which
includes granular vectors of sexism to aid explainability; ii) a new dataset of
20,000 social media comments with fine-grained labels, along with larger
unlabelled datasets for model adaptation; and iii) baseline models as well as
an analysis of the methods, results and errors for participant submissions to
our task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SemEval-2023 Task 10 (ACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-Free Structured Pruning with Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azade Nova, Hanjun Dai, Dale Schuurmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can large language models build causal graphs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephanie Long, Tibor Schuster, Alexandre Piché, Department of Family Medicine, McGill University,  Mila, Université de Montreal, ServiceNow Research
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building causal graphs can be a laborious process. To ensure all relevant
causal pathways have been captured, researchers often have to discuss with
clinicians and experts while also reviewing extensive relevant medical
literature. By encoding common and medical knowledge, large language models
(LLMs) represent an opportunity to ease this process by automatically scoring
edges (i.e., connections between two variables) in potential graphs. LLMs
however have been shown to be brittle to the choice of probing words, context,
and prompts that the user employs. In this work, we evaluate if LLMs can be a
useful tool in complementing causal graph development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Accurate Materials Data from Research Papers with
  <span class="highlight-title">Conversation</span>al Language Models and <span class="highlight-title">Prompt</span> Engineering -- Example of <span class="highlight-title">ChatGPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej P. Polak, Dane Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a growing effort to replace hand extraction of data from
research papers with automated data extraction based on natural language
processing (NLP), language models (LMs), and recently, large language models
(LLMs). Although these methods enable efficient extraction of data from large
sets of research papers, they require a significant amount of up-front effort,
expertise, and coding. In this work we propose the ChatExtract method that can
fully automate very accurate data extraction with essentially no initial effort
or background using an advanced conversational LLM (or AI). ChatExtract
consists of a set of engineered prompts applied to a conversational LLM that
both identify sentences with data, extract data, and assure its correctness
through a series of follow-up questions. These follow-up questions address a
critical challenge associated with LLMs - their tendency to provide factually
inaccurate responses. ChatExtract can be applied with any conversational LLMs
and yields very high quality data extraction. In tests on materials data we
find precision and recall both over 90% from the best conversational LLMs,
likely rivaling or exceeding human accuracy in many cases. We demonstrate that
the exceptional performance is enabled by the information retention in a
conversational model combined with purposeful redundancy and introducing
uncertainty through follow-up prompts. These results suggest that approaches
similar to ChatExtract, due to their simplicity, transferability and accuracy
are likely to replace other methods of data extraction in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Summarizing Evidence from Clinical Trials: A Prototype
  Highlighting Current Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjana Ramprasad, Denis Jered McInerney, Iain J. Marshal, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TrialsSummarizer, a system that aims to automatically summarize
evidence presented in the set of randomized controlled trials most relevant to
a given query. Building on prior work, the system retrieves trial publications
matching a query specifying a combination of condition, intervention(s), and
outcome(s), and ranks these according to sample size and estimated study
quality. The top-k such studies are passed through a neural multi-document
summarization system, yielding a synopsis of these trials. We consider two
architectures: A standard sequence-to-sequence model based on BART and a
multi-headed architecture intended to provide greater transparency to
end-users. Both models produce fluent and relevant summaries of evidence
retrieved for queries, but their tendency to introduce unsupported statements
render them inappropriate for use in this domain at present. The proposed
architecture may help users verify outputs allowing users to trace generated
tokens back to inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making a Computational Attorney <span class="chip">SDM'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dell Zhang, Frank Schilder, Jack G. Conrad, Masoud Makrehchi, David von Rickenbach, Isabelle Moulinier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This "blue sky idea" paper outlines the opportunities and challenges in data
mining and machine learning involving making a computational attorney -- an
intelligent software agent capable of helping human lawyers with a wide range
of complex high-level legal tasks such as drafting legal briefs for the
prosecution or defense in court. In particular, we discuss what a ChatGPT-like
Large Legal Language Model (L$^3$M) can and cannot do today, which will inspire
researchers with promising short-term and long-term research objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 2023 SIAM International
  Conference on Data Mining (SDM'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disambiguation of Company names via Deep Recurrent Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Basile, Riccardo Crupi, Michele Grasso, Alessandro Mercanti, Daniele Regoli, Simone Scarsi, Shuyi Yang, Andrea Cosentini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Name Entity Disambiguation is the Natural Language Processing task of
identifying textual records corresponding to the same Named Entity, i.e.
real-world entities represented as a list of attributes (names, places,
organisations, etc.). In this work, we face the task of disambiguating
companies on the basis of their written names. We propose a Siamese LSTM
Network approach to extract -- via supervised learning -- an embedding of
company name strings in a (relatively) low dimensional vector space and use
this representation to identify pairs of company names that actually represent
the same company (i.e. the same Entity).
  Given that the manual labelling of string pairs is a rather onerous task, we
analyse how an Active Learning approach to prioritise the samples to be
labelled leads to a more efficient overall learning pipeline.
  With empirical investigations, we show that our proposed Siamese Network
outperforms several benchmark approaches based on standard string matching
algorithms when enough labelled data are available. Moreover, we show that
Active Learning prioritisation is indeed helpful when labelling resources are
limited, and let the learning models reach the out-of-sample performance
saturation with less labelled data with respect to standard (random) data
labelling approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Elsevier. 26 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ German <span class="highlight-title">BERT</span> Model for Legal Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshil Darji, Jelena Mitrović, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of BERT, one of the most popular language models, has led to
improvements in many Natural Language Processing (NLP) tasks. One such task is
Named Entity Recognition (NER) i.e. automatic identification of named entities
such as location, person, organization, etc. from a given text. It is also an
important base step for many NLP tasks such as information extraction and
argumentation mining. Even though there is much research done on NER using BERT
and other popular language models, the same is not explored in detail when it
comes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such
as sentence similarity or NER specifically on legal data. There are only a
handful of models for NER tasks using BERT language models, however, none of
these are aimed at legal documents in German. In this paper, we fine-tune a
popular BERT language model trained on German data (German BERT) on a Legal
Entity Recognition (LER) dataset. To make sure our model is not overfitting, we
performed a stratified 10-fold cross-validation. The results we achieve by
fine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model
used by the authors of the same LER dataset. Finally, we make the model openly
available via HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICAART 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-<span class="highlight-title">Consist</span>ency Improves Chain of Thought <span class="highlight-title">Reasoning</span> in Language Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11171v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11171v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought prompting combined with pre-trained large language models
has achieved encouraging results on complex reasoning tasks. In this paper, we
propose a new decoding strategy, self-consistency, to replace the naive greedy
decoding used in chain-of-thought prompting. It first samples a diverse set of
reasoning paths instead of only taking the greedy one, and then selects the
most consistent answer by marginalizing out the sampled reasoning paths.
Self-consistency leverages the intuition that a complex reasoning problem
typically admits multiple different ways of thinking leading to its unique
correct answer. Our extensive empirical evaluation shows that self-consistency
boosts the performance of chain-of-thought prompting with a striking margin on
a range of popular arithmetic and commonsense reasoning benchmarks, including
GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and
ARC-challenge (+3.9%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023. V2: added PaLM results; V3: added UL2
  results; V4: camera ready version at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition
  and Robust Speech-to-Text Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Anwar, Bowen Shi, Vedanuj Goswami, Wei-Ning Hsu, Juan Pino, Changhan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MuAViC, a multilingual audio-visual corpus for robust speech
recognition and robust speech-to-text translation providing 1200 hours of
audio-visual speech in 9 languages. It is fully transcribed and covers 6
English-to-X translation as well as 6 X-to-English translation directions. To
the best of our knowledge, this is the first open benchmark for audio-visual
speech-to-text translation and the largest open benchmark for multilingual
audio-visual speech recognition. Our baseline results show that MuAViC is
effective for building noise-robust speech recognition and translation models.
We make the corpus available at https://github.com/facebookresearch/muavic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LambdaKG: A Library for <span class="highlight-title">Pre-train</span>ed Language Model-Based <span class="highlight-title">Knowledge</span> Graph
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Zhoubo Li, Xiaohan Wang, Yuqi Zhu, Ningyu Zhang, Jintian Zhang, Siyuan Cheng, Bozhong Tian, Shumin Deng, Feiyu Xiong, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress and the project website is
  https://zjunlp.github.io/project/promptkg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STACC: Code Comment Classification using Sentence<span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Al-Kaswan, Maliheh Izadi, Arie van Deursen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code comments are a key resource for information about software artefacts.
Depending on the use case, only some types of comments are useful. Thus,
automatic approaches to classify these comments have been proposed. In this
work, we address this need by proposing, STACC, a set of
SentenceTransformers-based binary classifiers. These lightweight classifiers
are trained and tested on the NLBSE Code Comment Classification tool
competition dataset, and surpass the baseline by a significant margin,
achieving an average F1 score of 0.74 against the baseline of 0.31, which is an
improvement of 139%. A replication package, as well as the models themselves,
are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComSearch: Equation Searching with Combinatorial Strategy for Solving
  Math Word Problems with Weak Supervision <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianying Liu, Wenyu Guan, Jianhao Shen, Fei Cheng, Sadao Kurohashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies have introduced a weakly-supervised paradigm for solving
math word problems requiring only the answer value annotation. While these
methods search for correct value equation candidates as pseudo labels, they
search among a narrow sub-space of the enormous equation space. To address this
problem, we propose a novel search algorithm with combinatorial strategy
\textbf{ComSearch}, which can compress the search space by excluding
mathematically equivalent equations. The compression allows the searching
algorithm to enumerate all possible equations and obtain high-quality data. We
investigate the noise in the pseudo labels that hold wrong mathematical logic,
which we refer to as the \textit{false-matching} problem, and propose a ranking
model to denoise the pseudo labels. Our approach holds a flexible framework to
utilize two existing supervised math word problem solvers to train pseudo
labels, and both achieve state-of-the-art performance in the weak supervision
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 long paper, 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceive and predict: <span class="highlight-title">self-supervised</span> speech representation based <span class="highlight-title">loss</span>
  functions for speech enhancement <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, William Ravenscroft, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the domain of speech enhancement has explored the use of
self-supervised speech representations to aid in the training of neural speech
enhancement models. However, much of this work focuses on using the deepest or
final outputs of self supervised speech representation models, rather than the
earlier feature encodings. The use of self supervised representations in such a
way is often not fully motivated. In this work it is shown that the distance
between the feature encodings of clean and noisy speech correlate strongly with
psychoacoustically motivated measures of speech quality and intelligibility, as
well as with human Mean Opinion Score (MOS) ratings. Experiments using this
distance as a loss function are performed and improved performance over the use
of STFT spectrogram distance based loss as well as other common loss functions
from speech enhancement literature is demonstrated using objective measures
such as perceptual evaluation of speech quality (PESQ) and short-time objective
intelligibility (STOI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Knowledge</span>-aware Bayesian Co-attention for Multimodal Emotion Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhao, Yu Wang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition is a challenging research area that aims to
fuse different modalities to predict human emotion. However, most existing
models that are based on attention mechanisms have difficulty in learning
emotionally relevant parts on their own. To solve this problem, we propose to
incorporate external emotion-related knowledge in the co-attention based fusion
of pre-trained models. To effectively incorporate this knowledge, we enhance
the co-attention model with a Bayesian attention module (BAM) where a prior
distribution is estimated using the emotion-related knowledge. Experimental
results on the IEMOCAP dataset show that the proposed approach can outperform
several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can discrete information extraction <span class="highlight-title">prompt</span>s generalize across language
  models? <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathanaël Carraz Rakotonirina, Roberto Dessì, Fabio Petroni, Sebastian Riedel, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study whether automatically-induced prompts that effectively extract
information from a language model can also be used, out-of-the-box, to probe
other language models for the same information. After confirming that discrete
prompts induced with the AutoPrompt algorithm outperform manual and semi-manual
prompts on the slot-filling task, we demonstrate a drop in performance for
AutoPrompt prompts learned on a model and tested on another. We introduce a way
to induce prompts by mixing language models at training time that results in
prompts that generalize well across models. We conduct an extensive analysis of
the induced prompts, finding that the more general prompts include a larger
proportion of existing English words and have a less order-dependent and more
uniform distribution of information across their component tokens. Our work
provides preliminary evidence that it's possible to generate discrete prompts
that can be induced once and used with a number of different models, and gives
insights on the properties characterizing such prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dialogue</span> State Distillation Network with Inter-slot Contrastive Learning
  for <span class="highlight-title">Dialogue</span> State Tracking <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Dandan Song, Chong Liu, Siu Cheung Hui, Fei Li, Qiang Ju, Xiaonan He, Jian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In task-oriented dialogue systems, Dialogue State Tracking (DST) aims to
extract users' intentions from the dialogue history. Currently, most existing
approaches suffer from error propagation and are unable to dynamically select
relevant information when utilizing previous dialogue states. Moreover, the
relations between the updates of different slots provide vital clues for DST.
However, the existing approaches rely only on predefined graphs to indirectly
capture the relations. In this paper, we propose a Dialogue State Distillation
Network (DSDN) to utilize relevant information of previous dialogue states and
migrate the gap of utilization between training and testing. Thus, it can
dynamically exploit previous dialogue states and avoid introducing error
propagation simultaneously. Further, we propose an inter-slot contrastive
learning loss to effectively capture the slot co-update relations from dialogue
context. Experiments are conducted on the widely used MultiWOZ 2.0 and MultiWOZ
2.1 datasets. The experimental results show that our proposed model achieves
the state-of-the-art performance for DST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Deep Semantics for Test Completion <span class="chip">ICSE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Nie, Rahul Banerjee, Junyi Jessy Li, Raymond J. Mooney, Milos Gligoric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing tests is a time-consuming yet essential task during software
development. We propose to leverage recent advances in deep learning for text
and code generation to assist developers in writing tests. We formalize the
novel task of test completion to automatically complete the next statement in a
test method based on the context of prior statements and the code under test.
We develop TeCo -- a deep learning model using code semantics for test
completion. The key insight underlying TeCo is that predicting the next
statement in a test method requires reasoning about code execution, which is
hard to do with only syntax-level data that existing code completion models
use. TeCo extracts and uses six kinds of code semantics data, including the
execution result of prior statements and the execution context of the test
method. To provide a testbed for this new task, as well as to evaluate TeCo, we
collect a corpus of 130,934 test methods from 1,270 open-source Java projects.
Our results show that TeCo achieves an exact-match accuracy of 18, which is 29%
higher than the best baseline using syntax-level data only. When measuring
functional correctness of generated next statement, TeCo can generate runnable
code in 29% of the cases compared to 18% obtained by the best baseline.
Moreover, TeCo is significantly better than prior work on test oracle
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper in ICSE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">ChatGPT</span> Assess Human <span class="highlight-title">Persona</span>lities? A General Evaluation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocong Rao, Cyril Leung, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) especially ChatGPT have produced impressive
results in various areas, but their potential human-like psychology is still
largely unexplored. Existing works study the virtual personalities of LLMs but
rarely explore the possibility of analyzing human personalities via LLMs. This
paper presents a generic evaluation framework for LLMs to assess human
personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically,
we first devise unbiased prompts by randomly permuting options in MBTI
questions and adopt the average testing result to encourage more impartial
answer generation. Then, we propose to replace the subject in question
statements to enable flexible queries and assessments on different subjects
from LLMs. Finally, we re-formulate the question instructions in a manner of
correctness evaluation to facilitate LLMs to generate clearer responses. The
proposed framework enables LLMs to flexibly assess personalities of different
groups of people. We further propose three evaluation metrics to measure the
consistency, robustness, and fairness of assessment results from
state-of-the-art LLMs including ChatGPT and InstructGPT. Our experiments reveal
ChatGPT's ability to assess human personalities, and the average results
demonstrate that it can achieve more consistent and fairer assessments in spite
of lower robustness against prompt biases compared with InstructGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our codes are available at https://github.com/Kali-Hac/ChatGPT-MBTI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effects of Parameter Norm Growth During <span class="highlight-title">Transformer</span> Training: Inductive
  Bias from Gradient Descent <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.09697v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.09697v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such "saturated" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at EMNLP 2021. March 7, 2023: Removed irreproducible numbers
  reported in a footnote with erratum note</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Parallelism Tradeoff: Limitations of Log-Precision <span class="highlight-title">Transformer</span>s <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Ashish Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their omnipresence in modern NLP, characterizing the computational
power of transformer neural nets remains an interesting open question. We prove
that transformers whose arithmetic precision is logarithmic in the number of
input tokens (and whose feedforward nets are computable using space linear in
their input) can be simulated by constant-depth logspace-uniform threshold
circuits. This provides insight on the power of transformers using known
results in complexity theory. For example, if $\mathsf L \neq \mathsf P$ (i.e.,
not all poly-time problems can be solved using logarithmic space), then
transformers cannot even accurately solve linear equalities or check membership
in an arbitrary context-free grammar with empty productions. Our result
intuitively emerges from the transformer architecture's high parallelizability.
We thus speculatively introduce the idea of a fundamental parallelism tradeoff:
any model architecture as parallelizable as the transformer will obey
limitations similar to it. Since parallelism is key to training models at
massive scale, this suggests a potential inherent weakness of the scaling
paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TACL. Formerly entitled "Log-Precision Transformers are
  Constant-Depth Threshold Circuits". Updated with minor corrections March 6,
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-Distribution Detection and Selective <span class="highlight-title">Generation</span> for Conditional
  Language Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, Peter J. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms typically assume independent and identically
distributed samples in training and at test time. Much work has shown that
high-performing ML classifiers can degrade significantly and provide
overly-confident, wrong classification predictions, particularly for
out-of-distribution (OOD) inputs. Conditional language models (CLMs) are
predominantly trained to classify the next token in an output sequence, and may
suffer even worse degradation on OOD inputs as the prediction is done
auto-regressively over many steps. Furthermore, the space of potential
low-quality outputs is larger as arbitrary text can be generated and it is
important to know when to trust the generated output. We present a highly
accurate and lightweight OOD detection method for CLMs, and demonstrate its
effectiveness on abstractive summarization and translation. We also show how
our method can be used under the common and realistic setting of distribution
shift for selective generation (analogous to selective prediction for
classification) of high-quality outputs, while automatically abstaining from
low-quality ones, enabling safer deployment of generative language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering large 3D volumes: A sampling-based approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Lang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications of X-ray computed tomography, an unsupervised
segmentation of the reconstructed 3D volumes forms an important step in the
image processing chain for further investigation of the digitized object.
Therefore, the goal is to train a clustering algorithm on the volume, which
produces a voxelwise classification by assigning a cluster index to each voxel.
However, clustering methods, e.g., K-Means, typically have an asymptotic
polynomial runtime with respect to the dataset size, and thus, these techniques
are rarely applicable to large volumes. In this work, we introduce a novel
clustering technique based on random sampling, which allows for the voxelwise
classification of arbitrarily large volumes. The presented method conducts
efficient linear passes over the data to extract a representative random sample
of a fixed size on which the classifier can be trained. Then, a final linear
pass performs the segmentation and assigns a cluster index to each individual
voxel. Quantitative and qualitative evaluations show that excellent results can
be achieved even with a very small sample size. Consequently, the unsupervised
segmentation by means of clustering becomes feasible for arbitrarily large
volumes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Privacy Preserving System for Movie Recommendations using Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Neumann, Andreas Lutz, Karsten Müller, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are employed by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Accordingly, we present a complete recommender system for movie
recommendations, which provides privacy and thus trustworthiness on two levels:
First, it is trained using federated learning and thus is, by its very nature,
privacy-preserving, while still enabling individual users to benefit from
global insights. And second, a novel federated learning scheme, FedQ, is
employed, which not only addresses the problem of non-i.i.d. and small local
datasets, but also prevents input data reconstruction attacks by aggregating
client models early. To reduce the communication overhead, compression is
applied, which significantly reduces the exchanged neural network updates to a
fraction of their original data. We conjecture that it may also improve data
privacy through its lossy quantization stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the ACM TORS Special Issue on Trustworthy Recommender
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Summarizing Evidence from Clinical Trials: A Prototype
  Highlighting Current Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjana Ramprasad, Denis Jered McInerney, Iain J. Marshal, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TrialsSummarizer, a system that aims to automatically summarize
evidence presented in the set of randomized controlled trials most relevant to
a given query. Building on prior work, the system retrieves trial publications
matching a query specifying a combination of condition, intervention(s), and
outcome(s), and ranks these according to sample size and estimated study
quality. The top-k such studies are passed through a neural multi-document
summarization system, yielding a synopsis of these trials. We consider two
architectures: A standard sequence-to-sequence model based on BART and a
multi-headed architecture intended to provide greater transparency to
end-users. Both models produce fluent and relevant summaries of evidence
retrieved for queries, but their tendency to introduce unsupported statements
render them inappropriate for use in this domain at present. The proposed
architecture may help users verify outputs allowing users to trace generated
tokens back to inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Remote Sensing Image Retrieval and Classification with Robust
  Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13392v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13392v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitri Gominski, Valérie Gouet-Brunet, Liming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high resolution remote sensing image analysis are currently
hampered by the difficulty of gathering enough annotated data for training deep
learning methods, giving rise to a variety of small datasets and associated
dataset-specific methods. Moreover, typical tasks such as classification and
retrieval lack a systematic evaluation on standard benchmarks and training
datasets, which make it hard to identify durable and generalizable scientific
contributions. We aim at unifying remote sensing image retrieval and
classification with a new large-scale training and testing dataset, SF300,
including both vertical and oblique aerial images and made available to the
research community, and an associated fine-tuning method. We additionally
propose a new adversarial fine-tuning method for global descriptors. We show
that our framework systematically achieves a boost of retrieval and
classification performance on nine different datasets compared to an ImageNet
pretrained baseline, with currently no other method to compare to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Performance margin with the proposed method is not statistically
  significant. Please refer to http://alegoria.ign.fr/en/SF300_dataset if you
  are interested in the dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LambdaKG: A Library for <span class="highlight-title">Pre-train</span>ed Language Model-Based <span class="highlight-title">Knowledge</span> Graph
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Zhoubo Li, Xiaohan Wang, Yuqi Zhu, Ningyu Zhang, Jintian Zhang, Siyuan Cheng, Bozhong Tian, Shumin Deng, Feiyu Xiong, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress and the project website is
  https://zjunlp.github.io/project/promptkg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, Shirui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale e-commercial platforms in the real-world usually contain various
recommendation scenarios (domains) to meet demands of diverse customer groups.
Multi-Domain Recommendation (MDR), which aims to jointly improve
recommendations on all domains and easily scales to thousands of domains, has
attracted increasing attention from practitioners and researchers. Existing MDR
methods usually employ a shared structure and several specific components to
respectively leverage reusable features and domain-specific information.
However, data distribution differs across domains, making it challenging to
develop a general model that can be applied to all circumstances. Additionally,
during training, shared parameters often suffer from the domain conflict while
specific parameters are inclined to overfitting on data sparsity domains. we
first present a scalable MDR platform served in Taobao that enables to provide
services for thousands of domains without specialists involved. To address the
problems of MDR methods, we propose a novel model agnostic learning framework,
namely MAMDR, for the multi-domain recommendation. Specifically, we first
propose a Domain Negotiation (DN) strategy to alleviate the conflict between
domains. Then, we develop a Domain Regularization (DR) to improve the
generalizability of specific parameters by learning from other domains. We
integrate these components into a unified framework and present MAMDR, which
can be applied to any model structure to perform multi-domain recommendation.
Finally, we present a large-scale implementation of MAMDR in the Taobao
application and construct various public MDR benchmark datasets which can be
used for following studies. Extensive experiments on both benchmark datasets
and industry datasets demonstrate the effectiveness and generalizability of
MAMDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICDE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benign Overfitting for Two-layer ReLU Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Kou, Zixiang Chen, Yuanzhou Chen, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep learning models with great expressive power can be trained to
overfit the training data but still generalize well. This phenomenon is
referred to as benign overfitting. Recently, a few studies have attempted to
theoretically understand benign overfitting in neural networks. However, these
works are either limited to neural networks with smooth activation functions or
to the neural tangent kernel regime. How and when benign overfitting can occur
in ReLU neural networks remains an open problem. In this work, we seek to
answer this question by establishing algorithm-dependent risk bounds for
learning two-layer ReLU convolutional neural networks with label-flipping
noise. We show that, under mild conditions, the neural network trained by
gradient descent can achieve near-zero training loss and Bayes optimal test
risk. Our result also reveals a sharp transition between benign and harmful
overfitting under different conditions on data distribution in terms of test
risk. Experiments on synthetic data back up our theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Scale <span class="highlight-title">Transformer</span>s to Predict Parameters of Diverse ImageNet
  Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Knyazev, Doha Hwang, Simon Lacoste-Julien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining a neural network on a large dataset is becoming a cornerstone in
machine learning that is within the reach of only a few communities with
large-resources. We aim at an ambitious goal of democratizing pretraining.
Towards that goal, we train and release a single neural network that can
predict high quality ImageNet parameters of other neural networks. By using
predicted parameters for initialization we are able to boost training of
diverse ImageNet models available in PyTorch. When transferred to other
datasets, models initialized with predicted parameters also converge faster and
reach competitive final performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/SamsungSAILMontreal/ghn3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Copilot to Pilot: Towards AI Supported Software Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohith Pudari, Neil A. Ernst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-supported programming has arrived, as shown by the introduction and
successes of large language models for code, such as Copilot/Codex
(Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on
programming challenges is now possible. However, software engineering is much
more than solving programming contests. Moving beyond code completion to
AI-supported software engineering will require an AI system that can, among
other things, understand how to avoid code smells, to follow language idioms,
and eventually (maybe!) propose rational software designs. In this study, we
explore the current limitations of AI-supported code completion tools like
Copilot and offer a simple taxonomy for understanding the classification of
AI-supported code completion tools in this space. We first perform an
exploratory study on Copilot's code suggestions for language idioms and code
smells. Copilot does not follow language idioms and avoid code smells in most
of our test scenarios. We then conduct additional investigation to determine
the current boundaries of AI-supported code completion tools like Copilot by
introducing a taxonomy of software abstraction hierarchies where 'basic
programming functionality' such as code compilation and syntax checking is at
the least abstract level, software architecture analysis and design are at the
most abstract level. We conclude by providing a discussion on challenges for
future development of AI-supported code completion tools to reach the design
level of abstraction in our taxonomy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Randomization for Robust, Affordable and Effective Closed-loop
  Control of Soft Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Tiboni, Andrea Protopapa, Tatiana Tommasi, Giuseppe Averta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robots are becoming extremely popular thanks to their intrinsic safety
to contacts and adaptability. However, the potentially infinite number of
Degrees of Freedom makes their modeling a daunting task, and in many cases only
an approximated description is available. This challenge makes reinforcement
learning (RL) based approaches inefficient when deployed on a realistic
scenario, due to the large domain gap between models and the real platform. In
this work, we demonstrate, for the first time, how Domain Randomization (DR)
can solve this problem by enhancing RL policies with: i) a higher robustness
w.r.t. environmental changes; ii) a higher affordability of learned policies
when the target model differs significantly from the training model; iii) a
higher effectiveness of the policy, which can even autonomously learn to
exploit the environment to increase the robot capabilities (environmental
constraints exploitation). Moreover, we introduce a novel algorithmic extension
of previous adaptive domain randomization methods for the automatic inference
of dynamics parameters for deformable objects. We provide results on four
different tasks and two soft robot designs, opening interesting perspectives
for future research on Reinforcement Learning for closed-loop soft robot
control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://andreaprotopapa.github.io/dr-soro/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Asymmetry for Synthetic Training Data <span class="highlight-title">Generation</span>: SynthIE and
  the Case of Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show great potential for synthetic data
generation. This work shows that useful data can be synthetically generated
even for tasks that cannot be solved directly by the LLM: we show that, for
problems with structured outputs, it is possible to prompt an LLM to perform
the task in the opposite direction, to generate plausible text for the target
structure. Leveraging the asymmetry in task difficulty makes it possible to
produce large-scale, high-quality data for complex tasks. We demonstrate the
effectiveness of this approach on closed information extraction, where
collecting ground-truth data is challenging, and no satisfactory dataset exists
to date. We synthetically generate a dataset of 1.8M data points, demonstrate
its superior quality compared to existing datasets in a human evaluation and
use it to finetune small models (220M and 770M parameters). The models we
introduce, SynthIE, outperform existing baselines of comparable size with a
substantial gap of 57 and 79 absolute points in micro and macro F1,
respectively. Code, data, and models are available at
https://github.com/epfl-dlab/SynthIE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundation Models for Decision Making: Problems, Methods, and
  Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, Dale Schuurmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pretrained on diverse data at scale have demonstrated
extraordinary capabilities in a wide range of vision and language tasks. When
such models are deployed in real world environments, they inevitably interface
with other entities and agents. For example, language models are often used to
interact with human beings through dialogue, and visual perception models are
used to autonomously navigate neighborhood streets. In response to these
developments, new paradigms are emerging for training foundation models to
interact with other agents and perform long-term reasoning. These paradigms
leverage the existence of ever-larger datasets curated for multimodal,
multitask, and generalist interaction. Research at the intersection of
foundation models and decision making holds tremendous promise for creating
powerful new systems that can interact effectively across a diverse range of
applications such as dialogue, autonomous driving, healthcare, education, and
robotics. In this manuscript, we examine the scope of foundation models for
decision making, and provide conceptual tools and technical background for
understanding the problem space and exploring new research directions. We
review recent approaches that ground foundation models in practical decision
making applications through a variety of methods such as prompting, conditional
generative modeling, planning, optimal control, and reinforcement learning, and
discuss common challenges and open problems in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wigner kernels: body-ordered equivariant machine learning without a
  basis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Bigi, Sergey N. Pozdnyakov, Michele Ceriotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning models based on a point-cloud representation of a physical
object are ubiquitous in scientific applications and particularly well-suited
to the atomic-scale description of molecules and materials. Among the many
different approaches that have been pursued, the description of local atomic
environments in terms of their neighbor densities has been used widely and very
succesfully. We propose a novel density-based method which involves computing
``Wigner kernels''. These are fully equivariant and body-ordered kernels that
can be computed iteratively with a cost that is independent of the
radial-chemical basis and grows only linearly with the maximum body-order
considered. This is in marked contrast to feature-space models, which comprise
an exponentially-growing number of terms with increasing order of correlations.
We present several examples of the accuracy of models based on Wigner kernels
in chemical applications, for both scalar and tensorial targets, reaching
state-of-the-art accuracy on the popular QM9 benchmark dataset, and we discuss
the broader relevance of these ideas to equivariant geometric machine-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multiplicative Value Function for Safe and Efficient Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Bührer, Zhejun Zhang, Alexander Liniger, Fisher Yu, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An emerging field of sequential decision problems is safe Reinforcement
Learning (RL), where the objective is to maximize the reward while obeying
safety constraints. Being able to handle constraints is essential for deploying
RL agents in real-world environments, where constraint violations can harm the
agent and the environment. To this end, we propose a safe model-free RL
algorithm with a novel multiplicative value function consisting of a safety
critic and a reward critic. The safety critic predicts the probability of
constraint violation and discounts the reward critic that only estimates
constraint-free returns. By splitting responsibilities, we facilitate the
learning task leading to increased sample efficiency. We integrate our approach
into two popular RL algorithms, Proximal Policy Optimization and Soft
Actor-Critic, and evaluate our method in four safety-focused environments,
including classical RL benchmarks augmented with safety constraints and robot
navigation tasks with images and raw Lidar scans as observations. Finally, we
make the zero-shot sim-to-real transfer where a differential drive robot has to
navigate through a cluttered room. Our code can be found at
https://github.com/nikeke19/Safe-Mult-RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repository available at https://github.com/nikeke19/Safe-Mult-RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Validation of a Hospital Digital Twin with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aurangzeb Ahmad, Vijay Chickarmane, Farinaz Ali Sabzpour, Nima Shariari, Taposh Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a surge of interest in developing Digital Twins of
process flows in healthcare to better understand bottlenecks and areas of
improvement. A key challenge is in the validation process. We describe a work
in progress for a digital twin using an agent based simulation model for
determining bed turnaround time for patients in hospitals. We employ a strategy
using machine learning for validating the model and implementing sensitivity
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicted Embedding Power Regression for Large-Scale Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Yang, William Gebhardt, Alexander G. Ororbia, Travis Desell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) inputs can compromise the performance and safety of
real world machine learning systems. While many methods exist for OOD detection
and work well on small scale datasets with lower resolution and few classes,
few methods have been developed for large-scale OOD detection. Existing
large-scale methods generally depend on maximum classification probability,
such as the state-of-the-art grouped softmax method. In this work, we develop a
novel approach that calculates the probability of the predicted class label
based on label distributions learned during the training process. Our method
performs better than current state-of-the-art methods with only a negligible
increase in compute cost. We evaluate our method against contemporary methods
across $14$ datasets and achieve a statistically significant improvement with
respect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introspective Cross-Attention Probing for Lightweight Transfer of
  <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Dukler, Alessandro Achille, Hao Yang, Varsha Vivek, Luca Zancato, Ben Bowman, Avinash Ravichandran, Charless Fowlkes, Ashwin Swaminathan, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InCA, a lightweight method for transfer learning that
cross-attends to any activation layer of a pre-trained model. During training,
InCA uses a single forward pass to extract multiple activations, which are
passed to external cross-attention adapters, trained anew and combined or
selected for downstream tasks. We show that, even when selecting a single
top-scoring adapter, InCA achieves performance comparable to full fine-tuning,
at a cost comparable to fine-tuning just the last layer. For example, with a
cross-attention probe 1.3% the size of a pre-trained ViT-L/16 model, we achieve
performance within 0.2% of the full fine-tuning paragon at 51% training cost of
the baseline, on average across 11 downstream classification tasks. Unlike
other forms of efficient adaptation, InCA does not require backpropagating
through the pre-trained model, thus leaving its execution unaltered at both
training and inference. The versatility of InCA is best illustrated in
fine-grained tasks, which may require accessing information absent in the last
layer but accessible in intermediate layer activations. Since the backbone is
fixed, InCA allows parallel ensembling as well as parallel execution of
multiple tasks. InCA achieves state-of-the-art performance in the
ImageNet-to-Sketch multi-task benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Inception-Residual-Based Architecture with Multi-Objective <span class="highlight-title">Loss</span> for
  Detecting Respiratory Anomalies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dat Ngo, Lam Pham, Huy Phan, Minh Tran, Delaram Jarchi, Sefki Kolozali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a deep learning system applied for detecting anomalies
from respiratory sound recordings. Initially, our system begins with audio
feature extraction using Gammatone and Continuous Wavelet transformation. This
step aims to transform the respiratory sound input into a two-dimensional
spectrogram where both spectral and temporal features are presented. Then, our
proposed system integrates Inception-residual-based backbone models combined
with multi-head attention and multi-objective loss to classify respiratory
anomalies. In this work, we conducted experiments over the benchmark dataset of
SPRSound (The Open-Source SJTU Paediatric Respiratory Sound) proposed by the
IEEE BioCAS 2022 challenge. As regards the Score computed by an average between
the average score and harmonic score, our proposed system gained significant
improvements of 9.7%, 15.8%, 17.0%, and 9.4% in Task 1-1, Task 1-2, Task 2-1,
and Task 2-2 compared to the challenge baseline system. Notably, we achieved
the Top-1 performance in Task 2-1 with the highest Score of 73.7%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mastering Strategy Card Game (Legends of Code and Magic) via End-to-End
  Policy and Optimistic Smooth Fictitious Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xi, Yongxin Zhang, Changnan Xiao, Xuefeng Huang, Shihong Deng, Haowei Liang, Jie Chen, Peng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning combined with Fictitious Play shows impressive
results on many benchmark games, most of which are, however, single-stage. In
contrast, real-world decision making problems may consist of multiple stages,
where the observation spaces and the action spaces can be completely different
across stages. We study a two-stage strategy card game Legends of Code and
Magic and propose an end-to-end policy to address the difficulties that arise
in multi-stage game. We also propose an optimistic smooth fictitious play
algorithm to find the Nash Equilibrium for the two-player game. Our approach
wins double championships of COG2022 competition. Extensive studies verify and
show the advancement of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Abstraction and <span class="highlight-title">Reasoning</span> through Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Camposampiero, Loic Houmard, Benjamin Estermann, Joël Mathys, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Artificial Intelligence (AI) models have achieved human or even
superhuman performance in narrowly defined applications, they still struggle to
show signs of broader and more flexible intelligence. The Abstraction and
Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how
close AI systems are to human-like cognitive abilities. Most current approaches
rely on carefully handcrafted domain-specific languages (DSLs), which are used
to brute-force solutions to the tasks present in ARC. In this work, we propose
a general framework for solving ARC based on natural language descriptions of
the tasks. While not yet beating state-of-the-art DSL models on ARC, we
demonstrate the immense potential of our approach hinted at by the ability to
solve previously unsolved tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors have contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification of Spatiotemporal Travel Demand with
  Probabilistic Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyi Wang, Shenhao Wang, Dingyi Zhuang, Haris Koutsopoulos, Jinhua Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have significantly improved the prediction accuracy of travel
demand using graph neural networks. However, these studies largely ignored
uncertainty that inevitably exists in travel demand prediction. To fill this
gap, this study proposes a framework of probabilistic graph neural networks
(Prob-GNN) to quantify the spatiotemporal uncertainty of travel demand. This
Prob-GNN framework is substantiated by deterministic and probabilistic
assumptions, and empirically applied to the task of predicting the transit and
ridesharing demand in Chicago. We found that the probabilistic assumptions
(e.g. distribution tail, support) have a greater impact on uncertainty
prediction than the deterministic ones (e.g. deep modules, depth). Among the
family of Prob-GNNs, the GNNs with truncated Gaussian and Laplace distributions
achieve the highest performance in transit and ridesharing data. Even under
significant domain shifts, Prob-GNNs can predict the ridership uncertainty in a
stable manner, when the models are trained on pre-COVID data and tested across
multiple periods during and after the COVID-19 pandemic. Prob-GNNs also reveal
the spatiotemporal pattern of uncertainty, which is concentrated on the
afternoon peak hours and the areas with large travel volumes. Overall, our
findings highlight the importance of incorporating randomness into deep
learning for spatiotemporal ridership prediction. Future research should
continue to investigate versatile probabilistic assumptions to capture
behavioral randomness, and further develop methods to quantify uncertainty to
build resilient cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Root Cause Identification for Collective Anomalies in Time Series given
  an Acyclic Summary Causal Graph with Loops <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles K. Assaad, Imad Ez-zejjari, Lei Zan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an approach for identifying the root causes of collective
anomalies given observational time series and an acyclic summary causal graph
which depicts an abstraction of causal relations present in a dynamic system at
its normal regime. The paper first shows how the problem of root cause
identification can be divided into many independent subproblems by grouping
related anomalies using d-separation. Further, it shows how, under this
setting, some root causes can be found directly from the graph and from the
time of appearance of anomalies. Finally, it shows, how the rest of the root
causes can be found by comparing direct causal effects in the normal and in the
anomalous regime. To this end, temporal adaptations of the back-door and the
single-door criterions are introduced. Extensive experiments conducted on both
simulated and real-world datasets demonstrate the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 26th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2023, Valencia, Spain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyXAB -- A Python Library for $\mathcal{X}$-Armed Bandit and Online
  Blackbox Optimization Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Li, Haoze Li, Jean Honorio, Qifan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Python open-source library for $\mathcal{X}$-armed bandit and
online blackbox optimization named PyXAB. PyXAB contains the implementations
for more than 10 $\mathcal{X}$-armed bandit algorithms, such as HOO, StoSOO,
HCT, and the most recent works GPO and VHCT. PyXAB also provides the most
commonly-used synthetic objectives to evaluate the performance of different
algorithms and the various choices of the hierarchical partitions on the
parameter space. The online documentation for PyXAB includes clear instructions
for installation, straight-forward examples, detailed feature descriptions, and
a complete reference of the API. PyXAB is released under the MIT license in
order to encourage both academic and industrial usage. The library can be
directly installed from PyPI with its source code available at
https://github.com/WilliamLwj/PyXAB
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When is Importance Weighting Correction Needed for Covariate Shift
  Adaptation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davit Gogolashvili, Matteo Zecchin, Motonobu Kanagawa, Marios Kountouris, Maurizio Filippone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates when the importance weighting (IW) correction is
needed to address covariate shift, a common situation in supervised learning
where the input distributions of training and test data differ. Classic results
show that the IW correction is needed when the model is parametric and
misspecified. In contrast, recent results indicate that the IW correction may
not be necessary when the model is nonparametric and well-specified. We examine
the missing case in the literature where the model is nonparametric and
misspecified, and show that the IW correction is needed for obtaining the best
approximation of the true unknown function for the test distribution. We do
this by analyzing IW-corrected kernel ridge regression, covering a variety of
settings, including parametric and nonparametric models, well-specified and
misspecified settings, and arbitrary weighting functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Skill Learning from Robotic Control for Generalizable Object
  Manipulation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Lu, Bo Yang, Bing Wang, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in robotic manipulation through reinforcement learning (RL) or
imitation learning (IL) have shown potential for tackling a range of tasks
e.g., opening a drawer or a cupboard. However, these techniques generalize
poorly to unseen objects. We conjecture that this is due to the
high-dimensional action space for joint control. In this paper, we take an
alternative approach and separate the task of learning 'what to do' from 'how
to do it' i.e., whole-body control. We pose the RL problem as one of
determining the skill dynamics for a disembodied virtual manipulator
interacting with articulated objects. The whole-body robotic kinematic control
is optimized to execute the high-dimensional joint motion to reach the goals in
the workspace. It does so by solving a quadratic programming (QP) model with
robotic singularity and kinematic constraints. Our experiments on manipulating
complex articulated objects show that the proposed approach is more
generalizable to unseen objects with large intra-class variations,
outperforming previous approaches. The evaluation results indicate that our
approach generates more compliant robotic motion and outperforms the pure RL
and IL baselines in task success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploration via Epistemic Value Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Schmitt, John Shawe-Taylor, Hado van Hasselt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to efficiently explore in reinforcement learning is an open problem. Many
exploration algorithms employ the epistemic uncertainty of their own value
predictions -- for instance to compute an exploration bonus or upper confidence
bound. Unfortunately the required uncertainty is difficult to estimate in
general with function approximation.
  We propose epistemic value estimation (EVE): a recipe that is compatible with
sequential decision making and with neural network function approximators. It
equips agents with a tractable posterior over all their parameters from which
epistemic value uncertainty can be computed efficiently.
  We use the recipe to derive an epistemic Q-Learning agent and observe
competitive performance on a series of benchmarks. Experiments confirm that the
EVE recipe facilitates efficient exploration in hard exploration tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELODIN: Naming Concepts in Embedding Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mello, Filipe Calegario, Geber Ramalho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements, the field of text-to-image synthesis still
suffers from lack of fine-grained control. Using only text, it remains
challenging to deal with issues such as concept coherence and concept
contamination. We propose a method to enhance control by generating specific
concepts that can be reused throughout multiple images, effectively expanding
natural language with new words that can be combined much like a painter's
palette. Unlike previous contributions, our method does not copy visuals from
input data and can generate concepts through text alone. We perform a set of
comparisons that finds our method to be a significant improvement over
text-only prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group conditional validity via multi-group learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Deng, Navid Ardeshir, Daniel Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of distribution-free conformal prediction and the
criterion of group conditional validity. This criterion is motivated by many
practical scenarios including hidden stratification and group fairness.
Existing methods achieve such guarantees under either restrictive grouping
structure or distributional assumptions, or they are overly-conservative under
heteroskedastic noise. We propose a simple reduction to the problem of
achieving validity guarantees for individual populations by leveraging
algorithms for a problem called multi-group learning. This allows us to port
theoretical guarantees from multi-group learning to obtain obtain sample
complexity guarantees for conformal prediction. We also provide a new algorithm
for multi-group learning for groups with hierarchical structure. Using this
algorithm in our reduction leads to improved sample complexity guarantees with
a simpler predictor structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feihu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the paper, we study a class of nonconvex nonconcave minimax optimization
problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in
$x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition
in $y$. Moreover, we propose a class of enhanced momentum-based gradient
descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic
Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use
various adaptive learning rates in updating the variables $x$ and $y$ without
relying on any global and coordinate-wise adaptive learning rates.
Theoretically, we present an effective convergence analysis framework for our
methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the
best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring
one sample at each loop in finding an $\epsilon$-stationary solution (i.e.,
$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This
manuscript commemorates the mathematician Boris Polyak (1935-2023).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured State Space Models for In-Context Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, Feryal Behbahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured state space sequence (S4) models have recently achieved
state-of-the-art performance on long-range sequence modeling tasks. These
models also have fast inference speeds and parallelisable training, making them
potentially useful in many reinforcement learning settings. We propose a
modification to a variant of S4 that enables us to initialise and reset the
hidden state in parallel, allowing us to tackle reinforcement learning tasks.
We show that our modified architecture runs asymptotically faster than
Transformers and performs better than LSTM models on a simple memory-based
task. Then, by leveraging the model's ability to handle long-range sequences,
we achieve strong performance on a challenging meta-learning task in which the
agent is given a randomly-sampled continuous control environment, combined with
a randomly-sampled linear projection of the environment's observations and
actions. Furthermore, we show the resulting model can adapt to
out-of-distribution held-out tasks. Overall, the results presented in this
paper suggest that the S4 models are a strong contender for the default
architecture used for in-context reinforcement learning
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing 3D deformations between longitudinal daily CBCT acquisitions
  using CNN for head and neck radiotherapy toxicity prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Trung Le, Chulmin Bang, Philippine Cordelle, Daniel Markel, Phuc Felix Nguyen-Tan, Houda Bahig, Samuel Kadoury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive radiotherapy is a growing field of study in cancer treatment due to
it's objective in sparing healthy tissue. The standard of care in several
institutions includes longitudinal cone-beam computed tomography (CBCT)
acquisitions to monitor changes, but have yet to be used to improve tumor
control while managing side-effects. The aim of this study is to demonstrate
the clinical value of pre-treatment CBCT acquired daily during radiation
therapy treatment for head and neck cancers for the downstream task of
predicting severe toxicity occurrence: reactive feeding tube (NG),
hospitalization and radionecrosis. For this, we propose a deformable 3D
classification pipeline that includes a component analyzing the Jacobian matrix
of the deformation between planning CT and longitudinal CBCT, as well as
clinical data. The model is based on a multi-branch 3D residual convolutional
neural network, while the CT to CBCT registration is based on a pair of
VoxelMorph architectures. Accuracies of 85.8% and 75.3% was found for
radionecrosis and hospitalization, respectively, with similar performance as
early as after the first week of treatment. For NG tube risk, performance
improves with increasing the timing of the CBCT fraction, reaching 83.1% after
the $5_{th}$ week of treatment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 2 equations, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diminishing Return of Value Expansion Methods in Model-Based
  Reinforcement Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Palenicek, Michael Lutter, Joao Carvalho, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based reinforcement learning is one approach to increase sample
efficiency. However, the accuracy of the dynamics model and the resulting
compounding error over modelled trajectories are commonly regarded as key
limitations. A natural question to ask is: How much more sample efficiency can
be gained by improving the learned dynamics models? Our paper empirically
answers this question for the class of model-based value expansion methods in
continuous control problems. Value expansion methods should benefit from
increased model accuracy by enabling longer rollout horizons and better value
function approximations. Our empirical study, which leverages oracle dynamics
models to avoid compounding model errors, shows that (1) longer horizons
increase sample efficiency, but the gain in improvement decreases with each
additional expansion step, and (2) the increased model accuracy only marginally
increases the sample efficiency compared to learned models with identical
horizons. Therefore, longer horizons and increased model accuracy yield
diminishing returns in terms of sample efficiency. These improvements in sample
efficiency are particularly disappointing when compared to model-free value
expansion methods. Even though they introduce no computational overhead, we
find their performance to be on-par with model-based value expansion methods.
Therefore, we conclude that the limitation of model-based value expansion
methods is not the model accuracy of the learned models. While higher model
accuracy is beneficial, our experiments show that even a perfect model will not
provide an un-rivalled sample efficiency but that the bottleneck lies
elsewhere.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">ChatGPT</span>: Beginning of an End of Manual Annotation? Use Case of Automatic
  Genre Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taja Kuzman, Nikola Ljubešić, Igor Mozetič
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has shown strong capabilities in natural language generation tasks,
which naturally leads researchers to explore where its abilities end. In this
paper, we examine whether ChatGPT can be used for zero-shot text
classification, more specifically, automatic genre identification. We compare
ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on
datasets, manually annotated with genres. The models are compared on test sets
in two languages: English and Slovenian. Results show that ChatGPT outperforms
the fine-tuned model when applied to the dataset which was not seen before by
either of the models. Even when applied on Slovenian language as an
under-resourced language, ChatGPT's performance is no worse than when applied
to English. However, if the model is fully prompted in Slovenian, the
performance drops significantly, showing the current limitations of ChatGPT
usage on smaller languages. The presented results lead us to questioning
whether this is the beginning of an end of laborious manual annotation
campaigns even for smaller languages, such as Slovenian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Graph Representations <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sadegh Akhondzadeh, Vijay Lingam, Aleksandar Bojchevski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today we have a good theoretical understanding of the representational power
of Graph Neural Networks (GNNs). For example, their limitations have been
characterized in relation to a hierarchy of Weisfeiler-Lehman (WL) isomorphism
tests. However, we do not know what is encoded in the learned representations.
This is our main question. We answer it using a probing framework to quantify
the amount of meaningful information captured in graph representations. Our
findings on molecular datasets show the potential of probing for understanding
the inductive biases of graph-based models. We compare different families of
models and show that transformer-based models capture more chemically relevant
information compared to models based on message passing. We also study the
effect of different design choices such as skip connections and virtual nodes.
We advocate for probing as a useful diagnostic tool for evaluating graph-based
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 figures, AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Momentum-Based Gradient Methods for Bilevel Optimization with
  Nonconvex Lower-Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feihu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization is a popular two-level hierarchical optimization, which
has been widely applied to many machine learning tasks such as hyperparameter
learning, meta learning and continual learning. Although many bilevel
optimization methods recently have been developed, the bilevel methods are not
well studied when the lower-level problem is nonconvex. To fill this gap, in
the paper, we study a class of nonconvex bilevel optimization problems, which
both upper-level and lower-level problems are nonconvex, and the lower-level
problem satisfies Polyak-Lojasiewicz (PL) condition. We propose an efficient
momentum-based gradient bilevel method (MGBiO) to solve these deterministic
problems. Meanwhile, we propose a class of efficient momentum-based stochastic
gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic
problems. Moreover, we provide a useful convergence analysis framework for our
methods. Specifically, under some mild conditions, we prove that our MGBiO
method has a sample (or gradient) complexity of $O(\epsilon^{-2})$ for finding
an $\epsilon$-stationary solution of the deterministic bilevel problems (i.e.,
$\|\nabla F(x)\|\leq \epsilon$), which improves the existing best results by a
factor of $O(\epsilon^{-1})$. Meanwhile, we prove that our MSGBiO and VR-MSGBiO
methods have sample complexities of $\tilde{O}(\epsilon^{-4})$ and
$\tilde{O}(\epsilon^{-3})$, respectively, in finding an $\epsilon$-stationary
solution of the stochastic bilevel problems (i.e., $\mathbb{E}\|\nabla
F(x)\|\leq \epsilon$), which improves the existing best results by a factor of
$O(\epsilon^{-3})$. This manuscript commemorates the mathematician Boris Polyak
(1935 -2023).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Latent Factor Analysis via a Fuzzy PID-Incorporated Stochastic
  Gradient Descent Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Jinli, Yuan Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A high-dimensional and incomplete (HDI) matrix can describe the complex
interactions among numerous nodes in various big data-related applications. A
stochastic gradient descent (SGD)-based latent factor analysis (LFA) model is
remarkably effective in extracting valuable information from an HDI matrix.
However, such a model commonly encounters the problem of slow convergence
because a standard SGD algorithm learns a latent factor relying on the
stochastic gradient of current instance error only without considering past
update information. To address this critical issue, this paper innovatively
proposes a Fuzzy PID-incorporated SGD (FPS) algorithm with two-fold ideas: 1)
rebuilding the instance learning error by considering the past update
information in an efficient way following the principle of PID, and 2)
implementing hyper-parameters and gain parameters adaptation following the
fuzzy rules. With it, an FPS-incorporated LFA model is further achieved for
fast processing an HDI matrix. Empirical studies on six HDI datasets
demonstrate that the proposed FPS-incorporated LFA model significantly
outperforms the state-of-the-art LFA models in terms of computational
efficiency for predicting the missing data of an HDI matrix with competitive
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FFT-based Dynamic Token Mixer for Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Tatsunami, Masato Taki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-head-self-attention (MHSA)-equipped models have achieved notable
performance in computer vision. Their computational complexity is proportional
to quadratic numbers of pixels in input feature maps, resulting in slow
processing, especially when dealing with high-resolution images. New types of
token-mixer are proposed as an alternative to MHSA to circumvent this problem:
an FFT-based token-mixer, similar to MHSA in global operation but with lower
computational complexity. However, despite its attractive properties, the
FFT-based token-mixer has not been carefully examined in terms of its
compatibility with the rapidly evolving MetaFormer architecture. Here, we
propose a novel token-mixer called dynamic filter and DFFormer and CDFFormer,
image recognition models using dynamic filters to close the gaps above.
CDFFormer achieved a Top-1 accuracy of 85.0%, close to the hybrid architecture
with convolution and MHSA. Other wide-ranging experiments and analysis,
including object detection and semantic segmentation, demonstrate that they are
competitive with state-of-the-art architectures; Their throughput and memory
efficiency when dealing with high-resolution image recognition is convolution
and MHSA, not much different from ConvFormer, and far superior to CAFormer. Our
results indicate that the dynamic filter is one of the token-mixer options that
should be seriously considered. The code is available at
https://github.com/okojoalg/dfformer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document-level Relation Extraction with Cross-sentence <span class="highlight-title">Reasoning</span> Graph <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfei Liu, Zhao Kang, Lizong Zhang, Ling Tian, Fujun Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) has recently moved from the sentence-level to
document-level, which requires aggregating document information and using
entities and mentions for reasoning. Existing works put entity nodes and
mention nodes with similar representations in a document-level graph, whose
complex edges may incur redundant information. Furthermore, existing studies
only focus on entity-level reasoning paths without considering global
interactions among entities cross-sentence. To these ends, we propose a novel
document-level RE model with a GRaph information Aggregation and Cross-sentence
Reasoning network (GRACR). Specifically, a simplified document-level graph is
constructed to model the semantic information of all mentions and sentences in
a document, and an entity-level graph is designed to explore relations of
long-distance cross-sentence entity pairs. Experimental results show that GRACR
achieves excellent performance on two public datasets of document-level RE. It
is especially effective in extracting potential relations of cross-sentence
entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by PAKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Client-specific Property Inference against Secure Aggregation in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raouf Kerkouche, Gergely Ács, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has become a widely used paradigm for collaboratively
training a common model among different participants with the help of a central
server that coordinates the training. Although only the model parameters or
other model updates are exchanged during the federated training instead of the
participant's data, many attacks have shown that it is still possible to infer
sensitive information such as membership, property, or outright reconstruction
of participant data. Although differential privacy is considered an effective
solution to protect against privacy attacks, it is also criticized for its
negative effect on utility. Another possible defense is to use secure
aggregation which allows the server to only access the aggregated update
instead of each individual one, and it is often more appealing because it does
not degrade model quality. However, combining only the aggregated updates,
which are generated by a different composition of clients in every round, may
still allow the inference of some client-specific information.
  In this paper, we show that simple linear models can effectively capture
client-specific properties only from the aggregated model updates due to the
linearity of aggregation. We formulate an optimization problem across different
rounds in order to infer a tested property of every client from the output of
the linear models, for example, whether they have a specific sample in their
training data (membership inference) or whether they misbehave and attempt to
degrade the performance of the common model by poisoning attacks. Our
reconstruction technique is completely passive and undetectable. We demonstrate
the efficacy of our approach on several scenarios which shows that secure
aggregation provides very limited privacy guarantees in practice. The source
code will be released upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianMLR: Learning Implicit Class Significance via Calibrated
  Multi-Label Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V. Bugra Yesilkaynak, Emine Dari, Alican Mertan, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-label frameworks only exploit the information deduced from the
bipartition of the labels into a positive and negative set. Therefore, they do
not benefit from the ranking order between positive labels, which is the
concept we introduce in this paper. We propose a novel multi-label ranking
method: GaussianMLR, which aims to learn implicit class significance values
that determine the positive label ranks instead of treating them as of equal
importance, by following an approach that unifies ranking and classification
tasks associated with multi-label ranking. Due to the scarcity of public
datasets, we introduce eight synthetic datasets generated under varying
importance factors to provide an enriched and controllable experimental
environment for this study. On both real-world and synthetic datasets, we carry
out extensive comparisons with relevant baselines and evaluate the performance
on both of the two sub-tasks. We show that our method is able to accurately
learn a representation of the incorporated positive rank order, which is not
only consistent with the ground truth but also proportional to the underlying
information. We strengthen our claims empirically by conducting comprehensive
experimental studies. Code is available at
https://github.com/MrGranddy/GaussianMLR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manually Selecting The Data Function for Supervised Learning of small
  <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Khanjari, Saeid Pourmand, Mohammad Reza Faridrohani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised learning problems may become ill-posed when there is a lack of
information, resulting in unstable and non-unique solutions. However, instead
of solely relying on regularization, initializing an informative ill-posed
operator is akin to posing better questions to achieve more accurate answers.
The Fredholm integral equation of the first kind (FIFK) is a reliable ill-posed
operator that can integrate distributions and prior knowledge as input
information. By incorporating input distributions and prior knowledge, the FIFK
operator can address the limitations of using high-dimensional input
distributions by semi-supervised assumptions, leading to more precise
approximations of the integral operator. Additionally, the FIFK's incorporation
of probabilistic principles can further enhance the accuracy and effectiveness
of solutions. In cases of noisy operator equations and limited data, the FIFK's
flexibility in defining problems using prior information or cross-validation
with various kernel designs is especially advantageous. This capability allows
for detailed problem definitions and facilitates achieving high levels of
accuracy and stability in solutions. In our study, we examined the FIFK through
two different approaches. Firstly, we implemented a semi-supervised assumption
by using the same Fredholm operator kernel and data function kernel and
incorporating unlabeled information. Secondly, we used the MSDF method, which
involves selecting different kernels on both sides of the equation to define
when the mapping kernel is different from the data function kernel. To assess
the effectiveness of the FIFK and the proposed methods in solving ill-posed
problems, we conducted experiments on a real-world dataset. Our goal was to
compare the performance of these methods against the widely used least-squares
method and other comparable methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Organelle-specific segmentation, spatial analysis, and visualization of
  volume electron microscopy <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Deborah Schmidt, Lucas Rieckert, Michele Solimena, Martin Weigert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volume electron microscopy is the method of choice for the in-situ
interrogation of cellular ultrastructure at the nanometer scale. Recent
technical advances have led to a rapid increase in large raw image datasets
that require computational strategies for segmentation and spatial analysis. In
this protocol, we describe a practical and annotation-efficient pipeline for
organelle-specific segmentation, spatial analysis, and visualization of large
volume electron microscopy datasets using freely available, user-friendly
software tools that can be run on a single standard workstation. We
specifically target researchers in the life sciences with limited computational
expertise, who face the following tasks within their volume electron microscopy
projects: i) How to generate 3D segmentation labels for different types of cell
organelles while minimizing manual annotation efforts, ii) how to analyze the
spatial interactions between organelle instances, and iii) how to best
visualize the 3D segmentation results. To meet these demands we give detailed
guidelines for choosing the most efficient segmentation tools for the specific
cell organelle. We furthermore provide easily executable components for spatial
analysis and 3D rendering and bridge compatibility issues between freely
available open-source tools, such that others can replicate our full pipeline
starting from a raw dataset up to the final plots and rendered images. We
believe that our detailed description can serve as a valuable reference for
similar projects requiring special strategies for single- or multiple organelle
analysis which can be achieved with computational resources commonly available
to single-user setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Decentralized Learning be more robust than Federated Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Raynal, Dario Pasquini, Carmela Troncoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized Learning (DL) is a peer--to--peer learning approach that allows
a group of users to jointly train a machine learning model. To ensure
correctness, DL should be robust, i.e., Byzantine users must not be able to
tamper with the result of the collaboration. In this paper, we introduce two
\textit{new} attacks against DL where a Byzantine user can: make the network
converge to an arbitrary model of their choice, and exclude an arbitrary user
from the learning process. We demonstrate our attacks' efficiency against
Self--Centered Clipping, the state--of--the--art robust DL protocol. Finally,
we show that the capabilities decentralization grants to Byzantine users result
in decentralized learning \emph{always} providing less robustness than
federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENTROPY: Environment <span class="highlight-title">Transformer</span> and Offline Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengqin Wang, Meixin Zhu, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based methods provide an effective approach to offline reinforcement
learning (RL). They learn an environmental dynamics model from interaction
experiences and then perform policy optimization based on the learned model.
However, previous model-based offline RL methods lack long-term prediction
capability, resulting in large errors when generating multi-step trajectories.
We address this issue by developing a sequence modeling architecture,
Environment Transformer, which can generate reliable long-horizon trajectories
based on offline datasets. We then propose a novel model-based offline RL
algorithm, ENTROPY, that learns the dynamics model and reward function by
ENvironment TRansformer and performs Offline PolicY optimization. We evaluate
the proposed method on MuJoCo continuous control RL environments. Results show
that ENTROPY performs comparably or better than the state-of-the-art
model-based and model-free offline RL methods and demonstrates more powerful
long-term trajectory prediction capability compared to existing model-based
offline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Multi-aspect Mining of Complex Time-stamped Event Streams <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Nakamura, Yasuko Matsubara, Koki Kawabata, Yuhei Umeda, Yuichiro Wada, Yasushi Sakurai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a huge, online stream of time-evolving events with multiple attributes,
such as online shopping logs: (item, price, brand, time), and local mobility
activities: (pick-up and drop-off locations, time), how can we summarize large,
dynamic high-order tensor streams? How can we see any hidden patterns, rules,
and anomalies? Our answer is to focus on two types of patterns, i.e.,
''regimes'' and ''components'', for which we present CubeScope, an efficient
and effective method over high-order tensor streams. Specifically, it
identifies any sudden discontinuity and recognizes distinct dynamical patterns,
''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also
performs multi-way summarization for all attributes (e.g., item, price, brand,
and time) and discovers hidden ''components'' representing latent groups (e.g.,
item/brand groups) and their relationship. Thanks to its concise but effective
summarization, CubeScope can also detect the sudden appearance of anomalies and
identify the types of anomalies that occur in practice. Our proposed method has
the following properties: (a) Effective: it captures dynamical multi-aspect
patterns, i.e., regimes and components, and statistically summarizes all the
events; (b) General: it is practical for successful application to data
compression, pattern discovery, and anomaly detection on various types of
tensor streams; (c) Scalable: our algorithm does not depend on the length of
the data stream and its dimensionality. Extensive experiments on real datasets
demonstrate that CubeScope finds meaningful patterns and anomalies correctly,
and consistently outperforms the state-of-the-art methods as regards accuracy
and execution speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method
  and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Kotb, Cornelius Weber, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based reinforcement learning (MBRL) with real-time planning has shown
great potential in locomotion and manipulation control tasks. However, the
existing planning methods, such as the Cross-Entropy Method (CEM), do not scale
well to complex high-dimensional environments. One of the key reasons for
underperformance is the lack of exploration, as these planning methods only aim
to maximize the cumulative extrinsic reward over the planning horizon.
Furthermore, planning inside the compact latent space in the absence of
observations makes it challenging to use curiosity-based intrinsic motivation.
We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for
encouraging exploration via curiosity. Our proposed method maximizes the sum of
state-action Q values over the planning horizon, in which these Q values
estimate the future extrinsic and intrinsic reward, hence encouraging reaching
novel observations. In addition, our model uses contrastive representation
learning to efficiently learn latent representations. Experiments on
image-based continuous control tasks from the DeepMind Control suite show that
CCEM is by a large margin more sample-efficient than previous MBRL algorithms
and compares favorably with the best model-free RL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding Pseudo-labels with Uncertainty Estimation for Test-Time
  Adaptation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Litrico, Alessio Del Bue, Pietro Morerio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard Unsupervised Domain Adaptation (UDA) methods assume the availability
of both source and target data during the adaptation. In this work, we
investigate the Test-Time Adaptation (TTA), a specific case of UDA where a
model is adapted to a target domain without access to source data. We propose a
novel approach for the TTA setting based on a loss reweighting strategy that
brings robustness against the noise that inevitably affects the pseudo-labels.
The classification loss is reweighted based on the reliability of the
pseudo-labels that is measured by estimating their uncertainty. Guided by such
reweighting strategy, the pseudo-labels are progressively refined by
aggregating knowledge from neighbouring samples. Furthermore, a self-supervised
contrastive framework is leveraged as a target space regulariser to enhance
such knowledge aggregation. A novel negative pairs exclusion strategy is
proposed to identify and exclude negative pairs made of samples sharing the
same class, even in presence of some noise in the pseudo-labels. Our method
outperforms previous methods on three major benchmarks by a large margin. We
set the new TTA state-of-the-art on VisDA-C and DomainNet with a performance
gain of +1.8\% on both benchmarks and on PACS with +12.3\% in the single-source
setting and +6.6\% in\ multi-target adaptation. Additional analyses demonstrate
that the proposed approach is robust to the noise, which results in
significantly more accurate pseudo-labels compared to state-of-the-art
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hamiltonian Systems with Mono-Implicit Runge-Kutta Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Håkon Noren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical integrators could be used to form interpolation conditions when
training neural networks to approximate the vector field of an ordinary
differential equation (ODE) from data. When numerical one-step schemes such as
the Runge-Kutta methods are used to approximate the temporal discretization of
an ODE with a known vector field, properties such as symmetry and stability are
much studied. Here, we show that using mono-implicit Runge-Kutta methods of
high order allows for accurate training of Hamiltonian neural networks on small
datasets. This is demonstrated by numerical experiments where the Hamiltonian
of the chaotic double pendulum in addition to the Fermi-Pasta-Ulam-Tsingou
system is learned from data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Multi-Camera Collaboration For 3D Human Pose Estimation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Ci, Mickel Liu, Xuehai Pan, Fangwei Zhong, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a multi-agent reinforcement learning (MARL) scheme for
proactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic
human crowds. Traditional fixed-viewpoint multi-camera solutions for human
motion capture (MoCap) are limited in capture space and susceptible to dynamic
occlusions. Active camera approaches proactively control camera poses to find
optimal viewpoints for 3D reconstruction. However, current methods still face
challenges with credit assignment and environment dynamics. To address these
issues, our proposed method introduces a novel Collaborative Triangulation
Contribution Reward (CTCR) that improves convergence and alleviates multi-agent
credit assignment issues resulting from using 3D reconstruction accuracy as the
shared reward. Additionally, we jointly train our model with multiple world
dynamics learning tasks to better capture environment dynamics and encourage
anticipatory behaviors for occlusion avoidance. We evaluate our proposed method
in four photo-realistic UE4 environments to ensure validity and
generalizability. Empirical results show that our method outperforms fixed and
active baselines in various scenarios with different numbers of cameras and
humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks in Vision-Language Image Understanding: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Senior, Gregory Slabaugh, Shanxin Yuan, Luca Rossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D image understanding is a complex problem within Computer Vision, but it
holds the key to providing human level scene comprehension. It goes further
than identifying the objects in an image, and instead it attempts to understand
the scene. Solutions to this problem form the underpinning of a range of tasks,
including image captioning, Visual Question Answering (VQA), and image
retrieval. Graphs provide a natural way to represent the relational arrangement
between objects in an image, and thus in recent years Graph Neural Networks
(GNNs) have become a standard component of many 2D image understanding
pipelines, becoming a core architectural component especially in the VQA group
of tasks. In this survey, we review this rapidly evolving field and we provide
a taxonomy of graph types used in 2D image understanding approaches, a
comprehensive list of the GNN models used in this domain, and a roadmap of
future potential developments. To the best of our knowledge, this is the first
comprehensive survey that covers image captioning, visual question answering,
and image retrieval techniques that focus on using GNNs as the main part of
their architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLT: Conditioned layout <span class="highlight-title">generation</span> with Joint Discrete-Continuous
  Diffusion Layout <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Levi, Eli Brosh, Mykola Mykhailych, Meir Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating visual layouts is an essential ingredient of graphic design. The
ability to condition layout generation on a partial subset of component
attributes is critical to real-world applications that involve user
interaction. Recently, diffusion models have demonstrated high-quality
generative performances in various domains. However, it is unclear how to apply
diffusion models to the natural representation of layouts which consists of a
mix of discrete (class) and continuous (location, size) attributes. To address
the conditioning layout generation problem, we introduce DLT, a joint
discrete-continuous diffusion model. DLT is a transformer-based model which has
a flexible conditioning mechanism that allows for conditioning on any given
subset of all the layout component classes, locations, and sizes. Our method
outperforms state-of-the-art generative models on various layout generation
datasets with respect to different metrics and conditioning settings.
Additionally, we validate the effectiveness of our proposed conditioning
mechanism and the joint continuous-diffusion process. This joint process can be
incorporated into a wide range of mixed discrete-continuous generative tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zeroth-Order Optimization Meets Human Feedback: Provable Learning via
  Ranking Oracles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on a novel optimization problem in which the
objective function is a black-box and can only be evaluated through a ranking
oracle. This problem is common in real-world applications, particularly in
cases where the function is assessed by human judges. Reinforcement Learning
with Human Feedback (RLHF) is a prominent example of such an application, which
is adopted by the recent works
\cite{ouyang2022training,liu2023languages,chatgpt,bai2022training} to improve
the quality of Large Language Models (LLMs) with human guidance. We propose
ZO-RankSGD, a first-of-its-kind zeroth-order optimization algorithm, to solve
this optimization problem with a theoretical guarantee. Specifically, our
algorithm employs a new rank-based random estimator for the descent direction
and is proven to converge to a stationary point. ZO-RankSGD can also be
directly applied to the policy search problem in reinforcement learning when
only a ranking oracle of the episode reward is available. This makes ZO-RankSGD
a promising alternative to existing RLHF methods, as it optimizes in an online
fashion and thus can work without any pre-collected data. Furthermore, we
demonstrate the effectiveness of ZO-RankSGD in a novel application: improving
the quality of images generated by a diffusion generative model with human
ranking feedback. Throughout experiments, we found that ZO-RankSGD can
significantly enhance the detail of generated images with only a few rounds of
human feedback. Overall, our work advances the field of zeroth-order
optimization by addressing the problem of optimizing functions with only
ranking feedback, and offers an effective approach for aligning human and
machine intentions in a wide range of domains. Our code is released here
\url{https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing formation enthalpies through an explainable machine learning
  method: the case of Lanthanide Orthophosphates solid solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Di Napoli, Xinzhe Wu, Thomas Bornhake, Piotr M. Kowalski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, the use of Machine and Deep Learning (MDL) methods in
Condensed Matter physics has seen a steep increase in the number of problems
tackled and methods employed. A number of distinct MDL approaches have been
employed in many different topics; from prediction of materials properties to
computation of Density Functional Theory potentials and inter-atomic force
fields. In many cases the result is a surrogate model which returns promising
predictions but is opaque on the inner mechanisms of its success. On the other
hand, the typical practitioner looks for answers that are explainable and
provide a clear insight on the mechanisms governing a physical phenomena. In
this work, we describe a proposal to use a sophisticated combination of
traditional Machine Learning methods to obtain an explainable model that
outputs an explicit functional formulation for the material property of
interest. We demonstrate the effectiveness of our methodology in deriving a new
highly accurate expression for the enthalpy of formation of solid solutions of
lanthanides orthophosphates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Decision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchao Hu, Li Shen, Ya Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) is a challenging task, whose objective is
to learn policies from static trajectory data without interacting with the
environment. Recently, offline RL has been viewed as a sequence modeling
problem, where an agent generates a sequence of subsequent actions based on a
set of static transition experiences. However, existing approaches that use
transformers to attend to all tokens naively can overlook the dependencies
between different tokens and limit long-term dependency learning. In this
paper, we propose the Graph Decision Transformer (GDT), a novel offline RL
approach that models the input sequence into a causal graph to capture
potential dependencies between fundamentally different concepts and facilitate
temporal and causal relationship learning. GDT uses a graph transformer to
process the graph inputs with relation-enhanced mechanisms, and an optional
sequence transformer to handle fine-grained spatial information in visual
tasks. Our experiments show that GDT matches or surpasses the performance of
state-of-the-art offline RL methods on image-based Atari and OpenAI Gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Precision Machine-Learning Based Indoor Localization with Massive
  MIMO System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoda Tian, Ilayda Yaman, Michiel Sandra, Xuesong Cai, Liang Liu, Fredrik Tufvesson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-precision cellular-based localization is one of the key technologies for
next-generation communication systems. In this paper, we investigate the
potential of applying machine learning (ML) to a massive multiple-input
multiple-output (MIMO) system to enhance localization accuracy. We analyze a
new ML-based localization pipeline that has two parallel fully connected neural
networks (FCNN). The first FCNN takes the instantaneous spatial covariance
matrix to capture angular information, while the second FCNN takes the channel
impulse responses to capture delay information. We fuse the estimated
coordinates of these two FCNNs for further accuracy improvement. To test the
localization algorithm, we performed an indoor measurement campaign with a
massive MIMO testbed at 3.7GHz. In the measured scenario, the proposed pipeline
can achieve centimeter-level accuracy by combining delay and angular
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Dimensional and Multi-Scale Modeling for Speech Separation
  Optimized by Discriminative Learning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxi Mu, Xinyu Yang, Wenjing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has shown advanced performance in speech separation, benefiting
from its ability to capture global features. However, capturing local features
and channel information of audio sequences in speech separation is equally
important. In this paper, we present a novel approach named Intra-SE-Conformer
and Inter-Transformer (ISCIT) for speech separation. Specifically, we design a
new network SE-Conformer that can model audio sequences in multiple dimensions
and scales, and apply it to the dual-path speech separation framework.
Furthermore, we propose Multi-Block Feature Aggregation to improve the
separation effect by selectively utilizing information from the intermediate
blocks of the separation network. Meanwhile, we propose a speaker similarity
discriminative loss to optimize the speech separation model to address the
problem of poor performance when speakers have similar voices. Experimental
results on the benchmark datasets WSJ0-2mix and WHAM! show that ISCIT can
achieve state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Stage Triple-Path Method for Speech Separation in Noisy and
  Reverberant Environments <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxi Mu, Xinyu Yang, Xiangyuan Yang, Wenjing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In noisy and reverberant environments, the performance of deep learning-based
speech separation methods drops dramatically because previous methods are not
designed and optimized for such situations. To address this issue, we propose a
multi-stage end-to-end learning method that decouples the difficult speech
separation problem in noisy and reverberant environments into three
sub-problems: speech denoising, separation, and de-reverberation. The
probability and speed of searching for the optimal solution of the speech
separation model are improved by reducing the solution space. Moreover, since
the channel information of the audio sequence in the time domain is crucial for
speech separation, we propose a triple-path structure capable of modeling the
channel dimension of audio sequences. Experimental results show that the
proposed multi-stage triple-path method can improve the performance of speech
separation models at the cost of little model parameter increment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Bipedal Walking for Humanoids with Current Feedback <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Pratap Singh, Zhaoming Xie, Pierre Gergondet, Fumio Kanehiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep reinforcement learning (RL) based techniques combined
with training in simulation have offered a new approach to developing control
policies for legged robots. However, the application of such approaches to real
hardware has largely been limited to quadrupedal robots with direct-drive
actuators and light-weight bipedal robots with low gear-ratio transmission
systems. Application to life-sized humanoid robots has been elusive due to the
large sim-to-real gap arising from their large size, heavier limbs, and a high
gear-ratio transmission systems. In this paper, we present an approach for
effectively overcoming the sim-to-real gap issue for humanoid robots arising
from inaccurate torque tracking at the actuator level. Our key idea is to
utilize the current feedback from the motors on the real robot, after training
the policy in a simulation environment artificially degraded with poor torque
tracking. Our approach successfully trains an end-to-end policy in simulation
that can be deployed on a real HRP-5P humanoid robot for bipedal locomotion on
challenging terrain. We also perform robustness tests on the RL policy and
compare its performance against a conventional model-based controller for
walking on uneven terrain. YouTube video: https://youtu.be/IeUaSsBRbNY
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2023). YouTube video:
  https://youtu.be/IeUaSsBRbNY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Modeling with Flow-Guided Density Ratio Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvin Heng, Abdul Fatir Ansari, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable
approach to generative modeling which builds on the stale (time-independent)
approximation of the gradient flow of entropy-regularized f-divergences
introduced in DGflow. In DGflow, the intractable time-dependent density ratio
is approximated by a stale estimator given by a GAN discriminator. This is
sufficient in the case of sample refinement, where the source and target
distributions of the flow are close to each other. However, this assumption is
invalid for generation and a naive application of the stale estimator fails due
to the large chasm between the two distributions. FDRL proposes to train a
density ratio estimator such that it learns from progressively improving
samples during the training process. We show that this simple method alleviates
the density chasm problem, allowing FDRL to generate images of dimensions as
high as $128\times128$, as well as outperform existing gradient flow baselines
on quantitative benchmarks. We also show the flexibility of FDRL with two use
cases. First, unconditional FDRL can be easily composed with external
classifiers to perform class-conditional generation. Second, FDRL can be
directly applied to unpaired image-to-image translation with no modifications
needed to the framework. Code is publicly available at
https://github.com/ajrheng/FDRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid quantum-classical convolutional neural network for phytoplankton
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangshang Shi, Zhimin Wang, Ruimin Shang, Yanan Li, Jiaxin Li, Guoqiang Zhong, Yongjian Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The taxonomic composition and abundance of phytoplankton, having direct
impact on marine ecosystem dynamic and global environment change, are listed as
essential ocean variables. Phytoplankton classification is very crucial for
Phytoplankton analysis, but it is very difficult because of the huge amount and
tiny volume of Phytoplankton. Machine learning is the principle way of
performing phytoplankton image classification automatically. When carrying out
large-scale research on the marine phytoplankton, the volume of data increases
overwhelmingly and more powerful computational resources are required for the
success of machine learning algorithms. Recently, quantum machine learning has
emerged as the potential solution for large-scale data processing by harnessing
the exponentially computational power of quantum computer. Here, for the first
time, we demonstrate the feasibility of quantum deep neural networks for
phytoplankton classification. Hybrid quantum-classical convolutional and
residual neural networks are developed based on the classical architectures.
These models make a proper balance between the limited function of the current
quantum devices and the large size of phytoplankton images, which make it
possible to perform phytoplankton classification on the near-term quantum
computers. Better performance is obtained by the quantum-enhanced models
against the classical counterparts. In particular, quantum models converge much
faster than classical ones. The present quantum models are versatile, and can
be applied for various tasks of image classification in the field of marine
science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying Text-Based Conspiracy Tweets related to COVID-19 using
  Contextualized Word Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Rehman, Rabeeh Ayaz Abbasi, Irfan ul Haq Qureshi, Akmal Saeed Khattak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The FakeNews task in MediaEval 2022 investigates the challenge of finding
accurate and high-performance models for the classification of conspiracy
tweets related to COVID-19. In this paper, we used BERT, ELMO, and their
combination for feature extraction and RandomForest as classifier. The results
show that ELMO performs slightly better than BERT, however their combination at
feature level reduces the performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Multimedia Benchmark Workshop 2022, Bergen, Norway and
  Online, 12-13 January 2023: https://2022.multimediaeval.com/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Inference for Neyman-Scott Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkuan Hong, Christian R. Shelton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neyman-Scott processes (NSPs) have been applied across a range of fields to
model points or temporal events with a hierarchy of clusters. Markov chain
Monte Carlo (MCMC) is typically used for posterior sampling in the model.
However, MCMC's mixing time can cause the resulting inference to be slow, and
thereby slow down model learning and prediction. We develop the first
variational inference (VI) algorithm for NSPs, and give two examples of
suitable variational posterior point process distributions. Our method
minimizes the inclusive Kullback-Leibler (KL) divergence for VI to obtain the
variational parameters. We generate samples from the approximate posterior
point processes much faster than MCMC, as we can directly estimate the
approximate posterior point processes without any MCMC steps or gradient
descent. We include synthetic and real-world data experiments that demonstrate
our VI algorithm achieves better prediction performance than MCMC when
computational time is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylometric Detection of AI-<span class="highlight-title">Generate</span>d Text in Twitter Timelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee, Kirill Trapeznikov, Scott Ruston, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in pre-trained language models have enabled convenient
methods for generating human-like text at a large scale. Though these
generation capabilities hold great potential for breakthrough applications, it
can also be a tool for an adversary to generate misinformation. In particular,
social media platforms like Twitter are highly susceptible to AI-generated
misinformation. A potential threat scenario is when an adversary hijacks a
credible user account and incorporates a natural language generator to generate
misinformation. Such threats necessitate automated detectors for AI-generated
tweets in a given user's Twitter timeline. However, tweets are inherently
short, thus making it difficult for current state-of-the-art pre-trained
language model-based detectors to accurately detect at what point the AI starts
to generate tweets in a given Twitter timeline. In this paper, we present a
novel algorithm using stylometric signals to aid detecting AI-generated tweets.
We propose models corresponding to quantifying stylistic changes in human and
AI tweets in two related tasks: Task 1 - discriminate between human and
AI-generated tweets, and Task 2 - detect if and when an AI starts to generate
tweets in a given Twitter timeline. Our extensive experiments demonstrate that
the stylometric features are effective in augmenting the state-of-the-art
AI-generated text detectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAST: Masked Augmentation Subspace Training for Generalizable
  <span class="highlight-title">Self-Supervised</span> Priors <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Huang, Hanlin Goh, Jiatao Gu, Josh Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Self-Supervised Learning (SSL) methods are able to learn feature
representations that are invariant to different data augmentations, which can
then be transferred to downstream tasks of interest. However, different
downstream tasks require different invariances for their best performance, so
the optimal choice of augmentations for SSL depends on the target task. In this
paper, we aim to learn self-supervised features that generalize well across a
variety of downstream tasks (e.g., object classification, detection and
instance segmentation) without knowing any task information beforehand. We do
so by Masked Augmentation Subspace Training (or MAST) to encode in the single
feature space the priors from different data augmentations in a factorized way.
Specifically, we disentangle the feature space into separate subspaces, each
induced by a learnable mask that selects relevant feature dimensions to model
invariance to a specific augmentation. We show the success of MAST in jointly
capturing generalizable priors from different augmentations, using both unique
and shared features across the subspaces. We further show that MAST benefits
from uncertainty modeling to reweight ambiguous samples from strong
augmentations that may cause similarity mismatch in each subspace. Experiments
demonstrate that MAST consistently improves generalization on various
downstream tasks, while being task-agnostic and efficient during SSL. We also
provide interesting insights about how different augmentations are related and
how uncertainty reflects learning difficulty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Deep Learning and Iterative Algorithms for Joint
  Channel Estimation and Signal Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocheng Ju, Haimiao Zhang, Lin Li, Xiao Li, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint channel estimation and signal detection (JCESD) is crucial in wireless
communication systems, but traditional algorithms perform poorly in low
signal-to-noise ratio (SNR) scenarios. Deep learning (DL) methods have been
investigated, but concerns regarding computational expense and lack of
validation in low-SNR settings remain. Hence, the development of a robust and
low-complexity model that can deliver excellent performance across a wide range
of SNRs is highly desirable. In this paper, we aim to establish a benchmark
where traditional algorithms and DL methods are validated on different channel
models, Doppler, and SNR settings. In particular, we propose a new DL model
where the backbone network is formed by unrolling the iterative algorithm, and
the hyperparameters are estimated by hypernetworks. Additionally, we adapt a
lightweight DenseNet to the task of JCESD for comparison. We evaluate different
methods in three aspects: generalization in terms of bit error rate (BER),
robustness, and complexity. Our results indicate that DL approaches outperform
traditional algorithms in the challenging low-SNR setting, while the iterative
algorithm performs better in highSNR settings. Furthermore, the iterative
algorithm is more robust in the presence of carrier frequency offset, whereas
DL methods excel when signals are corrupted by asymmetric Gaussian noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, this work has been submitted to the IEEE for possible
  publication. Code is available at https://github.com/j991222/MIMO_JCESD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Machine Learning Models to Characterize Temporal Evolution of
  Disadvantaged Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Jain, Narmadha Meenu Mohankumar, Heng Wan, Sumitrra Ganguly, Kyle D Wilson, David M Anderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disadvantaged communities (DAC), as defined by the Justice40 initiative of
the Department of Energy (DOE), USA, identifies census tracts across the USA to
determine where benefits of climate and energy investments are or are not
currently accruing. The DAC status not only helps in determining the
eligibility for future Justice40-related investments but is also critical for
exploring ways to achieve equitable distribution of resources. However,
designing inclusive and equitable strategies not just requires a good
understanding of current demographics, but also a deeper analysis of the
transformations that happened in those demographics over the years. In this
paper, machine learning (ML) models are trained on publicly available census
data from recent years to classify the DAC status at the census tracts level
and then the trained model is used to classify DAC status for historical years.
A detailed analysis of the feature and model selection along with the evolution
of disadvantaged communities between 2013 and 2018 is presented in this study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face: Fast, Accurate and Context-Aware Audio Annotation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Mehrdad Morsali, Hoda Mohammadzade, Saeed Bagheri Shouraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a context-aware framework for feature selection and
classification procedures to realize a fast and accurate audio event annotation
and classification. The context-aware design starts with exploring feature
extraction techniques to find an appropriate combination to select a set
resulting in remarkable classification accuracy with minimal computational
effort. The exploration for feature selection also embraces an investigation of
audio Tempo representation, an advantageous feature extraction method missed by
previous works in the environmental audio classification research scope. The
proposed annotation method considers outlier, inlier, and hard-to-predict data
samples to realize context-aware Active Learning, leading to the average
accuracy of 90% when only 15% of data possess initial annotation. Our proposed
algorithm for sound classification obtained average prediction accuracy of
98.05% on the UrbanSound8K dataset. The notebooks containing our source codes
and implementation results are available at https://github.com/gitmehrdad/FACE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECG Classification System for Arrhythmia Detection Using Convolutional
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Odugoudar, Jaskaran Singh Walia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arrhythmia is just one of the many cardiovascular illnesses that have been
extensively studied throughout the years. Using a multi-lead ECG data, this
research describes a deep learning (DL) technique based on a convolutional
neural network (CNN) algorithm to detect cardiovascular arrhythmia in patients.
The suggested CNN model has six layers total, two convolution layers, two
pooling layers, and two fully linked layers within a residual block, in
addition to the input and output layers. In this study, the classification of
the ECG signals into five groups, Left Bundle Branch Block (LBBB), Right Bundle
Branch Block (RBBB), Atrial Premature Contraction (APC), Premature Ventricular
Contraction (PVC), and Normal Beat is the main goal (N). Using the MIT-BIH
arrhythmia dataset, we assessed the suggested technique. The findings show that
our suggested strategy classified 15000 cases with an average accuracy of
98.2%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPool: Motif-Based Graph Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ifte Khairul Islam, Max Khanov, Esra Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural networks (GNNs) have recently become a powerful technique for
many graph-related tasks including graph classification. Current GNN models
apply different graph pooling methods that reduce the number of nodes and edges
to learn the higher-order structure of the graph in a hierarchical way. All
these methods primarily rely on the one-hop neighborhood. However, they do not
consider the higher- order structure of the graph. In this work, we propose a
multi-channel Motif-based Graph Pooling method named (MPool) captures the
higher-order graph structure with motif and local and global graph structure
with a combination of selection and clustering-based pooling operations. As the
first channel, we develop node selection-based graph pooling by designing a
node ranking model considering the motif adjacency of nodes. As the second
channel, we develop cluster-based graph pooling by designing a spectral
clustering model using motif adjacency. As the final layer, the result of each
channel is aggregated into the final graph representation. We perform extensive
experiments on eight benchmark datasets and show that our proposed method shows
better accuracy than the baseline methods for graph classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Membership Inferencing be Refuted? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Kong, Amrita Roy Chowdhury, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference (MI) attack is currently the most popular test for
measuring privacy leakage in machine learning models. Given a machine learning
model, a data point and some auxiliary information, the goal of an MI~attack is
to determine whether the data point was used to train the model. In this work,
we study the reliability of membership inference attacks in practice.
Specifically, we show that a model owner can plausibly refute the result of a
membership inference test on a data point $x$ by constructing a \textit{proof
of repudiation} that proves that the model was trained \textit{without} $x$. We
design efficient algorithms to construct proofs of repudiation for all data
points of the training dataset. Our empirical evaluation demonstrates the
practical feasibility of our algorithm by constructing proofs of repudiation
for popular machine learning models on MNIST and CIFAR-10. Consequently, our
results call for a re-evaluation of the implications of membership inference
attacks in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AHPA: Adaptive Horizontal Pod Autoscaling Systems on Alibaba Cloud
  Container Service for Kubernetes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Zhou, Chaoli Zhang, Lingna Ma, Jing Gu, Huajie Qian, Qingsong Wen, Liang Sun, Peng Li, Zhimin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing resource allocation policy for application instances in
Kubernetes cannot dynamically adjust according to the requirement of business,
which would cause an enormous waste of resources during fluctuations. Moreover,
the emergence of new cloud services puts higher resource management
requirements. This paper discusses horizontal POD resources management in
Alibaba Cloud Container Services with a newly deployed AI algorithm framework
named AHPA -- the adaptive horizontal pod auto-scaling system. Based on a
robust decomposition forecasting algorithm and performance training model, AHPA
offers an optimal pod number adjustment plan that could reduce POD resources
and maintain business stability. Since being deployed in April 2021, this
system has expanded to multiple customer scenarios, including logistics, social
networks, AI audio and video, e-commerce, etc. Compared with the previous
algorithms, AHPA solves the elastic lag problem, increasing CPU usage by 10%
and reducing resource cost by more than 20%. In addition, AHPA can
automatically perform flexible planning according to the predicted business
volume without manual intervention, significantly saving operation and
maintenance costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PreFallKD: Pre-Impact Fall Detection via CNN-ViT <span class="highlight-title">Knowledge</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin-Han Chi, Kai-Chun Liu, Chia-Yeh Hsieh,  Yu-Tsao, Chia-Tai Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fall accidents are critical issues in an aging and aged society. Recently,
many researchers developed pre-impact fall detection systems using deep
learning to support wearable-based fall protection systems for preventing
severe injuries. However, most works only employed simple neural network models
instead of complex models considering the usability in resource-constrained
mobile devices and strict latency requirements. In this work, we propose a
novel pre-impact fall detection via CNN-ViT knowledge distillation, namely
PreFallKD, to strike a balance between detection performance and computational
complexity. The proposed PreFallKD transfers the detection knowledge from the
pre-trained teacher model (vision transformer) to the student model
(lightweight convolutional neural networks). Additionally, we apply data
augmentation techniques to tackle issues of data imbalance. We conduct the
experiment on the KFall public dataset and compare PreFallKD with other
state-of-the-art models. The experiment results show that PreFallKD could boost
the student model during the testing phase and achieves reliable F1-score
(92.66%) and lead time (551.3 ms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoTEVer: Chain of Thought <span class="highlight-title">Prompt</span>ing Annotation Toolkit for Explanation
  Verification <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting enables large language models (LLMs) to
solve complex reasoning tasks by generating an explanation before the final
prediction. Despite it's promising ability, a critical downside of CoT
prompting is that the performance is greatly affected by the factuality of the
generated explanation. To improve the correctness of the explanations,
fine-tuning language models with explanation data is needed. However, there
exists only a few datasets that can be used for such approaches, and no data
collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for
annotating the factual correctness of generated explanations and collecting
revision data of wrong explanations. Furthermore, we suggest several use cases
where the data collected with CoTEVer can be utilized for enhancing the
faithfulness of explanations. Our toolkit is publicly available at
https://github.com/SeungoneKim/CoTEVer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023 Demo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyAD: Memory-efficient anomaly detection for time series data in
  Industrial IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Sun, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring and detecting abnormal events in cyber-physical systems is crucial
to industrial production. With the prevalent deployment of the Industrial
Internet of Things (IIoT), an enormous amount of time series data is collected
to facilitate machine learning models for anomaly detection, and it is of the
utmost importance to directly deploy the trained models on the IIoT devices.
However, it is most challenging to deploy complex deep learning models such as
Convolutional Neural Networks (CNNs) on these memory-constrained IIoT devices
embedded with microcontrollers (MCUs). To alleviate the memory constraints of
MCUs, we propose a novel framework named Tiny Anomaly Detection (TinyAD) to
efficiently facilitate onboard inference of CNNs for real-time anomaly
detection. First, we conduct a comprehensive analysis of depthwise separable
CNNs and regular CNNs for anomaly detection and find that the depthwise
separable convolution operation can reduce the model size by 50-90% compared
with the traditional CNNs. Then, to reduce the peak memory consumption of CNNs,
we explore two complementary strategies, in-place, and patch-by-patch memory
rescheduling, and integrate them into a unified framework. The in-place method
decreases the peak memory of the depthwise convolution by sparing a temporary
buffer to transfer the activation results, while the patch-by-patch method
further reduces the peak memory of layer-wise execution by slicing the input
data into corresponding receptive fields and executing in order. Furthermore,
by adjusting the dimension of convolution filters, these strategies apply to
both univariate time series and multidomain time series features. Extensive
experiments on real-world industrial datasets show that our framework can
reduce peak memory consumption by 2-5x with negligible computation overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Industrial Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Knowledge</span> Distillation between Text and Speech <span class="highlight-title">Pre-train</span>ed
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Ni, Yukun Ma, Wen Wang, Qian Chen, Dianwen Ng, Han Lei, Trung Hieu Nguyen, Chong Zhang, Bin Ma, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning on a massive amount of speech corpus leads to the recent success of
many self-supervised speech models. With knowledge distillation, these models
may also benefit from the knowledge encoded by language models that are
pre-trained on rich sources of texts. The distillation process, however, is
challenging due to the modal disparity between textual and speech embedding
spaces. This paper studies metric-based distillation to align the embedding
space of text and speech with only a small amount of data without modifying the
model structure. Since the semantic and granularity gap between text and speech
has been omitted in literature, which impairs the distillation, we propose the
Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages
text/speech units of variable granularity and prior distributions to achieve
better global and local alignments between text and speech pre-trained models.
We evaluate on three spoken language understanding benchmarks to show that PAD
is more effective in transferring linguistic knowledge than other metric-based
distillation approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADELT: Transpilation Between Deep Learning Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyuan Gong, Jiayi Wang, Alvin Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Adversarial DEep Learning Transpiler (ADELT) for source-to-source
transpilation between deep learning frameworks. Unlike prior approaches, we
decouple the transpilation of code skeletons and the mapping of API keywords
(an API function name or a parameter name). ADELT transpile code skeletons
using few-shot prompting on big language models. Based on contextual embeddings
extracted by a BERT for code, we train aligned API embeddings in a
domain-adversarial setup, upon which we generate a dictionary for keyword
translation. The model is trained on our unlabeled DL corpus from web crawl
data, without using any hand-crafted rules and parallel data. Our method
outperforms state-of-the-art transpilers on multiple transpilation pairs
including PyTorch-Keras and PyTorch-MXNet by 15.9pts and 12.0pts in exact match
scores respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Limits of Indiscriminate Data Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Lu, Gautam Kamth, Yaoliang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indiscriminate data poisoning attacks aim to decrease a model's test accuracy
by injecting a small amount of corrupted training data. Despite significant
interest, existing attacks remain relatively ineffective against modern machine
learning (ML) architectures. In this work, we introduce the notion of model
poisonability as a technical tool to explore the intrinsic limits of data
poisoning attacks. We derive an easily computable threshold to establish and
quantify a surprising phase transition phenomenon among popular ML models: data
poisoning attacks become effective only when the poisoning ratio exceeds our
threshold. Building on existing parameter corruption attacks and refining the
Gradient Canceling attack, we perform extensive experiments to confirm our
theoretical findings, test the predictability of our transition threshold, and
significantly improve existing data poisoning baselines over a range of
datasets and models. Our work highlights the critical role played by the
poisoning ratio, and sheds new insights on existing empirical results, attacks
and mitigation strategies in data poisoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approach to Learning Generalized Audio Representation Through Batch
  Embedding Covariance Regularization and Constant-Q Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, Bhiksha Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose embedding is highly desirable for few-shot even zero-shot
learning in many application scenarios, including audio tasks. In order to
understand representations better, we conducted a thorough error analysis and
visualization of HEAR 2021 submission results. Inspired by the analysis, this
work experiments with different front-end audio preprocessing methods,
including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),
and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover
a more holistic simulation of the frequency information received by the human
auditory system. We tested the models on the suite of HEAR 2021 tasks, which
encompass a broad category of tasks. Preliminary results show (1) the proposed
BECR can incur a more dispersed embedding on the test set, (2) BECR improves
the PaSST model without extra computation complexity, and (3) STFT
preprocessing outperforms CQT in all tasks we tested.
Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Efficient Fuzzy Clustering Method Based on Local Fuzzy
  Granules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Xie, Qiao Deng, Shuyin Xia, Yangzhou Zhao, Guoyin Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the problem of fuzzy clustering has been widely concerned.
The membership iteration of existing methods is mostly considered globally,
which has considerable problems in noisy environments, and iterative
calculations for clusters with a large number of different sample sizes are not
accurate and efficient. In this paper, starting from the strategy of
large-scale priority, the data is fuzzy iterated using granular-balls, and the
membership degree of data only considers the two granular-balls where it is
located, thus improving the efficiency of iteration. The formed fuzzy
granular-balls set can use more processing methods in the face of different
data scenarios, which enhances the practicability of fuzzy clustering
calculations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of and Roadmap for Data Science and Machine Learning for the
  Neuropsychiatric Phenotype of Autism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Washington, Dennis P. Wall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (autism) is a neurodevelopmental delay which affects
at least 1 in 44 children. Like many neurological disorder phenotypes, the
diagnostic features are observable, can be tracked over time, and can be
managed or even eliminated through proper therapy and treatments. Yet, there
are major bottlenecks in the diagnostic, therapeutic, and longitudinal tracking
pipelines for autism and related delays, creating an opportunity for novel data
science solutions to augment and transform existing workflows and provide
access to services for more affected families. Several prior efforts conducted
by a multitude of research labs have spawned great progress towards improved
digital diagnostics and digital therapies for children with autism. We review
the literature of digital health methods for autism behavior quantification
using data science. We describe both case-control studies and classification
systems for digital phenotyping. We then discuss digital diagnostics and
therapeutics which integrate machine learning models of autism-related
behaviors, including the factors which must be addressed for translational use.
Finally, we describe ongoing challenges and potent opportunities for the field
of autism data science. Given the heterogeneous nature of autism and the
complexities of the relevant behaviors, this review contains insights which are
relevant to neurological behavior analysis and digital psychiatry more broadly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning When to Treat Business Processes: Prescriptive Process
  Monitoring with Causal Inference and Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Dasht Bozorgi, Marlon Dumas, Marcello La Rosa, Artem Polyvyanyy, Mahmoud Shoush, Irene Teinemaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing the success rate of a process, i.e. the percentage of cases that
end in a positive outcome, is a recurrent process improvement goal. At runtime,
there are often certain actions (a.k.a. treatments) that workers may execute to
lift the probability that a case ends in a positive outcome. For example, in a
loan origination process, a possible treatment is to issue multiple loan offers
to increase the probability that the customer takes a loan. Each treatment has
a cost. Thus, when defining policies for prescribing treatments to cases,
managers need to consider the net gain of the treatments. Also, the effect of a
treatment varies over time: treating a case earlier may be more effective than
later in a case. This paper presents a prescriptive monitoring method that
automates this decision-making task. The method combines causal inference and
reinforcement learning to learn treatment policies that maximize the net gain.
The method leverages a conformal prediction technique to speed up the
convergence of the reinforcement learning mechanism by separating cases that
are likely to end up in a positive or negative outcome, from uncertain cases.
An evaluation on two real-life datasets shows that the proposed method
outperforms a state-of-the-art baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERUDITE: Human-in-the-Loop IoT for an Adaptive <span class="highlight-title">Persona</span>lized Learning
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Taherisadr, Mohammad Abdullah Al Faruque, Salma Elmalaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the rapid growth in wearable technologies and recent advancement in
machine learning and signal processing, monitoring complex human contexts
becomes feasible, paving the way to develop human-in-the-loop IoT systems that
naturally evolve to adapt to the human and environment state autonomously.
Nevertheless, a central challenge in designing many of these IoT systems arises
from the requirement to infer the human mental state, such as intention,
stress, cognition load, or learning ability. While different human contexts can
be inferred from the fusion of different sensor modalities that can correlate
to a particular mental state, the human brain provides a richer sensor modality
that gives us more insights into the required human context. This paper
proposes ERUDITE, a human-in-the-loop IoT system for the learning environment
that exploits recent wearable neurotechnology to decode brain signals. Through
insights from concept learning theory, ERUDITE can infer the human state of
learning and understand when human learning increases or declines. By
quantifying human learning as an input sensory signal, ERUDITE can provide
adequate personalized feedback to humans in a learning environment to enhance
their learning experience. ERUDITE is evaluated across $15$ participants and
showed that by using the brain signals as a sensor modality to infer the human
learning state and providing personalized adaptation to the learning
environment, the participants' learning performance increased on average by
$26\%$. Furthermore, we showed that ERUDITE can be deployed on an edge-based
prototype to evaluate its practicality and scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>It is under review in the IEEE IoT journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial Time and Private Learning of Unbounded Gaussian Mixture
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamil Arbas, Hassan Ashtiani, Christopher Liaw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of privately estimating the parameters of
$d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this,
we develop a technique to reduce the problem to its non-private counterpart.
This allows us to privatize existing non-private algorithms in a blackbox
manner, while incurring only a small overhead in the sample complexity and
running time. As the main application of our framework, we develop an
$(\varepsilon, \delta)$-differentially private algorithm to learn GMMs using
the non-private algorithm of Moitra and Valiant [MV10] as a blackbox.
Consequently, this gives the first sample complexity upper bound and first
polynomial time algorithm for privately learning GMMs without any boundedness
assumptions on the parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sufficient dimension reduction for feature matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of sufficient dimension reduction for feature
matrices, which arises often in sensor network localization, brain
neuroimaging, and electroencephalography analysis. In general, feature matrices
have both row- and column-wise interpretations and contain structural
information that can be lost with naive vectorization approaches. To address
this, we propose a method called principal support matrix machine (PSMM) for
the matrix sufficient dimension reduction. The PSMM converts the sufficient
dimension reduction problem into a series of classification problems by
dividing the response variables into slices. It effectively utilizes the matrix
structure by finding hyperplanes with rank-1 normal matrix that optimally
separate the sliced responses. Additionally, we extend our approach to the
higher-order tensor case. Our numerical analysis demonstrates that the PSMM
outperforms existing methods and has strong interpretability in real data
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CUDA: Convolution-based Unlearnable <span class="highlight-title">Dataset</span>s <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinu Sankar Sadasivan, Mahdi Soltanolkotabi, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale training of modern deep learning models heavily relies on
publicly available data on the web. This potentially unauthorized usage of
online data leads to concerns regarding data privacy. Recent works aim to make
unlearnable data for deep learning models by adding small, specially designed
noises to tackle this issue. However, these methods are vulnerable to
adversarial training (AT) and/or are computationally heavy. In this work, we
propose a novel, model-free, Convolution-based Unlearnable DAtaset (CUDA)
generation technique. CUDA is generated using controlled class-wise
convolutions with filters that are randomly generated via a private key. CUDA
encourages the network to learn the relation between filters and labels rather
than informative features for classifying the clean data. We develop some
theoretical analysis demonstrating that CUDA can successfully poison Gaussian
mixture data by reducing the clean data performance of the optimal Bayes
classifier. We also empirically demonstrate the effectiveness of CUDA with
various datasets (CIFAR-10, CIFAR-100, ImageNet-100, and Tiny-ImageNet), and
architectures (ResNet-18, VGG-16, Wide ResNet-34-10, DenseNet-121, DeIT,
EfficientNetV2-S, and MobileNetV2). Our experiments show that CUDA is robust to
various data augmentations and training approaches such as smoothing, AT with
different budgets, transfer learning, and fine-tuning. For instance, training a
ResNet-18 on ImageNet-100 CUDA achieves only 8.96$\%$, 40.08$\%$, and 20.58$\%$
clean test accuracies with empirical risk minimization (ERM), $L_{\infty}$ AT,
and $L_{2}$ AT, respectively. Here, ERM on the clean training data achieves a
clean test accuracy of 80.66$\%$. CUDA exhibits unlearnability effect with ERM
even when only a fraction of the training dataset is perturbed. Furthermore, we
also show that CUDA is robust to adaptive defenses designed specifically to
break it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Computer Vision Enabled damage detection model with improved YOLOv5
  based on <span class="highlight-title">Transformer</span> Prediction Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arunabha M. Roy, Jayabrata Bhaduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective:Computer vision-based up-to-date accurate damage classification and
localization are of decisive importance for infrastructure monitoring, safety,
and the serviceability of civil infrastructure. Current state-of-the-art deep
learning (DL)-based damage detection models, however, often lack superior
feature extraction capability in complex and noisy environments, limiting the
development of accurate and reliable object distinction. Method: To this end,
we present DenseSPH-YOLOv5, a real-time DL-based high-performance damage
detection model where DenseNet blocks have been integrated with the backbone to
improve in preserving and reusing critical feature information. Additionally,
convolutional block attention modules (CBAM) have been implemented to improve
attention performance mechanisms for strong and discriminating deep spatial
feature extraction that results in superior detection under various challenging
environments. Moreover, additional feature fusion layers and a Swin-Transformer
Prediction Head (SPH) have been added leveraging advanced self-attention
mechanism for more efficient detection of multiscale object sizes and
simultaneously reducing the computational complexity. Results: Evaluating the
model performance in large-scale Road Damage Dataset (RDD-2018), at a detection
rate of 62.4 FPS, DenseSPH-YOLOv5 obtains a mean average precision (mAP) value
of 85.25 %, F1-score of 81.18 %, and precision (P) value of 89.51 %
outperforming current state-of-the-art models. Significance: The present
research provides an effective and efficient damage localization model
addressing the shortcoming of existing DL-based damage detection models by
providing highly accurate localized bounding box prediction. Current work
constitutes a step towards an accurate and robust automated damage detection
system in real-time in-field applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amplitude-Varying Perturbation for Balancing Privacy and Utility in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yuan, Wei Ni, Ming Ding, Kang Wei, Jun Li, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While preserving the privacy of federated learning (FL), differential privacy
(DP) inevitably degrades the utility (i.e., accuracy) of FL due to model
perturbations caused by DP noise added to model updates. Existing studies have
considered exclusively noise with persistent root-mean-square amplitude and
overlooked an opportunity of adjusting the amplitudes to alleviate the adverse
effects of the noise. This paper presents a new DP perturbation mechanism with
a time-varying noise amplitude to protect the privacy of FL and retain the
capability of adjusting the learning performance. Specifically, we propose a
geometric series form for the noise amplitude and reveal analytically the
dependence of the series on the number of global aggregations and the
$(\epsilon,\delta)$-DP requirement. We derive an online refinement of the
series to prevent FL from premature convergence resulting from excessive
perturbation noise. Another important aspect is an upper bound developed for
the loss function of a multi-layer perceptron (MLP) trained by FL running the
new DP mechanism. Accordingly, the optimal number of global aggregations is
obtained, balancing the learning and privacy. Extensive experiments are
conducted using MLP, supporting vector machine, and convolutional neural
network models on four public datasets. The contribution of the new DP
mechanism to the convergence and accuracy of privacy-preserving FL is
corroborated, compared to the state-of-the-art Gaussian noise mechanism with a
persistent noise amplitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Sample Complexity of Vanilla Model-Based Offline Reinforcement
  Learning with Dependent Samples <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa O. Karabag, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (offline RL) considers problems where learning
is performed using only previously collected samples and is helpful for the
settings in which collecting new data is costly or risky. In model-based
offline RL, the learner performs estimation (or optimization) using a model
constructed according to the empirical transition frequencies. We analyze the
sample complexity of vanilla model-based offline RL with dependent samples in
the infinite-horizon discounted-reward setting. In our setting, the samples
obey the dynamics of the Markov decision process and, consequently, may have
interdependencies. Under no assumption of independent samples, we provide a
high-probability, polynomial sample complexity bound for vanilla model-based
off-policy evaluation that requires partial or uniform coverage. We extend this
result to the off-policy optimization under uniform coverage. As a comparison
to the model-based approach, we analyze the sample complexity of off-policy
evaluation with vanilla importance sampling in the infinite-horizon setting.
Finally, we provide an estimator that outperforms the sample-mean estimator for
almost deterministic dynamics that are prevalent in reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ adaPARL: Adaptive Privacy-Aware Reinforcement Learning for
  Sequential-Decision Making Human-in-the-Loop Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Taherisadr, Stelios Andrew Stavroulakis, Salma Elmalaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) presents numerous benefits compared to rule-based
approaches in various applications. Privacy concerns have grown with the
widespread use of RL trained with privacy-sensitive data in IoT devices,
especially for human-in-the-loop systems. On the one hand, RL methods enhance
the user experience by trying to adapt to the highly dynamic nature of humans.
On the other hand, trained policies can leak the user's private information.
Recent attention has been drawn to designing privacy-aware RL algorithms while
maintaining an acceptable system utility. A central challenge in designing
privacy-aware RL, especially for human-in-the-loop systems, is that humans have
intrinsic variability and their preferences and behavior evolve. The effect of
one privacy leak mitigation can be different for the same human or across
different humans over time. Hence, we can not design one fixed model for
privacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive
approach for privacy-aware RL, especially for human-in-the-loop IoT systems.
adaPARL provides a personalized privacy-utility trade-off depending on human
behavior and preference. We validate the proposed adaPARL on two IoT
applications, namely (i) Human-in-the-Loop Smart Home and (ii)
Human-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on
these two applications validate the generality of adaPARL and its ability to
provide a personalized privacy-utility trade-off. On average, for the first
application, adaPARL improves the utility by $57\%$ over the baseline and by
$43\%$ over randomization. adaPARL also reduces the privacy leak by $23\%$ on
average. For the second application, adaPARL decreases the privacy leak to
$44\%$ before the utility drops by $15\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at CPS-IoT week (IoTDI'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> speech representation learning for keyword-spotting with
  light-weight <span class="highlight-title">transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Gao, Yue Gu, Francesco Caliva, Yuzong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representation learning (S3RL) is revolutionizing the
way we leverage the ever-growing availability of data. While S3RL related
studies typically use large models, we employ light-weight networks to comply
with tight memory of compute-constrained devices. We demonstrate the
effectiveness of S3RL on a keyword-spotting (KS) problem by using transformers
with 330k parameters and propose a mechanism to enhance utterance-wise
distinction, which proves crucial for improving performance on classification
tasks. On the Google speech commands v2 dataset, the proposed method applied to
the Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement
compared to training from scratch. On an in-house KS dataset with four
different keywords, it provided 6% to 23.7% relative false accept improvement
at fixed false reject rate. We argue this demonstrates the applicability of
S3RL approaches to light-weight models for KS and confirms S3RL is a powerful
alternative to traditional supervised learning for resource-constrained
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRACT: Denoising Diffusion Models with Transitive Closure
  Time-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbot, Eric Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion models have demonstrated their proficiency for generative
sampling. However, generating good samples often requires many iterations.
Consequently, techniques such as binary time-distillation (BTD) have been
proposed to reduce the number of network calls for a fixed architecture. In
this paper, we introduce TRAnsitive Closure Time-distillation (TRACT), a new
method that extends BTD. For single step diffusion,TRACT improves FID by up to
2.4x on the same architecture, and achieves new single-step Denoising Diffusion
Implicit Models (DDIM) state-of-the-art FID (7.4 for ImageNet64, 3.8 for
CIFAR10). Finally we tease apart the method through extended ablations. The
PyTorch implementation will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do <span class="highlight-title">Transformer</span>s Learn Topic Structure: Towards a Mechanistic
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Yuanzhi Li, Andrej Risteski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the successes of transformers across many domains are indisputable,
accurate understanding of the learning mechanics is still largely lacking.
Their capabilities have been probed on benchmarks which include a variety of
structured and reasoning tasks -- but mathematical understanding is lagging
substantially behind. Recent lines of work have begun studying representational
aspects of this question: that is, the size/depth/complexity of attention-based
networks to perform certain tasks. However, there is no guarantee the learning
dynamics will converge to the constructions proposed. In our paper, we provide
fine-grained mechanistic understanding of how transformers learn "semantic
structure", understood as capturing co-occurrence structure of words.
Precisely, we show, through a combination of experiments on synthetic data
modeled by Latent Dirichlet Allocation (LDA), Wikipedia data, and mathematical
analysis that the embedding layer and the self-attention layer encode the
topical structure. In the former case, this manifests as higher average inner
product of embeddings between same-topic words. In the latter, it manifests as
higher average pairwise attention between same-topic words. The mathematical
results involve several assumptions to make the analysis tractable, which we
verify on data, and might be of independent interest as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A topological classifier to characterize brain states: When shape
  matters more than variance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aina Ferrà, Gloria Cecchini, Fritz-Pere Nobbe Fisas, Carles Casacuberta, Ignasi Cos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable accuracies attained by machine learning classifiers to
separate complex datasets in a supervised fashion, most of their operation
falls short to provide an informed intuition about the structure of data, and,
what is more important, about the phenomena being characterized by the given
datasets. By contrast, topological data analysis (TDA) is devoted to study the
shape of data clouds by means of persistence descriptors and provides a
quantitative characterization of specific topological features of the dataset
under scrutiny.
  In this article we introduce a novel TDA-based classifier that works on the
principle of assessing quantifiable changes on topological metrics caused by
the addition of new input to a subset of data. We used this classifier with a
high-dimensional electro-encephalographic (EEG) dataset recorded from eleven
participants during a decision-making experiment in which three motivational
states were induced through a manipulation of social pressure. After processing
a band-pass filtered version of EEG signals, we calculated silhouettes from
persistence diagrams associated with each motivated state, and classified
unlabeled signals according to their impact on each reference silhouette. Our
results show that in addition to providing accuracies within the range of those
of a nearest neighbour classifier, the TDA classifier provides formal intuition
of the structure of the dataset as well as an estimate of its intrinsic
dimension. Towards this end, we incorporated dimensionality reduction methods
to our procedure and found that the accuracy of our TDA classifier is generally
not sensitive to explained variance but rather to shape, contrary to what
happens with most machine learning classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Min-Max Bilevel Multi-objective Optimization with Applications in
  Machine Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Gu, Songtao Lu, Parikshit Ram, Lily Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a generic min-max multi-objective bilevel optimization problem
with applications in robust machine learning such as representation learning
and hyperparameter optimization. We design MORBiT, a novel single-loop gradient
descent-ascent bilevel optimization algorithm, to solve the generic problem and
present a novel analysis showing that MORBiT converges to the first-order
stationary point at a rate of $\widetilde{\mathcal{O}}(n^{1/2} K^{-2/5})$ for a
class of weakly convex problems with $n$ objectives upon $K$ iterations of the
algorithm. Our analysis utilizes novel results to handle the non-smooth min-max
multi-objective setup and to obtain a sublinear dependence in the number of
objectives $n$. Experimental results on robust representation learning and
robust hyperparameter optimization showcase (i) the advantages of considering
the min-max multi-objective setup, and (ii) convergence properties of the
proposed MORBiT. Our code is at https://github.com/minimario/MORBiT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 3 figures, ICLR 2023 version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boundary Graph Neural Networks for 3D Simulations <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.11299v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.11299v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Mayr, Sebastian Lehner, Arno Mayrhofer, Christoph Kloss, Sepp Hochreiter, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The abundance of data has given machine learning considerable momentum in
natural sciences and engineering, though modeling of physical processes is
often difficult. A particularly tough problem is the efficient representation
of geometric boundaries. Triangularized geometric boundaries are well
understood and ubiquitous in engineering applications. However, it is
notoriously difficult to integrate them into machine learning approaches due to
their heterogeneity with respect to size and orientation. In this work, we
introduce an effective theory to model particle-boundary interactions, which
leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify
graph structures to obey boundary conditions. The new BGNNs are tested on
complex 3D granular flow processes of hoppers, rotating drums and mixers, which
are all standard components of modern industrial machinery but still have
complicated geometry. BGNNs are evaluated in terms of computational efficiency
as well as prediction accuracy of particle flows and mixing entropies. BGNNs
are able to accurately reproduce 3D granular flows within simulation
uncertainties over hundreds of thousands of simulation timesteps. Most notably,
in our experiments, particles stay within the geometric objects without using
handcrafted conditions or restrictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for presentation at the Thirty-Seventh AAAI Conference on
  Artificial Intelligence (AAAI-23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence under Lipschitz smoothness of ease-controlled Random
  Reshuffling gradient Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giampaolo Liuzzi, Laura Palagi, Ruggiero Seccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimizing the average of a very large number of smooth and
possibly non-convex functions. This optimization problem has deserved much
attention in the past years due to the many applications in different fields,
the most challenging being training Machine Learning models. Widely used
approaches for solving this problem are mini-batch gradient methods which, at
each iteration, update the decision vector moving along the gradient of a
mini-batch of the component functions. We consider the Incremental Gradient
(IG) and the Random reshuffling (RR) methods which proceed in cycles, picking
batches in a fixed order or by reshuffling the order after each epoch.
Convergence properties of these schemes have been proved under different
assumptions, usually quite strong. We aim to define ease-controlled
modifications of the IG/RR schemes, which require a light additional
computational effort and can be proved to converge under very weak and standard
assumptions. In particular, we define two algorithmic schemes, monotone or
non-monotone, in which the IG/RR iteration is controlled by using a watchdog
rule and a derivative-free line search that activates only sporadically to
guarantee convergence. The two schemes also allow controlling the updating of
the stepsize used in the main IG/RR iteration, avoiding the use of preset
rules. We prove convergence under the lonely assumption of Lipschitz continuity
of the gradients of the component functions and perform extensive computational
analysis using Deep Neural Architectures and a benchmark of datasets. We
compare our implementation with both full batch gradient methods and online
standard implementation of IG/RR methods, proving that the computational effort
is comparable with the corresponding online methods and that the control on the
learning rate may allow faster decrease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add references, correct typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimum-statistical Collaboration Towards General and Efficient
  Black-box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.09215v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.09215v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Li, Chi-Hua Wang, Qifan Song, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we make the key delineation on the roles of resolution and
statistical uncertainty in hierarchical bandits-based black-box optimization
algorithms, guiding a more general analysis and a more efficient algorithm
design. We introduce the \textit{optimum-statistical collaboration}, an
algorithm framework of managing the interaction between optimization error flux
and statistical error flux evolving in the optimization process. We provide a
general analysis of this framework without specifying the forms of statistical
error and uncertainty quantifier. Our framework and its analysis, due to their
generality, can be applied to a large family of functions and partitions that
satisfy different local smoothness assumptions and have different numbers of
local optimums, which is much richer than the class of functions studied in
prior works. Our framework also inspires us to propose a better measure of the
statistical uncertainty and consequently a variance-adaptive algorithm
\texttt{VHCT}. In theory, we prove the algorithm enjoys rate-optimal regret
bounds under different local smoothness assumptions; in experiments, we show
the algorithm outperforms prior efforts in different settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prior and Posterior Networks: A <span class="highlight-title">Survey</span> on Evidential Deep Learning
  Methods For Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Ulmer, Christian Hardmeier, Jes Frellsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular approaches for quantifying predictive uncertainty in deep neural
networks often involve distributions over weights or multiple models, for
instance via Markov Chain sampling, ensembling, or Monte Carlo dropout. These
techniques usually incur overhead by having to train multiple model instances
or do not produce very diverse predictions. This comprehensive and extensive
survey aims to familiarize the reader with an alternative class of models based
on the concept of Evidential Deep Learning: For unfamiliar data, they aim to
admit "what they don't know", and fall back onto a prior belief. Furthermore,
they allow uncertainty estimation in a single model and forward pass by
parameterizing distributions over distributions. This survey recapitulates
existing works, focusing on the implementation in a classification setting,
before surveying the application of the same paradigm to regression. We also
reflect on the strengths and weaknesses compared to other existing methods and
provide the most fundamental derivations using a unified notation to aid future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning particle swarming models from data with Gaussian processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02735v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02735v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchao Feng, Charles Kulick, Yunxiang Ren, Sui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interacting particle or agent systems that display a rich variety of swarming
behaviours are ubiquitous in science and engineering. A fundamental and
challenging goal is to understand the link between individual interaction rules
and swarming. In this paper, we study the data-driven discovery of a
second-order particle swarming model that describes the evolution of $N$
particles in $\mathbb{R}^d$ under radial interactions. We propose a learning
approach that models the latent radial interaction function as Gaussian
processes, which can simultaneously fulfill two inference goals: one is the
nonparametric inference of {the} interaction function with pointwise
uncertainty quantification, and the other one is the inference of unknown
scalar parameters in the non-collective friction forces of the system. We
formulate the learning problem as a statistical inverse problem and provide a
detailed analysis of recoverability conditions, establishing that a coercivity
condition is sufficient for recoverability. Given data collected from $M$ i.i.d
trajectories with independent Gaussian observational noise, we provide a
finite-sample analysis, showing that our posterior mean estimator converges in
a Reproducing kernel Hilbert space norm, at an optimal rate in $M$ equal to the
one in the classical 1-dimensional Kernel Ridge regression. As a byproduct, we
show we can obtain a parametric learning rate in $M$ for the posterior marginal
variance using $L^{\infty}$ norm, and the rate could also involve $N$ and $L$
(the number of observation time instances for each trajectory), depending on
the condition number of the inverse problem. Numerical results on systems that
exhibit different swarming behaviors demonstrate efficient learning of our
approach from scarce noisy trajectory data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages; Appendix 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACon$^2$: Adaptive Conformal Consensus for Provable Blockchain Oracles <span class="chip">USENIX Security 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09330v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09330v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangdon Park, Osbert Bastani, Taesoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchains with smart contracts are distributed ledger systems that achieve
block-state consistency among distributed nodes by only allowing deterministic
operations of smart contracts. However, the power of smart contracts is enabled
by interacting with stochastic off-chain data, which in turn opens the
possibility to undermine the block-state consistency. To address this issue, an
oracle smart contract is used to provide a single consistent source of external
data; but, simultaneously, this introduces a single point of failure, which is
called the oracle problem. To address the oracle problem, we propose an
adaptive conformal consensus (ACon$^2$) algorithm that derives a consensus set
of data from multiple oracle contracts via the recent advance in online
uncertainty quantification learning. Interesting, the consensus set provides a
desired correctness guarantee under distribution shift and Byzantine
adversaries. We demonstrate the efficacy of the proposed algorithm on two price
datasets and an Ethereum case study. In particular, the Solidity implementation
of the proposed algorithm shows the potential practicality of the proposed
algorithm, implying that online machine learning algorithms are applicable to
address security issues in blockchains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to USENIX Security 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Knowledge</span>-augmented Graph Machine Learning for Drug Discovery: A <span class="highlight-title">Survey</span>
  from Precision to Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Zhong, Anastasia Barkova, Davide Mottin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Artificial Intelligence (AI) into the field of drug
discovery has been a growing area of interdisciplinary scientific research.
However, conventional AI models are heavily limited in handling complex
biomedical structures (such as 2D or 3D protein and molecule structures) and
providing interpretations for outputs, which hinders their practical
application. As of late, Graph Machine Learning (GML) has gained considerable
attention for its exceptional ability to model graph-structured biomedical data
and investigate their properties and functional relationships. Despite
extensive efforts, GML methods still suffer from several deficiencies, such as
the limited ability to handle supervision sparsity and provide interpretability
in learning and inference processes, and their ineffectiveness in utilising
relevant domain knowledge. In response, recent studies have proposed
integrating external biomedical knowledge into the GML pipeline to realise more
precise and interpretable drug discovery with limited training instances.
However, a systematic definition for this burgeoning research direction is yet
to be established. This survey presents a comprehensive overview of
long-standing drug discovery principles, provides the foundational concepts and
cutting-edge techniques for graph-structured data and knowledge databases, and
formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug
discovery. we propose a thorough review of related KaGML works, collected
following a carefully designed search methodology, and organise them into four
categories following a novel-defined taxonomy. To facilitate research in this
promptly emerging field, we also share collected practical resources that are
valuable for intelligent drug discovery and provide an in-depth discussion of
the potential avenues for future advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Calibrating Semantic Segmentation Models: Analyses and An Algorithm <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongdong Wang, Boqing Gong, Liqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of semantic segmentation calibration. For image
classification, lots of existing solutions are proposed to alleviate model
miscalibration of confidence. However, to date, confidence calibration research
on semantic segmentation is still limited. We provide a systematic study on the
calibration of semantic segmentation models and propose a simple yet effective
approach. First, we find that model capacity, crop size, multi-scale testing,
and prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration, and show
that selective scaling consistently outperforms other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023 (8 pages, 4 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Free Lunch from the Noise: Provable and Practical Exploration for
  Representation Learning <span class="chip">UAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzheng Ren, Tianjun Zhang, Csaba Szepesvári, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning lies at the heart of the empirical success of deep
learning for dealing with the curse of dimensionality. However, the power of
representation learning has not been fully exploited yet in reinforcement
learning (RL), due to i), the trade-off between expressiveness and
tractability; and ii), the coupling between exploration and representation
learning. In this paper, we first reveal the fact that under some noise
assumption in the stochastic control model, we can obtain the linear spectral
feature of its corresponding Markov transition operator in closed-form for
free. Based on this observation, we propose Spectral Dynamics Embedding
(SPEDE), which breaks the trade-off and completes optimistic exploration for
representation learning by exploiting the structure of the noise. We provide
rigorous theoretical analysis of SPEDE, and demonstrate the practical superior
performance over the existing state-of-the-art empirical algorithms on several
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UAI 2022. The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Reward Functions for Robotic Manipulation by Observing Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minttu Alakuijala, Gabriel Dulac-Arnold, Julien Mairal, Jean Ponce, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Observing a human demonstrator manipulate objects provides a rich, scalable
and inexpensive source of data for learning robotic policies. However,
transferring skills from human videos to a robotic manipulator poses several
challenges, not least a difference in action and observation spaces. In this
work, we use unlabeled videos of humans solving a wide range of manipulation
tasks to learn a task-agnostic reward function for robotic manipulation
policies. Thanks to the diversity of this training data, the learned reward
function sufficiently generalizes to image observations from a previously
unseen robot embodiment and environment to provide a meaningful prior for
directed exploration in reinforcement learning. We propose two methods for
scoring states relative to a goal image: through direct temporal regression,
and through distances in an embedding space obtained with time-contrastive
learning. By conditioning the function on a goal image, we are able to reuse
one model across a variety of tasks. Unlike prior work on leveraging human
videos to teach robots, our method, Human Offline Learned Distances (HOLD)
requires neither a priori data from the robot environment, nor a set of
task-specific human demonstrations, nor a predefined notion of correspondence
across morphologies, yet it is able to accelerate training of several
manipulation tasks on a simulated robot arm compared to using only a sparse
reward obtained from task completion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Variable Representation for Reinforcement Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep latent variable models have achieved significant empirical successes in
model-based reinforcement learning (RL) due to their expressiveness in modeling
complex transition dynamics. On the other hand, it remains unclear
theoretically and empirically how latent variable models may facilitate
learning, planning, and exploration to improve the sample efficiency of RL. In
this paper, we provide a representation view of the latent variable models for
state-action value functions, which allows both tractable variational learning
algorithm and effective implementation of the optimism/pessimism principle in
the face of uncertainty for exploration. In particular, we propose a
computationally efficient planning algorithm with UCB exploration by
incorporating kernel embeddings of latent variable models. Theoretically, we
establish the sample complexity of the proposed approach in the online and
offline settings. Empirically, we demonstrate superior performance over current
state-of-the-art algorithms across various benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. The first two authors contribute equally. Project Website:
  https://rlrep.github.io/lvrep/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Decomposition Representation for Reinforcement Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E. Gonzalez, Dale Schuurmans, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning often plays a critical role in reinforcement learning
by managing the curse of dimensionality. A representative class of algorithms
exploits a spectral decomposition of the stochastic transition dynamics to
construct representations that enjoy strong theoretical properties in an
idealized setting. However, current spectral methods suffer from limited
applicability because they are constructed for state-only aggregation and
derived from a policy-dependent transition kernel, without considering the
issue of exploration. To address these issues, we propose an alternative
spectral method, Spectral Decomposition Representation (SPEDER), that extracts
a state-action abstraction from the dynamics without inducing spurious
dependence on the data collection policy, while also balancing the
exploration-versus-exploitation trade-off during learning. A theoretical
analysis establishes the sample efficiency of the proposed algorithm in both
the online and offline settings. In addition, an experimental investigation
demonstrates superior performance over current state-of-the-art algorithms
across several benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Methods for Convex Risk Averse Distributed Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghui Lan, Zhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the communication complexity of convex risk-averse
optimization over a network. The problem generalizes the well-studied
risk-neutral finite-sum distributed optimization problem and its importance
stems from the need to handle risk in an uncertain environment. For algorithms
in the literature, there exists a gap in communication complexities for solving
risk-averse and risk-neutral problems. We propose two distributed algorithms,
namely the distributed risk averse optimization (DRAO) method and the
distributed risk averse optimization with sliding (DRAO-S) method, to close the
gap. Specifically, the DRAO method achieves the optimal communication
complexity by assuming a certain saddle point subproblem can be easily solved
in the server node. The DRAO-S method removes the strong assumption by
introducing a novel saddle point sliding subroutine which only requires the
projection over the ambiguity set $P$. We observe that the number of
$P$-projections performed by DRAO-S is optimal. Moreover, we develop matching
lower complexity bounds to show the communication complexities of both DRAO and
DRAO-S to be improvable. Numerical experiments are conducted to demonstrate the
encouraging empirical performance of the DRAO-S method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuda Song, Yifei Zhou, Ayush Sekhari, J. Andrew Bagnell, Akshay Krishnamurthy, Wen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a hybrid reinforcement learning setting (Hybrid RL), in which an
agent has access to an offline dataset and the ability to collect experience
via real-world online interaction. The framework mitigates the challenges that
arise in both pure offline and online RL settings, allowing for the design of
simple and highly effective algorithms, in both theory and practice. We
demonstrate these advantages by adapting the classical Q learning/iteration
algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In
our theoretical results, we prove that the algorithm is both computationally
and statistically efficient whenever the offline dataset supports a
high-quality policy and the environment has bounded bilinear rank. Notably, we
require no assumptions on the coverage provided by the initial distribution, in
contrast with guarantees for policy gradient/iteration methods. In our
experimental results, we show that Hy-Q with neural network function
approximation outperforms state-of-the-art online, offline, and hybrid RL
baselines on challenging benchmarks, including Montezuma's Revenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 6 figures. Published at ICLR 2023. Code available at
  https://github.com/yudasong/HyQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Patch Selection for High-Resolution Image Recognition <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Bergner, Christoph Lippert, Aravindh Mahendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution images are prevalent in various applications, such as
autonomous driving and computer-aided diagnosis. However, training neural
networks on such images is computationally challenging and easily leads to
out-of-memory errors even on modern GPUs. We propose a simple method, Iterative
Patch Selection (IPS), which decouples the memory usage from the input size and
thus enables the processing of arbitrarily large images under tight hardware
constraints. IPS achieves this by selecting only the most salient patches,
which are then aggregated into a global representation for image recognition.
For both patch selection and aggregation, a cross-attention based transformer
is introduced, which exhibits a close connection to Multiple Instance Learning.
Our method demonstrates strong performance and has wide applicability across
different domains, training regimes and image sizes while using minimal
accelerator memory. For example, we are able to finetune our model on
whole-slide images consisting of up to 250k patches (>16 gigapixels) with only
5 GB of GPU VRAM at a batch size of 16.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid training of quantum recurrent neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Siemaszko, Adam Buraczewski, Bertrand Le Saux, Magdalena Stobińska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series prediction is essential for human activities in diverse areas. A
common approach to this task is to harness Recurrent Neural Networks (RNNs).
However, while their predictions are quite accurate, their learning process is
complex and, thus, time and energy consuming. Here, we propose to extend the
concept of RRNs by including continuous-variable quantum resources in it, and
to use a quantum-enhanced RNN to overcome these obstacles. The design of the
Continuous-Variable Quantum RNN (CV-QRNN) is rooted in the continuous-variable
quantum computing paradigm. By performing extensive numerical simulations, we
demonstrate that the quantum network is capable of learning-time dependence of
several types of temporal data, and that it converges to the optimal weights in
fewer epochs than a classical network. Furthermore, for a small number of
trainable parameters, it can achieve lower losses than its classical
counterpart. CV-QRNN can be implemented using commercially available
quantum-photonic hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1906.07801v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1906.07801v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Grünwald, Rianne de Heide, Wouter Koolen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop the theory of hypothesis testing based on the e-value, a notion of
evidence that, unlike the p-value, allows for effortlessly combining results
from several studies in the common scenario where the decision to perform a new
study may depend on previous outcomes. Tests based on e-values are safe, i.e.
they preserve Type-I error guarantees, under such optional continuation. We
define growth-rate optimality (GRO) as an analogue of power in an optional
continuation context, and we show how to construct GRO e-variables for general
testing problems with composite null and alternative, emphasizing models with
nuisance parameters. GRO e-values take the form of Bayes factors with special
priors. We illustrate the theory using several classic examples including a
one-sample safe t-test and the 2 x 2 contingency table. Sharing Fisherian,
Neymanian and Jeffreys-Bayesian interpretations, e-values may provide a
methodology acceptable to adherents of all three schools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as discussion paper to the Journal of the Royal Statistical
  Society series B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Importance of Sign Labeling: The Hamburg Sign Language Notation
  System Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Ferlin, Sylwia Majchrowska, Marta Plantykow, Alicja Kwaśniwska, Agnieszka Mikołajczyk-Bareła, Milena Olech, Jakub Nalepa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling is the cornerstone of supervised machine learning, which has been
exploited in a plethora of various applications, with sign language recognition
being one of them. However, such algorithms must be fed with a huge amount of
consistently labeled data during the training process to elaborate a
well-generalizing model. In addition, there is a great need for an automated
solution that works with any nationally diversified sign language. Although
there are language-agnostic transcription systems, such as the Hamburg Sign
Language Notation System (HamNoSys) that describe the signer's initial position
and body movement instead of the glosses' meanings, there are still issues with
providing accurate and reliable labels for every real-world use case. In this
context, the industry relies heavily on manual attribution and labeling of the
available video data. In this work, we tackle this issue and thoroughly analyze
the HamNoSys labels provided by various maintainers of open sign language
corpora in five sign languages, in order to examine the challenges encountered
in labeling video data. We also investigate the consistency and objectivity of
HamNoSys-based labels for the purpose of training machine learning models. Our
findings provide valuable insights into the limitations of the current labeling
methods and pave the way for future research on developing more accurate and
efficient solutions for sign language recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding Graph Network Simulators using Physical Sensor Observations <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Linkerhägner, Niklas Freymuth, Paul Maria Scheikl, Franziska Mathis-Ullrich, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical simulations that accurately model reality are crucial for many
engineering disciplines such as mechanical engineering and robotic motion
planning. In recent years, learned Graph Network Simulators produced accurate
mesh-based simulations while requiring only a fraction of the computational
cost of traditional simulators. Yet, the resulting predictors are confined to
learning from data generated by existing mesh-based simulators and thus cannot
include real world sensory information such as point cloud data. As these
predictors have to simulate complex physical systems from only an initial
state, they exhibit a high error accumulation for long-term predictions. In
this work, we integrate sensory information to ground Graph Network Simulators
on real world observations. In particular, we predict the mesh state of
deformable objects by utilizing point cloud data. The resulting model allows
for accurate predictions over longer time horizons, even under uncertainties in
the simulation, such as unknown material properties. Since point clouds are
usually not available for every time step, especially in online settings, we
employ an imputation-based model. The model can make use of such additional
information only when provided, and resorts to a standard Graph Network
Simulator, otherwise. We experimentally validate our approach on a suite of
prediction tasks for mesh-based interactions between soft and rigid bodies. Our
method results in utilization of additional point cloud information to
accurately predict stable simulations where existing Graph Network Simulators
fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a poster at the 11th International Conference on Learning
  Representations (ICLR), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LambdaKG: A Library for <span class="highlight-title">Pre-train</span>ed Language Model-Based <span class="highlight-title">Knowledge</span> Graph
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Zhoubo Li, Xiaohan Wang, Yuqi Zhu, Ningyu Zhang, Jintian Zhang, Siyuan Cheng, Bozhong Tian, Shumin Deng, Feiyu Xiong, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress and the project website is
  https://zjunlp.github.io/project/promptkg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Positive unlabeled learning with tensor networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bojan Žunkovič
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positive unlabeled learning is a binary classification problem with positive
and unlabeled data. It is common in domains where negative labels are costly or
impossible to obtain, e.g., medicine and personalized advertising. We apply the
locally purified state tensor network to the positive unlabeled learning
problem and test our model on the MNIST image and 15 categorical/mixed
datasets. On the MNIST dataset, we obtain close to the state-of-the-art results
even with very few labeled positive samples. We significantly improve the
state-of-the-art on categorical datasets. Further, we show that the agreement
fraction between outputs of different models on unlabeled samples is a good
indicator of the model's performance. Finally, our method can generate new
positive and negative instances, which we demonstrate on simple synthetic
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising Masked AutoEncoders Help Robust Classification <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06983v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06983v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanlin Wu, Hang Ye, Yuntian Gu, Huishuai Zhang, Liwei Wang, Di He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new self-supervised method, which is called
Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers
of images. In DMAE, we corrupt each image by adding Gaussian noises to each
pixel value and randomly masking several patches. A Transformer-based
encoder-decoder model is then trained to reconstruct the original image from
the corrupted one. In this learning paradigm, the encoder will learn to capture
relevant semantics for the downstream tasks, which is also robust to Gaussian
additive noises. We show that the pre-trained encoder can naturally be used as
the base classifier in Gaussian smoothed models, where we can analytically
compute the certified radius for any data point. Although the proposed method
is simple, it yields significant performance improvement in downstream
classification tasks. We show that the DMAE ViT-Base model, which just uses
1/10 parameters of the model developed in recent work arXiv:2206.10550,
achieves competitive or better certified accuracy in various settings. The DMAE
ViT-Large model significantly surpasses all previous results, establishing a
new state-of-the-art on ImageNet dataset. We further demonstrate that the
pre-trained model has good transferability to the CIFAR-10 dataset, suggesting
its wide adaptability. Models and code are available at
https://github.com/quanlin-wu/dmae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Neural Networks for Reversible Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have led to a paradigm shift in the field of
reversible steganography. A fundamental pillar of reversible steganography is
predictive modelling which can be realised via deep neural networks. However,
non-trivial errors exist in inferences about some out-of-distribution and noisy
data. In view of this issue, we propose to consider uncertainty in predictive
models based upon a theoretical framework of Bayesian deep learning, thereby
creating an adaptive steganographic system. Most modern deep-learning models
are regarded as deterministic because they only offer predictions while failing
to provide uncertainty measurement. Bayesian neural networks bring a
probabilistic perspective to deep learning and can be regarded as self-aware
intelligent machinery; that is, a machine that knows its own limitations. To
quantify uncertainty, we apply Bayesian statistics to model the predictive
distribution and approximate it through Monte Carlo sampling with stochastic
forward passes. We further show that predictive uncertainty can be disentangled
into aleatoric and epistemic uncertainties and these quantities can be learnt
unsupervised. Experimental results demonstrate an improvement delivered by
Bayesian uncertainty analysis upon steganographic rate-distortion performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAV Path Planning Employing MPC- Reinforcement Learning Method
  Considering Collision Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahya Ramezani, Hamed Habibi, Jose luis Sanchez Lopez, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of Unmanned Aerial (UA V) path planning
in complex and uncertain environments by designing a Model Predictive Control
(MPC), based on a Long-Short-Term Memory (LSTM) network integrated into the
Deep Deterministic Policy Gradient algorithm. In the proposed solution,
LSTM-MPC operates as a deterministic policy within the DDPG network, and it
leverages a predicting pool to store predicted future states and actions for
improved robustness and efficiency. The use of the predicting pool also enables
the initialization of the critic network, leading to improved convergence speed
and reduced failure rate compared to traditional reinforcement learning and
deep reinforcement learning methods. The effectiveness of the proposed solution
is evaluated by numerical simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow Annealed Importance Sampling Bootstrap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.01893v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.01893v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurence Illing Midgley, Vincent Stimper, Gregor N. C. Simm, Bernhard Schölkopf, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flows are tractable density models that can approximate
complicated target distributions, e.g. Boltzmann distributions of physical
systems. However, current methods for training flows either suffer from
mode-seeking behavior, use samples from the target generated beforehand by
expensive MCMC methods, or use stochastic losses that have high variance. To
avoid these problems, we augment flows with annealed importance sampling (AIS)
and minimize the mass-covering $\alpha$-divergence with $\alpha=2$, which
minimizes importance weight variance. Our method, Flow AIS Bootstrap (FAB),
uses AIS to generate samples in regions where the flow is a poor approximation
of the target, facilitating the discovery of new modes. We apply FAB to
multimodal targets and show that we can approximate them very accurately where
previous methods fail. To the best of our knowledge, we are the first to learn
the Boltzmann distribution of the alanine dipeptide molecule using only the
unnormalized target density, without access to samples generated via Molecular
Dynamics (MD) simulations: FAB produces better results than training via
maximum likelihood on MD samples while using 100 times fewer target
evaluations. After reweighting the samples, we obtain unbiased histograms of
dihedral angles that are almost identical to the ground truth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bounding Information Leakage in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.03875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.03875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Del Grosso, Georg Pichler, Catuscia Palamidessi, Pablo Piantanida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, it has been shown that Machine Learning models can leak sensitive
information about their training data. This information leakage is exposed
through membership and attribute inference attacks. Although many attack
strategies have been proposed, little effort has been made to formalize these
problems. We present a novel formalism, generalizing membership and attribute
inference attack setups previously studied in the literature and connecting
them to memorization and generalization. First, we derive a universal bound on
the success rate of inference attacks and connect it to the generalization gap
of the target model. Second, we study the question of how much sensitive
information is stored by the algorithm about its training set and we derive
bounds on the mutual information between the sensitive attributes and model
parameters. Experimentally, we illustrate the potential of our approach by
applying it to both synthetic data and classification tasks on natural images.
Finally, we apply our formalism to different attribute inference strategies,
with which an adversary is able to recover the identity of writers in the
PenDigits dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in [Elsevier
  Neurocomputing](https://doi.org/10.1016/j.neucom.2023.02.058)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> of Machine Learning Based Intrusion Detection Methods for
  Internet of Medical Things 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09657v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09657v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Si-Ahmed, Mohammed Ali Al-Garadi, Narhimene Boustia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of Medical Things (IoMT) has revolutionized the healthcare
industry by enabling physiological data collection using sensors, which are
transmitted to remote servers for continuous analysis by physicians and
healthcare professionals. This technology offers numerous benefits, including
early disease detection and automatic medication for patients with chronic
illnesses. However, IoMT technology also presents significant security risks,
such as violating patient privacy or exposing sensitive data to interception
attacks due to wireless communication, which could be fatal for the patient.
Additionally, traditional security measures, such as cryptography, are
challenging to implement in medical equipment due to the heterogeneous
communication and their limited computation, storage, and energy capacity.
These protection methods are also ineffective against new and zero-day attacks.
It is essential to adopt robust security measures to ensure data integrity,
confidentiality, and availability during data collection, transmission,
storage, and processing. In this context, using Intrusion Detection Systems
(IDS) based on Machine Learning (ML) can bring a complementary security
solution adapted to the unique characteristics of IoMT systems. Therefore, this
paper investigates how IDS based on ML can address security and privacy issues
in IoMT systems. First, the generic three-layer architecture of IoMT is
provided, and the security requirements of IoMT systems are outlined. Then, the
various threats that can affect IoMT security are identified, and the
advantages, disadvantages, methods, and datasets used in each solution based on
ML at the three layers that make up IoMT are presented. Finally, the paper
discusses the challenges and limitations of applying IDS based on ML at each
layer of IoMT, which can serve as a future research direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 3 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MHCCL: Masked Hierarchical Cluster-wise Contrastive Learning for
  Multivariate Time Series <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianwen Meng, Hangwei Qian, Yong Liu, Lizhen Cui, Yonghui Xu, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning semantic-rich representations from raw unlabeled time series data is
critical for downstream tasks such as classification and forecasting.
Contrastive learning has recently shown its promising representation learning
capability in the absence of expert annotations. However, existing contrastive
approaches generally treat each instance independently, which leads to false
negative pairs that share the same semantics. To tackle this problem, we
propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model,
which exploits semantic information obtained from the hierarchical structure
consisting of multiple latent partitions for multivariate time series.
Motivated by the observation that fine-grained clustering preserves higher
purity while coarse-grained one reflects higher-level semantics, we propose a
novel downward masking strategy to filter out fake negatives and supplement
positives by incorporating the multi-granularity information from the
clustering hierarchy. In addition, a novel upward masking strategy is designed
in MHCCL to remove outliers of clusters at each partition to refine prototypes,
which helps speed up the hierarchical clustering process and improves the
clustering quality. We conduct experimental evaluations on seven widely-used
multivariate time series datasets. The results demonstrate the superiority of
MHCCL over the state-of-the-art approaches for unsupervised time series
representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceive and predict: <span class="highlight-title">self-supervised</span> speech representation based <span class="highlight-title">loss</span>
  functions for speech enhancement <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, William Ravenscroft, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the domain of speech enhancement has explored the use of
self-supervised speech representations to aid in the training of neural speech
enhancement models. However, much of this work focuses on using the deepest or
final outputs of self supervised speech representation models, rather than the
earlier feature encodings. The use of self supervised representations in such a
way is often not fully motivated. In this work it is shown that the distance
between the feature encodings of clean and noisy speech correlate strongly with
psychoacoustically motivated measures of speech quality and intelligibility, as
well as with human Mean Opinion Score (MOS) ratings. Experiments using this
distance as a loss function are performed and improved performance over the use
of STFT spectrogram distance based loss as well as other common loss functions
from speech enhancement literature is demonstrated using objective measures
such as perceptual evaluation of speech quality (PESQ) and short-time objective
intelligibility (STOI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Based Uncertainty in Value Functions <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of quantifying uncertainty over expected cumulative
rewards in model-based reinforcement learning. In particular, we focus on
characterizing the variance over values induced by a distribution over MDPs.
Previous work upper bounds the posterior variance over values by solving a
so-called uncertainty Bellman equation, but the over-approximation may result
in inefficient exploration. We propose a new uncertainty Bellman equation whose
solution converges to the true posterior variance over values and explicitly
characterizes the gap in previous work. Moreover, our uncertainty
quantification technique is easily integrated into common exploration
strategies and scales naturally beyond the tabular setting by using standard
deep reinforcement learning architectures. Experiments in difficult exploration
tasks, both in tabular and continuous control settings, show that our sharper
uncertainty estimates improve sample-efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relative representations enable zero-shot latent space communication <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks embed the geometric structure of a data manifold lying in a
high-dimensional space into latent representations. Ideally, the distribution
of the data points in the latent space should depend only on the task, the
data, the loss, and other architecture-specific constraints. However, factors
such as the random weights initialization, training hyperparameters, or other
sources of randomness in the training phase may induce incoherent latent spaces
that hinder any form of reuse. Nevertheless, we empirically observe that, under
the same data and modeling choices, the angles between the encodings within
distinct latent spaces do not change. In this work, we propose the latent
similarity between each sample and a fixed set of anchors as an alternative
data representation, demonstrating that it can enforce the desired invariances
without any additional training. We show how neural architectures can leverage
these relative representations to guarantee, in practice, invariance to latent
isometries and rescalings, effectively enabling latent space communication:
from zero-shot model stitching to latent space comparison between diverse
settings. We extensively validate the generalization capability of our approach
on different datasets, spanning various modalities (images, text, graphs),
tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs,
GCNs, transformers).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 notable top 5%, 26 pages, 11 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private, fair and accurate: Training large-scale, privacy-preserving AI
  models in medical imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Alexander Ziller, Christiane Kuhl, Marcus Makowski, Sven Nebelung, Rickmer Braren, Daniel Rueckert, Daniel Truhn, Georgios Kaissis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) models are increasingly used in the medical
domain. However, as medical data is highly sensitive, special precautions to
ensure its protection are required. The gold standard for privacy preservation
is the introduction of differential privacy (DP) to model training. Prior work
indicates that DP has negative implications on model accuracy and fairness,
which are unacceptable in medicine and represent a main barrier to the
widespread use of privacy-preserving techniques. In this work, we evaluated the
effect of privacy-preserving training of AI models for chest radiograph
diagnosis regarding accuracy and fairness compared to non-private training. For
this, we used a large dataset (N=193,311) of high quality clinical chest
radiographs, which were retrospectively collected and manually labeled by
experienced radiologists. We then compared non-private deep convolutional
neural networks (CNNs) and privacy-preserving (DP) models with respect to
privacy-utility trade-offs measured as area under the
receiver-operator-characteristic curve (AUROC), and privacy-fairness
trade-offs, measured as Pearson's r or Statistical Parity Difference. We found
that the non-private CNNs achieved an average AUROC score of 0.90 +- 0.04 over
all labels, whereas the DP CNNs with a privacy budget of epsilon=7.89 resulted
in an AUROC of 0.87 +- 0.04, i.e., a mere 2.6% performance decrease compared to
non-private training. Furthermore, we found the privacy-preserving training not
to amplify discrimination against age, sex or co-morbidity. Our study shows
that -- under the challenging realistic circumstances of a real-life clinical
dataset -- the privacy-preserving training of diagnostic deep learning models
is possible with excellent diagnostic accuracy and fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 tables, 5 figures, 11 supplementary materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Low Rank Matrix Completion <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Jain, Soumyabrata Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of {\em online} low-rank matrix completion with
$\mathsf{M}$ users, $\mathsf{N}$ items and $\mathsf{T}$ rounds. In each round,
the algorithm recommends one item per user, for which it gets a (noisy) reward
sampled from a low-rank user-item preference matrix. The goal is to design a
method with sub-linear regret (in $\mathsf{T}$) and nearly optimal dependence
on $\mathsf{M}$ and $\mathsf{N}$. The problem can be easily mapped to the
standard multi-armed bandit problem where each item is an {\em independent}
arm, but that leads to poor regret as the correlation between arms and users is
not exploited. On the other hand, exploiting the low-rank structure of reward
matrix is challenging due to non-convexity of the low-rank manifold. We first
demonstrate that the low-rank structure can be exploited using a simple
explore-then-commit (ETC) approach that ensures a regret of $O(\mathsf{polylog}
(\mathsf{M}+\mathsf{N}) \mathsf{T}^{2/3})$. That is, roughly only
$\mathsf{polylog} (\mathsf{M}+\mathsf{N})$ item recommendations are required
per user to get a non-trivial solution. We then improve our result for the
rank-$1$ setting which in itself is quite challenging and encapsulates some of
the key issues. Here, we propose \textsc{OCTAL} (Online Collaborative filTering
using iterAtive user cLustering) that guarantees nearly optimal regret of
$O(\mathsf{polylog} (\mathsf{M}+\mathsf{N}) \mathsf{T}^{1/2})$. OCTAL is based
on a novel technique of clustering users that allows iterative elimination of
items and leads to a nearly optimal minimax rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 7 figures (Accepted at ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Video Quality Assessment on User <span class="highlight-title">Generate</span>d Contents from
  Aesthetic and Technical Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in user-generated-content (UGC) videos calls for the
development of effective video quality assessment (VQA) algorithms. However,
the objective of the UGC-VQA problem is still ambiguous and can be viewed from
two perspectives: the technical perspective, measuring the perception of
distortions; and the aesthetic perspective, which relates to preference and
recommendation on contents. To understand how these two perspectives affect
overall subjective opinions in UGC-VQA, we conduct a large-scale subjective
study to collect human quality opinions on overall quality of videos as well as
perceptions from aesthetic and technical perspectives. The collected
Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality
opinions on UGC videos are universally and inevitably affected by both
aesthetic and technical perspectives. In light of this, we propose the
Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of
UGC videos based on the two perspectives. The DOVER proves state-of-the-art
performance in UGC-VQA under very high efficiency. With perspective opinions in
DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable
clear-cut quality evaluations from a single aesthetic or technical perspective.
Code at https://github.com/VQAssessment/DOVER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigation of chemical structure recognition by encoder-decoder
  models in learning progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumpei Nemoto, Tadahaya Mizuno, Hiroyuki Kusuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Descriptor generation methods using latent representations of
encoder$-$decoder (ED) models with SMILES as input are useful because of the
continuity of descriptor and restorability to the structure. However, it is not
clear how the structure is recognized in the learning progress of ED models. In
this work, we created ED models of various learning progress and investigated
the relationship between structural information and learning progress. We
showed that compound substructures were learned early in ED models by
monitoring the accuracy of downstream tasks and input$-$output substructure
similarity using substructure$-$based descriptors, which suggests that existing
evaluation methods based on the accuracy of downstream tasks may not be
sensitive enough to evaluate the performance of ED models with SMILES as
descriptor generation methods. On the other hand, we showed that structure
restoration was time$-$consuming, and in particular, insufficient learning led
to the estimation of a larger structure than the actual one. It can be inferred
that determining the endpoint of the structure is a difficult task for the
model. To our knowledge, this is the first study to link the learning progress
of SMILES by ED model to chemical structures for a wide range of chemicals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Prototype-oriented Set Representations for Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.09140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.09140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dandan Guo, Long Tian, Minghe Zhang, Mingyuan Zhou, Hongyuan Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from set-structured data is a fundamental problem that has recently
attracted increasing attention, where a series of summary networks are
introduced to deal with the set input. In fact, many meta-learning problems can
be treated as set-input tasks. Most existing summary networks aim to design
different architectures for the input set in order to enforce permutation
invariance. However, scant attention has been paid to the common cases where
different sets in a meta-distribution are closely related and share certain
statistical properties. Viewing each set as a distribution over a set of global
prototypes, this paper provides a novel prototype-oriented optimal transport
(POT) framework to improve existing summary networks. To learn the distribution
over the global prototypes, we minimize its regularized optimal transport
distance to the set empirical distribution over data points, providing a
natural unsupervised way to improve the summary network. Since our
plug-and-play framework can be applied to many meta-learning problems, we
further instantiate it to the cases of few-shot classification and implicit
meta generative modeling. Extensive experiments demonstrate that our framework
significantly improves the existing summary networks on learning more powerful
summary statistics from sets and can be successfully integrated into
metric-based few-shot classification and generative modeling applications,
providing a promising tool for addressing set-input and meta-learning problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using multimodal learning and deep generative models for corporate
  bankruptcy prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rogelio A. Mancisidor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces for the first time, to the best of our knowledge,
the concept of multimodal learning in bankruptcy prediction models. We use the
Conditional Multimodal Discriminative (CMMD) model to learn multimodal
representations that embed information from accounting, market, and textual
modalities. The CMMD model needs a sample with all data modalities for model
training. At test time, the CMMD model only needs access to accounting and
market modalities to generate multimodal representations, which are further
used to make bankruptcy predictions. This fact makes the use of bankruptcy
prediction models using textual data realistic and possible, since accounting
and market data are available for all companies unlike textual data. The
empirical results in this research show that the classification performance of
our proposed methodology is superior compared to that of a large number of
traditional classifier models. We also show that our proposed methodology
solves the limitation of previous bankruptcy models using textual data, as they
can only make predictions for a small proportion of companies. Finally, based
on multimodal representations, we introduce an index that is able to capture
the uncertainty of the financial situation of companies during periods of
financial distress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoTTS: End-to-End Text-to-Speech Synthesis through Differentiable
  Duration Modeling <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bac Nguyen, Fabien Cardinaux, Stefan Uhlich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel text-to-speech (TTS) models have recently enabled fast and
highly-natural speech synthesis. However, they typically require external
alignment models, which are not necessarily optimized for the decoder as they
are not jointly trained. In this paper, we propose a differentiable duration
method for learning monotonic alignments between input and output sequences.
Our method is based on a soft-duration mechanism that optimizes a stochastic
process in expectation. Using this differentiable duration method, we introduce
AutoTTS, a direct text-to-waveform speech synthesis model. AutoTTS enables
high-fidelity speech synthesis through a combination of adversarial training
and matching the total ground-truth duration. Experimental results show that
our model obtains competitive results while enjoying a much simpler training
pipeline. Audio samples are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Valuation Without Training of a Model <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nohyun Ki, Hoyong Choi, Hye Won Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent works on understanding deep learning try to quantify how much
individual data instances influence the optimization and generalization of a
model. Such attempts reveal characteristics and importance of individual
instances, which may provide useful information in diagnosing and improving
deep learning. However, most of the existing works on data valuation require
actual training of a model, which often demands high-computational cost. In
this paper, we provide a training-free data valuation score, called
complexity-gap score, which is a data-centric score to quantify the influence
of individual instances in generalization of two-layer overparameterized neural
networks. The proposed score can quantify irregularity of the instances and
measure how much each data instance contributes in the total movement of the
network parameters during training. We theoretically analyze and empirically
demonstrate the effectiveness of the complexity-gap score in finding `irregular
or mislabeled' data instances, and also provide applications of the score in
analyzing datasets and diagnosing training dynamics. Our code is publicly
available at https://github.com/JJchy/CG_score
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can discrete information extraction <span class="highlight-title">prompt</span>s generalize across language
  models? <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathanaël Carraz Rakotonirina, Roberto Dessì, Fabio Petroni, Sebastian Riedel, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study whether automatically-induced prompts that effectively extract
information from a language model can also be used, out-of-the-box, to probe
other language models for the same information. After confirming that discrete
prompts induced with the AutoPrompt algorithm outperform manual and semi-manual
prompts on the slot-filling task, we demonstrate a drop in performance for
AutoPrompt prompts learned on a model and tested on another. We introduce a way
to induce prompts by mixing language models at training time that results in
prompts that generalize well across models. We conduct an extensive analysis of
the induced prompts, finding that the more general prompts include a larger
proportion of existing English words and have a less order-dependent and more
uniform distribution of information across their component tokens. Our work
provides preliminary evidence that it's possible to generate discrete prompts
that can be induced once and used with a number of different models, and gives
insights on the properties characterizing such prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Assisted Algorithm Unrolling for Online Optimization with
  Budget Constraints <span class="chip">AAAI'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Yang, Shaolei Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online optimization with multiple budget constraints is challenging since the
online decisions over a short time horizon are coupled together by strict
inventory constraints. The existing manually-designed algorithms cannot achieve
satisfactory average performance for this setting because they often need a
large number of time steps for convergence and/or may violate the inventory
constraints. In this paper, we propose a new machine learning (ML) assisted
unrolling approach, called LAAU (Learning-Assisted Algorithm Unrolling), which
unrolls the online decision pipeline and leverages an ML model for updating the
Lagrangian multiplier online. For efficient training via backpropagation, we
derive gradients of the decision pipeline over time. We also provide the
average cost bounds for two cases when training data is available offline and
collected online, respectively. Finally, we present numerical results to
highlight that LAAU can outperform the existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Deep Semantics for Test Completion <span class="chip">ICSE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Nie, Rahul Banerjee, Junyi Jessy Li, Raymond J. Mooney, Milos Gligoric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing tests is a time-consuming yet essential task during software
development. We propose to leverage recent advances in deep learning for text
and code generation to assist developers in writing tests. We formalize the
novel task of test completion to automatically complete the next statement in a
test method based on the context of prior statements and the code under test.
We develop TeCo -- a deep learning model using code semantics for test
completion. The key insight underlying TeCo is that predicting the next
statement in a test method requires reasoning about code execution, which is
hard to do with only syntax-level data that existing code completion models
use. TeCo extracts and uses six kinds of code semantics data, including the
execution result of prior statements and the execution context of the test
method. To provide a testbed for this new task, as well as to evaluate TeCo, we
collect a corpus of 130,934 test methods from 1,270 open-source Java projects.
Our results show that TeCo achieves an exact-match accuracy of 18, which is 29%
higher than the best baseline using syntax-level data only. When measuring
functional correctness of generated next statement, TeCo can generate runnable
code in 29% of the cases compared to 18% obtained by the best baseline.
Moreover, TeCo is significantly better than prior work on test oracle
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper in ICSE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GANStrument: Adversarial Instrument Sound Synthesis with Pitch-invariant
  Instance Conditioning <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaku Narita, Junichi Shimizu, Taketo Akama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose GANStrument, a generative adversarial model for instrument sound
synthesis. Given a one-shot sound as input, it is able to generate pitched
instrument sounds that reflect the timbre of the input within an interactive
time. By exploiting instance conditioning, GANStrument achieves better fidelity
and diversity of synthesized sounds and generalization ability to various
inputs. In addition, we introduce an adversarial training scheme for a
pitch-invariant feature extractor that significantly improves the pitch
accuracy and timbre consistency. Experimental results show that GANStrument
outperforms strong baselines that do not use instance conditioning in terms of
generation quality and input editability. Qualitative examples are available
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, Accepted to 2023 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP), Audio examples:
  https://ganstrument.github.io/ganstrument-demo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLUTR: Curriculum Learning via Unsupervised Task Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdus Salam Azad, Izzeddin Gur, Jasper Emhoff, Nathaniel Alexis, Aleksandra Faust, Pieter Abbeel, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) algorithms are often known for sample
inefficiency and difficult generalization. Recently, Unsupervised Environment
Design (UED) emerged as a new paradigm for zero-shot generalization by
simultaneously learning a task distribution and agent policies on the generated
tasks. This is a non-stationary process where the task distribution evolves
along with agent policies; creating an instability over time. While past works
demonstrated the potential of such approaches, sampling effectively from the
task space remains an open challenge, bottlenecking these approaches. To this
end, we introduce CLUTR: a novel unsupervised curriculum learning algorithm
that decouples task representation and curriculum learning into a two-stage
optimization. It first trains a recurrent variational autoencoder on randomly
generated tasks to learn a latent task manifold. Next, a teacher agent creates
a curriculum by maximizing a minimax REGRET-based objective on a set of latent
tasks sampled from this manifold. Using the fixed-pretrained task manifold, we
show that CLUTR successfully overcomes the non-stationarity problem and
improves stability. Our experimental results show CLUTR outperforms PAIRED, a
principled and popular UED method, in the challenging CarRacing and navigation
environments: achieving 10.6X and 45\% improvement in zero-shot generalization,
respectively. CLUTR also performs comparably to the non-UED state-of-the-art
for CarRacing, while requiring 500X fewer environment interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Currently Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovery of Single Independent Latent Variable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Shaham, Jonathan Svirsky, Ori Katz, Ronen Talmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent variable discovery is a central problem in data analysis with a broad
range of applications in applied science. In this work, we consider data given
as an invertible mixture of two statistically independent components and assume
that one of the components is observed while the other is hidden. Our goal is
to recover the hidden component. For this purpose, we propose an autoencoder
equipped with a discriminator. Unlike the standard nonlinear ICA problem, which
was shown to be non-identifiable, in the special case of ICA we consider here,
we show that our approach can recover the component of interest up to
entropy-preserving transformation. We demonstrate the performance of the
proposed approach in several tasks, including image synthesis, voice cloning,
and fetal ECG extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at Neurips 2022. In the current
  version the proof of the lemma is modified</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deconstructed <span class="highlight-title">Generation</span>-Based Zero-Shot Model <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dubing Chen, Yuming Shen, Haofeng Zhang, Philip H. S. Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on Generalized Zero-Shot Learning (GZSL) has focused
primarily on generation-based methods. However, current literature has
overlooked the fundamental principles of these methods and has made limited
progress in a complex manner. In this paper, we aim to deconstruct the
generator-classifier framework and provide guidance for its improvement and
extension. We begin by breaking down the generator-learned unseen class
distribution into class-level and instance-level distributions. Through our
analysis of the role of these two types of distributions in solving the GZSL
problem, we generalize the focus of the generation-based approach, emphasizing
the importance of (i) attribute generalization in generator learning and (ii)
independent classifier learning with partially biased data. We present a simple
method based on this analysis that outperforms SotAs on four public GZSL
datasets, demonstrating the validity of our deconstruction. Furthermore, our
proposed method remains effective even without a generative model, representing
a step towards simplifying the generator-classifier structure. Our code is
available at \url{https://github.com/cdb342/DGZ}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rate-Optimal Contextual Online Matching Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuantong Li, Chi-hua Wang, Guang Cheng, Will Wei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-sided online matching platforms have been employed in various markets.
However, agents' preferences in present market are usually implicit and unknown
and must be learned from data. With the growing availability of side
information involved in the decision process, modern online matching
methodology demands the capability to track preference dynamics for agents
based on their contextual information. This motivates us to consider a novel
Contextual Online Matching Bandit prOblem (COMBO), which allows dynamic
preferences in matching decisions. Existing works focus on multi-armed bandit
with static preference, but this is insufficient: the two-sided preference
changes as along as one-side's contextual information updates, resulting in
non-static matching. In this paper, we propose a Centralized Contextual -
Explore Then Commit (CC-ETC) algorithm to adapt to the COMBO. CC-ETC solves
online matching with dynamic preference. In theory, we show that CC-ETC
achieves a sublinear regret upper bound O(log(T)) and is a rate-optimal
algorithm by proving a matching lower bound. In the experiments, we demonstrate
that CC-ETC is robust to variant preference schemes, dimensions of contexts,
reward noise levels, and contexts variation levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Distributional and Risk-sensitive Reinforcement Learning with
  Provable Regret Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Zhi-Quan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the regret guarantee for risk-sensitive reinforcement learning
(RSRL) via distributional reinforcement learning (DRL) methods. In particular,
we consider finite episodic Markov decision processes whose objective is the
entropic risk measure (EntRM) of return. We identify a key property of the
EntRM, the monotonicity-preserving property, which enables the risk-sensitive
distributional dynamic programming framework. We then propose two novel DRL
algorithms that implement optimism through two different schemes, including a
model-free one and a model-based one.
  We prove that both of them attain $\tilde{\mathcal{O}}(\frac{\exp(|\beta|
H)-1}{|\beta|H}H\sqrt{HS^2AT})$ regret upper bound, where $S$ is the number of
states, $A$ the number of states, $H$ the time horizon and $T$ the number of
total time steps. It matches RSVI2 proposed in \cite{fei2021exponential} with a
much simpler regret analysis. To the best of our knowledge, this is the first
regret analysis of DRL, which bridges DRL and RSRL in terms of sample
complexity. Finally, we improve the existing lower bound by proving a tighter
bound of $\Omega(\frac{\exp(\beta H/6)-1}{\beta H}H\sqrt{SAT})$ for $\beta>0$
case, which recovers the tight lower bound $\Omega(H\sqrt{SAT})$ in the
risk-neutral setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP
  Initialization <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Han, Tong Zhao, Yozen Liu, Xia Hu, Neil Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training graph neural networks (GNNs) on large graphs is complex and
extremely time consuming. This is attributed to overheads caused by sparse
matrix multiplication, which are sidestepped when training multi-layer
perceptrons (MLPs) with only node features. MLPs, by ignoring graph context,
are simple and faster for graph data, however they usually sacrifice prediction
accuracy, limiting their applications for graph data. We observe that for most
message passing-based GNNs, we can trivially derive an analog MLP (we call this
a PeerMLP) with an equivalent weight space, by setting the trainable parameters
with the same shapes, making us curious about \textbf{\emph{how do GNNs using
weights from a fully trained PeerMLP perform?}} Surprisingly, we find that GNNs
initialized with such weights significantly outperform their PeerMLPs,
motivating us to use PeerMLP training as a precursor, initialization step to
GNN training. To this end, we propose an embarrassingly simple, yet hugely
effective initialization method for GNN training acceleration, called MLPInit.
Our extensive experiments on multiple large-scale graph datasets with diverse
GNN architectures validate that MLPInit can accelerate the training of GNNs (up
to 33X speedup on OGB-Products) and often improve prediction performance (e.g.,
up to $7.97\%$ improvement for GraphSAGE across $7$ datasets for node
classification, and up to $17.81\%$ improvement across $4$ datasets for link
prediction on metric Hits@10). The code is available at
\href{https://github.com/snap-research/MLPInit-for-GNNs}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Deep Neural Networks Have Zero Duality Gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06482v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06482v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Wang, Tolga Ergen, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks is a challenging non-convex optimization
problem. Recent work has proven that the strong duality holds (which means zero
duality gap) for regularized finite-width two-layer ReLU networks and
consequently provided an equivalent convex training problem. However, extending
this result to deeper networks remains to be an open problem. In this paper, we
prove that the duality gap for deeper linear networks with vector outputs is
non-zero. In contrast, we show that the zero duality gap can be obtained by
stacking standard deep networks in parallel, which we call a parallel
architecture, and modifying the regularization. Therefore, we prove the strong
duality and existence of equivalent convex problems that enable globally
optimal training of deep networks. As a by-product of our analysis, we
demonstrate that the weight decay regularization on the network parameters
explicitly encourages low-rank solutions via closed-form expressions. In
addition, we show that strong duality holds for three-layer standard ReLU
networks given rank-1 data matrices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Machine Learning: A <span class="highlight-title">Survey</span> on Problems, Methods and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances of data-driven machine learning have revolutionized fields
like computer vision, reinforcement learning, and many scientific and
engineering domains. In many real-world and scientific problems, systems that
generate data are governed by physical laws. Recent work shows that it provides
potential benefits for machine learning models by incorporating the physical
prior and collected data, which makes the intersection of machine learning and
physics become a prevailing paradigm. By integrating the data and mathematical
physics models seamlessly, it can guide the machine learning model towards
solutions that are physically plausible, improving accuracy and efficiency even
in uncertain and high-dimensional contexts. In this survey, we present this
learning paradigm called Physics-Informed Machine Learning (PIML) which is to
build a model that leverages empirical data and available physical prior
knowledge to improve performance on a set of tasks that involve a physical
mechanism. We systematically review the recent development of physics-informed
machine learning from three perspectives of machine learning tasks,
representation of physical prior, and methods for incorporating physical prior.
We also propose several important open research problems based on the current
trends in the field. We argue that encoding different forms of physical prior
into model architectures, optimizers, inference algorithms, and significant
domain-specific applications like inverse engineering design and robotic
control is far from being fully explored in the field of physics-informed
machine learning. We believe that the interdisciplinary research of
physics-informed machine learning will significantly propel research progress,
foster the creation of more effective machine learning models, and also offer
invaluable assistance in addressing long-standing problems in related
disciplines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight Certification of Adversarially Trained Neural Networks via
  Nonconvex Low-Rank Semidefinite Relaxations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Ming Chiu, Richard Y. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is well-known to produce high-quality neural network
models that are empirically robust against adversarial perturbations.
Nevertheless, once a model has been adversarially trained, one often desires a
certification that the model is truly robust against all future attacks.
Unfortunately, when faced with adversarially trained models, all existing
approaches have significant trouble making certifications that are strong
enough to be practically useful. Linear programming (LP) techniques in
particular face a "convex relaxation barrier" that prevent them from making
high-quality certifications, even after refinement with mixed-integer linear
programming (MILP) techniques, and even when using state-of-the-art
computational facilities. In this paper, we propose a nonconvex certification
technique, based on a low-rank restriction of a semidefinite programming (SDP)
relaxation. The nonconvex relaxation makes strong certifications comparable to
much more expensive SDP methods, while optimizing over dramatically fewer
variables comparable to much weaker LP methods. Despite nonconvexity, we show
how off-the-shelf local optimization algorithms can be used to achieve and to
certify global optimality in polynomial time. Our experiments find that the
nonconvex relaxation almost completely closes the gap towards exact
certification of adversarially trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gluformer: <span class="highlight-title">Transformer</span>-Based <span class="highlight-title">Persona</span>lized Glucose Forecasting with
  Uncertainty Quantification <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renat Sergazinov, Mohammadreza Armandpour, Irina Gaynanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models achieve state-of-the art results in predicting blood
glucose trajectories, with a wide range of architectures being proposed.
However, the adaptation of such models in clinical practice is slow, largely
due to the lack of uncertainty quantification of provided predictions. In this
work, we propose to model the future glucose trajectory conditioned on the past
as an infinite mixture of basis distributions (i.e., Gaussian, Laplace, etc.).
This change allows us to learn the uncertainty and predict more accurately in
the cases when the trajectory has a heterogeneous or multi-modal distribution.
To estimate the parameters of the predictive distribution, we utilize the
Transformer architecture. We empirically demonstrate the superiority of our
method over existing state-of-the-art techniques both in terms of accuracy and
uncertainty on the synthetic and benchmark glucose data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, IEEE ICASSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIP: Towards Universal Visual Reward and Representation via
  Value-Implicit <span class="highlight-title">Pre-Train</span>ing <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, Amy Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward and representation learning are two long-standing challenges for
learning an expanding set of robot manipulation skills from sensory
observations. Given the inherent cost and scarcity of in-domain, task-specific
robot data, learning from large, diverse, offline human videos has emerged as a
promising path towards acquiring a generally useful visual representation for
control; however, how these human videos can be used for general-purpose reward
learning remains an open question. We introduce
$\textbf{V}$alue-$\textbf{I}$mplicit $\textbf{P}$re-training (VIP), a
self-supervised pre-trained visual representation capable of generating dense
and smooth reward functions for unseen robotic tasks. VIP casts representation
learning from human videos as an offline goal-conditioned reinforcement
learning problem and derives a self-supervised dual goal-conditioned
value-function objective that does not depend on actions, enabling pre-training
on unlabeled human videos. Theoretically, VIP can be understood as a novel
implicit time contrastive objective that generates a temporally smooth
embedding, enabling the value function to be implicitly defined via the
embedding distance, which can then be used to construct the reward for any
goal-image specified downstream task. Trained on large-scale Ego4D human videos
and without any fine-tuning on in-domain, task-specific data, VIP's frozen
representation can provide dense visual reward for an extensive set of
simulated and $\textbf{real-robot}$ tasks, enabling diverse reward-based visual
control methods and significantly outperforming all prior pre-trained
representations. Notably, VIP can enable simple, $\textbf{few-shot}$ offline RL
on a suite of real-world robot tasks with as few as 20 trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023, Notable-Top-25% (Spotlight). Project website:
  https://sites.google.com/view/vip-rl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Reinforcement Learning Approach for Finding Non-Exploitable
  Strategies in Two-Player Atari Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Ding, Dijia Su, Qinghua Liu, Chi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes new, end-to-end deep reinforcement learning algorithms
for learning two-player zero-sum Markov games. Different from prior efforts on
training agents to beat a fixed set of opponents, our objective is to find the
Nash equilibrium policies that are free from exploitation by even the
adversarial opponents. We propose (a) Nash-DQN algorithm, which integrates the
deep learning techniques from single DQN into the classic Nash Q-learning
algorithm for solving tabular Markov games; (b) Nash-DQN-Exploiter algorithm,
which additionally adopts an exploiter to guide the exploration of the main
agent. We conduct experimental evaluation on tabular examples as well as
various two-player Atari games. Our empirical results demonstrate that (i) the
policies found by many existing methods including Neural Fictitious Self Play
and Policy Space Response Oracle can be prone to exploitation by adversarial
opponents; (ii) the output policies of our algorithms are robust to
exploitation, and thus outperform existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Memory- and Time-Efficient Backpropagation for Training Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyan Meng, Mingqing Xiao, Shen Yan, Yisen Wang, Zhouchen Lin, Zhi-Quan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) are promising energy-efficient models for
neuromorphic computing. For training the non-differentiable SNN models, the
backpropagation through time (BPTT) with surrogate gradients (SG) method has
achieved high performance. However, this method suffers from considerable
memory cost and training time during training. In this paper, we propose the
Spatial Learning Through Time (SLTT) method that can achieve high performance
while greatly improving training efficiency compared with BPTT. First, we show
that the backpropagation of SNNs through the temporal domain contributes just a
little to the final calculated gradients. Thus, we propose to ignore the
unimportant routes in the computational graph during backpropagation. The
proposed method reduces the number of scalar multiplications and achieves a
small memory occupation that is independent of the total time steps.
Furthermore, we propose a variant of SLTT, called SLTT-K, that allows
backpropagation only at K time steps, then the required number of scalar
multiplications is further reduced and is independent of the total time steps.
Experiments on both static and neuromorphic datasets demonstrate superior
training efficiency and performance of our SLTT. In particular, our method
achieves state-of-the-art accuracy on ImageNet, while the memory cost and
training time are reduced by more than 70% and 50%, respectively, compared with
BPTT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Semi-supervised Medical Image Segmentation with
  Anatomical-aware Contrastive Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown great promise over annotation scarcity
problems in the context of medical image segmentation. Existing approaches
typically assume a balanced class distribution for both labeled and unlabeled
medical images. However, medical image data in reality is commonly imbalanced
(i.e., multi-class label imbalance), which naturally yields blurry contours and
usually incorrectly labels rare objects. Moreover, it remains unclear whether
all negative samples are equally negative. In this work, we present ACTION, an
Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised
medical image segmentation. Specifically, we first develop an iterative
contrastive distillation algorithm by softly labeling the negatives rather than
binary supervision between positive and negative pairs. We also capture more
semantically similar features from the randomly chosen negative set compared to
the positives to enforce the diversity of the sampled data. Second, we raise a
more important question: Can we really handle imbalanced samples to yield
better performance? Hence, the key innovation in ACTION is to learn global
semantic relationship across the entire dataset and local anatomical features
among the neighbouring pixels with minimal additional memory footprint. During
the training, we introduce anatomical contrast by actively sampling a sparse
set of hard negative pixels, which can generate smoother segmentation
boundaries and more accurate predictions. Extensive experiments across two
benchmark datasets and different unlabeled settings show that ACTION
significantly outperforms the current state-of-the-art semi-supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reparameterization through Spatial Gradient Scaling <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Detkov, Mohammad Salameh, Muhammad Fetrat Qharabagh, Jialin Zhang, Wei Lui, Shangling Jui, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reparameterization aims to improve the generalization of deep neural networks
by transforming convolutional layers into equivalent multi-branched structures
during training. However, there exists a gap in understanding how
reparameterization may change and benefit the learning process of neural
networks. In this paper, we present a novel spatial gradient scaling method to
redistribute learning focus among weights in convolutional networks. We prove
that spatial gradient scaling achieves the same learning dynamics as a branched
reparameterization yet without introducing structural changes into the network.
We further propose an analytical approach that dynamically learns scalings for
each convolutional layer based on the spatial characteristics of its input
feature map gauged by mutual information. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that without searching for reparameterized structures, our
proposed scaling method outperforms the state-of-the-art reparameterization
strategies at a lower computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023. Code available at
  https://github.com/Ascend-Research/Reparameterization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NSGA-PINN: A Multi-Objective Optimization Method for Physics-Informed
  Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binghang Lu, Christian B. Moya, Guang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents NSGA-PINN, a multi-objective optimization framework for
effective training of Physics-Informed Neural Networks (PINNs). The proposed
framework uses the Non-dominated Sorting Genetic Algorithm (NSGA-II) to enable
traditional stochastic gradient optimization algorithms (e.g., ADAM) to escape
local minima effectively. Additionally, the NSGA-II algorithm enables
satisfying the initial and boundary conditions encoded into the loss function
during physics-informed training precisely. We demonstrate the effectiveness of
our framework by applying NSGA-PINN to several ordinary and partial
differential equation problems. In particular, we show that the proposed
framework can handle challenging inverse problems with noisy data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 35 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Tang, Yatong Chen, Yang Liu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of long-term fairness involves the interplay between
decision-making and the underlying data generating process. In this paper,
through causal modeling with a directed acyclic graph (DAG) on the
decision-distribution interplay, we investigate the possibility of achieving
long-term fairness from a dynamic perspective. We propose Tier Balancing, a
technically more challenging but more natural notion to achieve in the context
of long-term, dynamic fairness analysis. Different from previous fairness
notions that are defined purely on observed variables, our notion goes one step
further, capturing behind-the-scenes situation changes on the unobserved latent
causal factors that directly carry out the influence from the current decision
to the future data distribution. Under the specified dynamics, we prove that in
general one cannot achieve the long-term fairness goal only through one-step
interventions. Furthermore, in the effort of approaching long-term fairness, we
consider the mission of "getting closer to" the long-term fairness goal and
present possibility and impossibility results accordingly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerate the Warm-up Stage in the Lasso Computation via a Homotopic
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.13934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.13934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Zhao, Xiaoming Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In optimization, it is known that when the objective functions are strictly
convex and well-conditioned, gradient-based approaches can be extremely
effective, e.g., achieving the exponential rate of convergence. On the other
hand, the existing Lasso-type estimator in general cannot achieve the optimal
rate due to the undesirable behavior of the absolute function at the origin. A
homotopic method is to use a sequence of surrogate functions to approximate the
$\ell_1$ penalty that is used in the Lasso-type of estimators. The surrogate
functions will converge to the $\ell_1$ penalty in the Lasso estimator. At the
same time, each surrogate function is strictly convex, which enables a provable
faster numerical rate of convergence. In this paper, we demonstrate that by
meticulously defining the surrogate functions, one can prove a faster numerical
convergence rate than any existing methods in computing for the Lasso-type of
estimators. Namely, the state-of-the-art algorithms can only guarantee
$O(1/\epsilon)$ or $O(1/\sqrt{\epsilon})$ convergence rates, while we can prove
an $O([\log(1/\epsilon)]^2)$ for the newly proposed algorithm. Our numerical
simulations show that the new algorithm also performs better empirically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian Metric Learning via Optimal Transport <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Scarvelis, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an optimal transport-based model for learning a metric tensor
from cross-sectional samples of evolving probability measures on a common
Riemannian manifold. We neurally parametrize the metric as a spatially-varying
matrix field and efficiently optimize our model's objective using a simple
alternating scheme. Using this learned metric, we can nonlinearly interpolate
between probability measures and compute geodesics on the manifold. We show
that metrics learned using our method improve the quality of trajectory
inference on scRNA and bird migration data at the cost of little additional
cross-sectional data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low Budget Active Learning via Wasserstein Distance: An Integer
  Programming Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02968v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02968v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafid Mahmood, Sanja Fidler, Marc T. Law
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning is the process of training a model with limited labeled data
by selecting a core subset of an unlabeled data pool to label. The large scale
of data sets used in deep learning forces most sample selection strategies to
employ efficient heuristics. This paper introduces an integer optimization
problem for selecting a core set that minimizes the discrete Wasserstein
distance from the unlabeled pool. We demonstrate that this problem can be
tractably solved with a Generalized Benders Decomposition algorithm. Our
strategy uses high-quality latent features that can be obtained by unsupervised
learning on the unlabeled pool. Numerical results on several data sets show
that our optimization approach is competitive with baselines and particularly
outperforms them in the low budget regime where less than one percent of the
data set is labeled.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-in-the-Loop Mixup 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Umang Bhatt, Weiyang Liu, Vihari Piratla, Ilia Sucholutsky, Bradley Love, Adrian Weller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning model representations to humans has been found to improve robustness
and generalization. However, such methods often focus on standard observational
data. Synthetic data is proliferating and powering many advances in machine
learning; yet, it is not always clear whether synthetic labels are perceptually
aligned to humans -- rendering it likely model representations are not human
aligned. We focus on the synthetic data used in mixup: a powerful regularizer
shown to improve model robustness, generalization, and calibration. We design a
comprehensive series of elicitation interfaces, which we release as HILL MixE
Suite, and recruit 159 participants to provide perceptual judgments along with
their uncertainties, over mixup examples. We find that human perceptions do not
consistently align with the labels traditionally used for synthetic points, and
begin to demonstrate the applicability of these findings to potentially
increase the reliability of downstream models, particularly when incorporating
human uncertainty. We release all elicited judgments in a new data hub we call
H-Mix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01888v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01888v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young Wu, Jeremy McMahan, Xiaojin Zhu, Qiaomin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In offline multi-agent reinforcement learning (MARL), agents estimate
policies from a given dataset. We study reward-poisoning attacks in this
setting where an exogenous attacker modifies the rewards in the dataset before
the agents see the dataset. The attacker wants to guide each agent into a
nefarious target policy while minimizing the $L^p$ norm of the reward
modification. Unlike attacks on single-agent RL, we show that the attacker can
install the target policy as a Markov Perfect Dominant Strategy Equilibrium
(MPDSE), which rational agents are guaranteed to follow. This attack can be
significantly cheaper than separate single-agent attacks. We show that the
attack works on various MARL agents including uncertainty-aware learners, and
we exhibit linear programs to efficiently solve the attack problem. We also
study the relationship between the structure of the datasets and the minimal
attack cost. Our work paves the way for studying defense in offline MARL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vector Optimization with Stochastic Bandit Feedback <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.12311v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.12311v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Çağın Ararat, Cem Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce vector optimization problems with stochastic bandit feedback, in
which preferences among designs are encoded by a polyhedral ordering cone $C$.
Our setup generalizes the best arm identification problem to vector-valued
rewards by extending the concept of Pareto set beyond multi-objective
optimization. We characterize the sample complexity of ($\epsilon,\delta$)-PAC
Pareto set identification by defining a new cone-dependent notion of
complexity, called the ordering complexity. In particular, we provide
gap-dependent and worst-case lower bounds on the sample complexity and show
that, in the worst-case, the sample complexity scales with the square of
ordering complexity. Furthermore, we investigate the sample complexity of the
na\"ive elimination algorithm and prove that it nearly matches the worst-case
sample complexity. Finally, we run experiments to verify our theoretical
results and illustrate how $C$ and sampling budget affect the Pareto set, the
returned ($\epsilon,\delta$)-PAC Pareto set, and the success of identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 3 tables, 2 figure; Proceedings of the 26th International
  Conference on Artificial Intelligence and Statistics (AISTATS) 2023,
  Valencia, Spain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal quantum <span class="highlight-title">dataset</span> for learning a unitary transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00546v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00546v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Yu, Xuanqiang Zhao, Benchi Zhao, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unitary transformations formulate the time evolution of quantum states. How
to learn a unitary transformation efficiently is a fundamental problem in
quantum machine learning. The most natural and leading strategy is to train a
quantum machine learning model based on a quantum dataset. Although the
presence of more training data results in better models, using too much data
reduces the efficiency of training. In this work, we solve the problem on the
minimum size of sufficient quantum datasets for learning a unitary
transformation exactly, which reveals the power and limitation of quantum data.
First, we prove that the minimum size of a dataset with pure states is $2^n$
for learning an $n$-qubit unitary transformation. To fully explore the
capability of quantum data, we introduce a practical quantum dataset consisting
of $n+1$ elementary tensor product states that are sufficient for exact
training. The main idea is to simplify the structure utilizing decoupling,
which leads to an exponential improvement in the size of the datasets with pure
states. Furthermore, we show that the size of the quantum dataset with mixed
states can be reduced to a constant, which yields an optimal quantum dataset
for learning a unitary. We showcase the applications of our results in oracle
compiling and Hamiltonian simulation. Notably, to accurately simulate a 3-qubit
one-dimensional nearest-neighbor Heisenberg model, our circuit only uses $96$
elementary quantum gates, which is significantly less than $4080$ gates in the
circuit constructed by the Trotter-Suzuki product formula.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages including appendix, v2 added remarks and references, v3 is
  closed to the published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effects of Parameter Norm Growth During <span class="highlight-title">Transformer</span> Training: Inductive
  Bias from Gradient Descent <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.09697v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.09697v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such "saturated" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at EMNLP 2021. March 7, 2023: Removed irreproducible numbers
  reported in a footnote with erratum note</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability in Deep Reinforcement Learning, a <span class="highlight-title">Review</span> into Current
  Methods and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hickling, Abdelhafid Zenati, Nabil Aouf, Phillippa Spencer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found, they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem, the field of Explainable Artificial Intelligence (XAI) has emerged.
This entails a variety of different methods that look to open the DRL black
boxes, ranging from the use of interpretable symbolic Decision Trees (DT) to
numerical methods like Shapley Values. This review looks at which methods are
being used and for which applications. This is done to identify which models
are the best suited to each application or if a method is being underutilised.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures, Paper Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Adaptive Meta-Learning Framework for Advancing Spatial
  Generalizability <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexiong Liu, Licheng Liu, Yiqun Xie, Zhenong Jin, Xiaowei Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal machine learning is critically needed for a variety of
societal applications, such as agricultural monitoring, hydrological forecast,
and traffic management. These applications greatly rely on regional features
that characterize spatial and temporal differences. However, spatio-temporal
data often exhibit complex patterns and significant data variability across
different locations. The labels in many real-world applications can also be
limited, which makes it difficult to separately train independent models for
different locations. Although meta learning has shown promise in model
adaptation with small samples, existing meta learning methods remain limited in
handling a large number of heterogeneous tasks, e.g., a large number of
locations with varying data patterns. To bridge the gap, we propose
task-adaptive formulations and a model-agnostic meta-learning framework that
ensembles regionally heterogeneous data into location-sensitive meta tasks. We
conduct task adaptation following an easy-to-hard task hierarchy in which
different meta models are adapted to tasks of different difficulty levels. One
major advantage of our proposed method is that it improves the model adaptation
to a large number of heterogeneous tasks. It also enhances the model
generalization by automatically adapting the meta model of the corresponding
difficulty level to any new tasks. We demonstrate the superiority of our
proposed framework over a diverse set of baselines and state-of-the-art
meta-learning frameworks. Our extensive experiments on real crop yield data
show the effectiveness of the proposed method in handling spatial-related
heterogeneous tasks in real societal applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the Thirty-Seventh AAAI Conference on Artificial Intelligence,
  February 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Py-Feat: Python Facial Expression Analysis Toolbox 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.03509v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.03509v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hyun Cheong, Eshin Jolly, Tiankang Xie, Sophie Byrne, Matthew Kenney, Luke J. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying facial expressions is a notoriously difficult endeavor. Recent
advances in the field of affective computing have yielded impressive progress
in automatically detecting facial expressions from pictures and videos.
However, much of this work has yet to be widely disseminated in social science
domains such as psychology. Current state of the art models require
considerable domain expertise that is not traditionally incorporated into
social science training programs. Furthermore, there is a notable absence of
user-friendly and open-source software that provides a comprehensive set of
tools and functions that support facial expression research. In this paper, we
introduce Py-Feat, an open-source Python toolbox that provides support for
detecting, preprocessing, analyzing, and visualizing facial expression data.
Py-Feat makes it easy for domain experts to disseminate and benchmark computer
vision models and also for end users to quickly process, analyze, and visualize
face expression data. We hope this platform will facilitate increased use of
facial expression data in human behavior research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating a Brain Network Predictive of Stress and Genotype with
  Supervised Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.05209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.05209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin Talbot, David Dunson, Kafui Dzirasa, David Carlson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Targeted stimulation of the brain has the potential to treat mental
illnesses. We propose an approach to help design the stimulation protocol by
identifying electrical dynamics across many brain regions that relate to
illness states. We model multi-region electrical activity as a superposition of
activity from latent networks, where the weights on the latent networks relate
to an outcome of interest. In order to improve on drawbacks of latent factor
modeling in this context, we focus on supervised autoencoders (SAEs), which can
improve predictive performance while maintaining a generative model. We explain
why SAEs yield improved predictions, describe the distributional assumptions
under which SAEs are an appropriate modeling choice, and provide modeling
constraints to ensure biological relevance of the learned network. We use the
analysis strategy to find a network associated with stress that characterizes a
genotype associated with bipolar disorder. This discovered network aligns with
a previously used stimulation technique, providing experimental validation of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Open Catalyst 2022 (OC22) <span class="highlight-title">Dataset</span> and Challenges for Oxide
  Electrocatalysts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, Felix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, C. Lawrence Zitnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of machine learning models for electrocatalysts requires a
broad set of training data to enable their use across a wide variety of
materials. One class of materials that currently lacks sufficient training data
is oxides, which are critical for the development of OER catalysts. To address
this, we developed the OC22 dataset, consisting of 62,331 DFT relaxations
(~9,854,504 single point calculations) across a range of oxide materials,
coverages, and adsorbates. We define generalized total energy tasks that enable
property prediction beyond adsorption energies; we test baseline performance of
several graph neural networks; and we provide pre-defined dataset splits to
establish clear benchmarks for future efforts. In the most general task,
GemNet-OC sees a ~36% improvement in energy predictions when combining the
chemically dissimilar OC20 and OC22 datasets via fine-tuning. Similarly, we
achieved a ~19% improvement in total energy predictions on OC20 and a ~9%
improvement in force predictions in OC22 when using joint training. We
demonstrate the practical utility of a top performing model by capturing
literature adsorption energies and important OER scaling relationships. We
expect OC22 to provide an important benchmark for models seeking to incorporate
intricate long-range electrostatic and magnetic interactions in oxide surfaces.
Dataset and baseline models are open sourced, and a public leaderboard is
available to encourage continued community developments on the total energy
tasks and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Collapse with Normalized Features: A Geometric Analysis over the
  Riemannian Manifold <span class="chip">NeurIPS'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training overparameterized deep networks for classification tasks, it
has been widely observed that the learned features exhibit a so-called "neural
collapse" phenomenon. More specifically, for the output features of the
penultimate layer, for each class the within-class features converge to their
means, and the means of different classes exhibit a certain tight frame
structure, which is also aligned with the last layer's classifier. As feature
normalization in the last layer becomes a common practice in modern
representation learning, in this work we theoretically justify the neural
collapse phenomenon for normalized features. Based on an unconstrained feature
model, we simplify the empirical loss function in a multi-class classification
task into a nonconvex optimization problem over the Riemannian manifold by
constraining all features and classifiers over the sphere. In this context, we
analyze the nonconvex landscape of the Riemannian optimization problem over the
product of spheres, showing a benign global landscape in the sense that the
only global minimizers are the neural collapse solutions while all other
critical points are strict saddles with negative curvature. Experimental
results on practical deep networks corroborate our theory and demonstrate that
better representations can be learned faster via feature normalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed to this work equally; 38 pages, 13
  figures. Accepted at NeurIPS'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Generalization Power of Overfitted Two-Layer Neural Tangent
  Kernel Models <span class="chip">ICML21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.05243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.05243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhong Ju, Xiaojun Lin, Ness B. Shroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the generalization performance of min $\ell_2$-norm
overfitting solutions for the neural tangent kernel (NTK) model of a two-layer
neural network with ReLU activation that has no bias term. We show that,
depending on the ground-truth function, the test error of overfitted NTK models
exhibits characteristics that are different from the "double-descent" of other
overparameterized linear models with simple Fourier or Gaussian features.
Specifically, for a class of learnable functions, we provide a new upper bound
of the generalization error that approaches a small limiting value, even when
the number of neurons $p$ approaches infinity. This limiting value further
decreases with the number of training samples $n$. For functions outside of
this class, we provide a lower bound on the generalization error that does not
diminish to zero even when $n$ and $p$ are both large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML21. This version fixes an error of Lemma 31 and
  other parts affected by this error. The main results remain the same except
  some small changes on certain coefficients of Eq.(9)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Representation Learning for Instantaneous and Temporal Effects in
  Interactive Systems <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M. Asano, Taco Cohen, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal representation learning is the task of identifying the underlying
causal variables and their relations from high-dimensional observations, such
as images. Recent work has shown that one can reconstruct the causal variables
from temporal sequences of observations under the assumption that there are no
instantaneous causal relations between them. In practical applications,
however, our measurement or frame rate might be slower than many of the causal
effects. This effectively creates "instantaneous" effects and invalidates
previous identifiability results. To address this issue, we propose iCITRIS, a
causal representation learning method that allows for instantaneous effects in
intervened temporal sequences when intervention targets can be observed, e.g.,
as actions of an agent. iCITRIS identifies the potentially multidimensional
causal variables from temporal observations, while simultaneously using a
differentiable causal discovery method to learn their causal graph. In
experiments on three datasets of interactive systems, iCITRIS accurately
identifies the causal variables and their causal graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Learning Representations
  (ICLR), 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud
  Compression <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Sheng Liu, Jia-Fong Yeh, Hao Hsu, Hung-Ting Su, Ming-Sui Lee, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large amount of data collected by LiDAR sensors brings the issue of LiDAR
point cloud compression (PCC). Previous works on range image-based LiDAR PCC
follow the predictive coding paradigm, structuring a simple prototype of a
coding framework. However, their prediction methods give an inaccurate result
due to the negligence of invalid pixels in range images and the omission of
future frames in the time step. Moreover, their handcrafted design of residual
coding methods could not fully exploit spatial redundancy. To remedy this, we
propose a coding framework BIRD-PCC. Our prediction module is aware of the
coordinates of invalid pixels in range images and takes a bidirectional scheme.
Also, we introduce a deep-learned residual coding module that can further
exploit spatial redundancy within a residual frame. Experiments conducted on
SemanticKITTI and KITTI-360 datasets show that BIRD-PCC outperforms other
methods in most bitrate conditions and generalizes well to unseen environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">Pre-train</span>ed AudioLDM for Sound <span class="highlight-title">Generation</span>: A Benchmark Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yuan, Haohe Liu, Jinhua Liang, Xubo Liu, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have recently achieved breakthroughs in sound
generation. Despite the outstanding sample quality, current sound generation
models face issues on small-scale datasets (e.g., overfitting and low coverage
of sound classes), significantly limiting performance. In this paper, we make
the first attempt to investigate the benefits of pre-training on sound
generation with AudioLDM, the cutting-edge model for audio generation, as the
backbone. Our study demonstrates the advantages of the pre-trained AudioLDM,
especially in data-scarcity scenarios. In addition, the baselines and
evaluation protocol for sound generation systems are not consistent enough to
compare different studies directly. Aiming to facilitate further study on sound
generation tasks, we benchmark the sound generation task on various
frequently-used datasets. We hope our results on transfer learning and
benchmarks can provide references for further research on conditional sound
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FSVVD: A <span class="highlight-title">Dataset</span> of Full Scene Volumetric Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyuan Hu, Yili Jin, Haowen Yang, Junhua Liu, Fangxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a rapid development of immersive multimedia which
bridges the gap between the real world and virtual space. Volumetric videos, as
an emerging representative 3D video paradigm that empowers extended reality,
stand out to provide unprecedented immersive and interactive video watching
experience. Despite the tremendous potential, the research towards 3D
volumetric video is still in its infancy, relying on sufficient and complete
datasets for further exploration. However, existing related volumetric video
datasets mostly only include a single object, lacking details about the scene
and the interaction between them. In this paper, we focus on the current most
widely used data format, point cloud, and for the first time release a
full-scene volumetric video dataset that includes multiple people and their
daily activities interacting with the external environments. Comprehensive
dataset description and analysis are conducted, with potential usage of this
dataset. The dataset and additional tools can be accessed via the following
website: https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MMSys'23 Open Dataset and Software Track, A preliminary
  version. The dataset and additional tools can be accessed via
  https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approach to Learning Generalized Audio Representation Through Batch
  Embedding Covariance Regularization and Constant-Q Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, Bhiksha Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose embedding is highly desirable for few-shot even zero-shot
learning in many application scenarios, including audio tasks. In order to
understand representations better, we conducted a thorough error analysis and
visualization of HEAR 2021 submission results. Inspired by the analysis, this
work experiments with different front-end audio preprocessing methods,
including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),
and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover
a more holistic simulation of the frequency information received by the human
auditory system. We tested the models on the suite of HEAR 2021 tasks, which
encompass a broad category of tasks. Preliminary results show (1) the proposed
BECR can incur a more dispersed embedding on the test set, (2) BECR improves
the PaSST model without extra computation complexity, and (3) STFT
preprocessing outperforms CQT in all tasks we tested.
Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compose & Embellish: Well-Structured Piano Performance <span class="highlight-title">Generation</span> via A
  Two-Stage Approach <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08212v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08212v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Lun Wu, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even with strong sequence models like Transformers, generating expressive
piano performances with long-range musical structures remains challenging.
Meanwhile, methods to compose well-structured melodies or lead sheets (melody +
chords), i.e., simpler forms of music, gained more success. Observing the
above, we devise a two-stage Transformer-based framework that Composes a lead
sheet first, and then Embellishes it with accompaniment and expressive touches.
Such a factorization also enables pretraining on non-piano data. Our objective
and subjective experiments show that Compose & Embellish shrinks the gap in
structureness between a current state of the art and real performances by half,
and improves other musical aspects such as richness and coherence as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Predictive Analytics in Reversible Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang, Xu Wang, Sisheng Chen, Isao Echizen, Victor Sanchez, Chang-Tsun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is regarded as a promising solution for reversible
steganography. There is an accelerating trend of representing a reversible
steo-system by monolithic neural networks, which bypass intermediate operations
in traditional pipelines of reversible steganography. This end-to-end paradigm,
however, suffers from imperfect reversibility. By contrast, the modular
paradigm that incorporates neural networks into modules of traditional
pipelines can stably guarantee reversibility with mathematical explainability.
Prediction-error modulation is a well-established reversible steganography
pipeline for digital images. It consists of a predictive analytics module and a
reversible coding module. Given that reversibility is governed independently by
the coding module, we narrow our focus to the incorporation of neural networks
into the analytics module, which serves the purpose of predicting pixel
intensities and a pivotal role in determining capacity and imperceptibility.
The objective of this study is to evaluate the impacts of different training
configurations upon predictive accuracy of neural networks and provide
practical insights. In particular, we investigate how different initialisation
strategies for input images may affect the learning process and how different
training strategies for dual-layer prediction respond to the problem of
distributional shift. Furthermore, we compare steganographic performance of
various model architectures with different loss functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Neural Networks for Reversible Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have led to a paradigm shift in the field of
reversible steganography. A fundamental pillar of reversible steganography is
predictive modelling which can be realised via deep neural networks. However,
non-trivial errors exist in inferences about some out-of-distribution and noisy
data. In view of this issue, we propose to consider uncertainty in predictive
models based upon a theoretical framework of Bayesian deep learning, thereby
creating an adaptive steganographic system. Most modern deep-learning models
are regarded as deterministic because they only offer predictions while failing
to provide uncertainty measurement. Bayesian neural networks bring a
probabilistic perspective to deep learning and can be regarded as self-aware
intelligent machinery; that is, a machine that knows its own limitations. To
quantify uncertainty, we apply Bayesian statistics to model the predictive
distribution and approximate it through Monte Carlo sampling with stochastic
forward passes. We further show that predictive uncertainty can be disentangled
into aleatoric and epistemic uncertainties and these quantities can be learnt
unsupervised. Experimental results demonstrate an improvement delivered by
Bayesian uncertainty analysis upon steganographic rate-distortion performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the predictability in reversible steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang, Xu Wang, Sisheng Chen, Hitoshi Kiya, Isao Echizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks have advanced the frontiers of reversible
steganography. The core strength of neural networks is the ability to render
accurate predictions for a bewildering variety of data. Residual modulation is
recognised as the most advanced reversible steganographic algorithm for digital
images. The pivot of this algorithm is predictive analytics in which pixel
intensities are predicted given some pixel-wise contextual information. This
task can be perceived as a low-level vision problem and hence neural networks
for addressing a similar class of problems can be deployed. On top of the prior
art, this paper investigates predictability of pixel intensities based on
supervised and unsupervised learning frameworks. Predictability analysis
enables adaptive data embedding, which in turn leads to a better trade-off
between capacity and imperceptibility. While conventional methods estimate
predictability by the statistics of local image patterns, learning-based
frameworks consider further the degree to which correct predictions can be made
by a designated predictor. Not only should the image patterns be taken into
account but also the predictor in use. Experimental results show that
steganographic performance can be significantly improved by incorporating the
learning-based predictability analysers into a reversible steganographic
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Video Quality Assessment on User <span class="highlight-title">Generate</span>d Contents from
  Aesthetic and Technical Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in user-generated-content (UGC) videos calls for the
development of effective video quality assessment (VQA) algorithms. However,
the objective of the UGC-VQA problem is still ambiguous and can be viewed from
two perspectives: the technical perspective, measuring the perception of
distortions; and the aesthetic perspective, which relates to preference and
recommendation on contents. To understand how these two perspectives affect
overall subjective opinions in UGC-VQA, we conduct a large-scale subjective
study to collect human quality opinions on overall quality of videos as well as
perceptions from aesthetic and technical perspectives. The collected
Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality
opinions on UGC videos are universally and inevitably affected by both
aesthetic and technical perspectives. In light of this, we propose the
Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of
UGC videos based on the two perspectives. The DOVER proves state-of-the-art
performance in UGC-VQA under very high efficiency. With perspective opinions in
DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable
clear-cut quality evaluations from a single aesthetic or technical perspective.
Code at https://github.com/VQAssessment/DOVER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action-<span class="highlight-title">GPT</span>: Leveraging Large-scale Language Models for Improved and
  Generalized Action <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Action-GPT, a plug-and-play framework for incorporating Large
Language Models (LLMs) into text-based action generation models. Action phrases
in current motion capture datasets contain minimal and to-the-point
information. By carefully crafting prompts for LLMs, we generate richer and
fine-grained descriptions of the action. We show that utilizing these detailed
descriptions instead of the original action phrases leads to better alignment
of text and motion spaces. We introduce a generic approach compatible with
stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion
models. In addition, the approach enables multiple text descriptions to be
utilized. Our experiments show (i) noticeable qualitative and quantitative
improvement in the quality of synthesized motions, (ii) benefits of utilizing
multiple LLM-generated descriptions, (iii) suitability of the prompt function,
and (iv) zero-shot generation capabilities of the proposed approach. Project
page: https://actiongpt.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, pretrained models and sample videos will be made available at
  \url{https://actiongpt.github.io}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-based Event-centric Online Video Question Answering on a
  Newly Constructed ATBS <span class="highlight-title">Dataset</span> <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Kong, Shuhong Ye, Chenglin Yao, Jianfeng Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks facilitate video question answering (VideoQA), but the
real-world applications on video streams such as CCTV and live cast place
higher demands on the solver. To address the challenges of VideoQA on long
videos of unknown length, we define a new set of problems called Online
Open-ended Video Question Answering (O^2VQA). It requires an online
state-updating mechanism for the solver to decide if the collected information
is sufficient to conclude an answer. We then propose a Confidence-based
Event-centric Online Video Question Answering (CEO-VQA) model to solve this
problem. Furthermore, a dataset called Answer Target in Background Stream
(ATBS) is constructed to evaluate this newly developed online VideoQA
application. Compared to the baseline VideoQA method that watches the whole
video, the experimental results show that the proposed method achieves a
significant performance gain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2023 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-frequency Network for Robust Speaker Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiguo Li, Tianzi Zhang, Xiaobin Liu, Lirong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide deployment of speech-based biometric systems usually demands
high-performance speaker recognition algorithms. However, most of the prior
works for speaker recognition either process the speech in the frequency domain
or time domain, which may produce suboptimal results because both time and
frequency domains are important for speaker recognition. In this paper, we
attempt to analyze the speech signal in both time and frequency domains and
propose the time-frequency network~(TFN) for speaker recognition by extracting
and fusing the features in the two domains. Based on the recent advance of deep
neural networks, we propose a convolution neural network to encode the raw
speech waveform and the frequency spectrum into domain-specific features, which
are then fused and transformed into a classification feature space for speaker
recognition. Experimental results on the publicly available datasets TIMIT and
LibriSpeech show that our framework is effective to combine the information in
the two domains and performs better than the state-of-the-art methods for
speaker recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-06T00:00:00Z">2023-03-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Activity Prediction Models in Drug Discovery with the Ability
  to Understand Human Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Seidl, Andreu Vall, Sepp Hochreiter, Günter Klambauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activity and property prediction models are the central workhorses in drug
discovery and materials sciences, but currently they have to be trained or
fine-tuned for new tasks. Without training or fine-tuning, scientific language
models could be used for such low-data tasks through their announced zero- and
few-shot capabilities. However, their predictive quality at activity prediction
is lacking. In this work, we envision a novel type of activity prediction model
that is able to adapt to new prediction tasks at inference time, via
understanding textual information describing the task. To this end, we propose
a new architecture with separate modules for chemical and natural language
inputs, and a contrastive pre-training objective on data from large biochemical
databases. In extensive experiments, we show that our method CLAMP yields
improved predictive performance on few-shot learning benchmarks and zero-shot
problems in drug discovery. We attribute the advances of our method to the
modularized architecture and to our pre-training objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages + 18 pages appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AmQA: Amharic Question Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tilahun Abedissa, Ricardo Usbeck, Yaregal Assabie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) returns concise answers or answer lists from natural
language text given a context document. Many resources go into curating QA
datasets to advance robust models' development. There is a surge of QA datasets
for languages like English, however, this is not true for Amharic. Amharic, the
official language of Ethiopia, is the second most spoken Semitic language in
the world. There is no published or publicly available Amharic QA dataset.
Hence, to foster the research in Amharic QA, we present the first Amharic QA
(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia
articles. Additionally, we run an XLMR Large-based baseline model to spark
open-domain QA research interest. The best-performing baseline achieves an
F-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension
settings respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The AI Ghostwriter Effect: Users Do Not Perceive Ownership of
  AI-<span class="highlight-title">Generate</span>d Text But Self-Declare as Authors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiona Draxler, Anna Werner, Florian Lehmann, Matthias Hoppe, Albrecht Schmidt, Daniel Buschek, Robin Welsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI interaction in text production increases complexity in authorship.
In two empirical studies (n1 = 30 & n2 = 96), we investigate authorship and
ownership in human-AI collaboration for personalized language generation
models. We show an AI Ghostwriter Effect: Users do not consider themselves the
owners and authors of AI-generated text but refrain from publicly declaring AI
authorship. The degree of personalization did not impact the AI Ghostwriter
Effect, and control over the model increased participants' sense of ownership.
We also found that the discrepancy between the sense of ownership and the
authorship declaration is stronger in interactions with a human ghostwriter and
that people use similar rationalizations for authorship in AI ghostwriters and
human ghostwriters. We discuss how our findings relate to psychological
ownership and human-AI interaction to lay the foundations for adapting
authorship frameworks and user interfaces in AI in text-generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print; currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Faith</span>fulness-Aware Decoding Strategies for Abstractive Summarization <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in understanding and improving faithfulness in
abstractive summarization, the question of how decoding strategies affect
faithfulness is less studied. We present a systematic study of the effect of
generation techniques such as beam search and nucleus sampling on faithfulness
in abstractive summarization. We find a consistent trend where beam search with
large beam sizes produces the most faithful summaries while nucleus sampling
generates the least faithful ones. We propose two faithfulness-aware generation
methods to further improve faithfulness over current generation techniques: (1)
ranking candidates generated by beam search using automatic faithfulness
metrics and (2) incorporating lookahead heuristics that produce a faithfulness
score on the future summary. We show that both generation methods significantly
improve faithfulness across two datasets as evaluated by four automatic
faithfulness metrics and human evaluation. To reduce computational cost, we
demonstrate a simple distillation approach that allows the model to generate
faithful summaries with just greedy decoding. Our code is publicly available at
https://github.com/amazon-science/faithful-summarization-generation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 (17 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Choice Over Control: How Users Write with Large Language Models using
  Diegetic and Non-Diegetic <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a conceptual perspective on prompts for Large Language Models
(LLMs) that distinguishes between (1) diegetic prompts (part of the narrative,
e.g. "Once upon a time, I saw a fox..."), and (2) non-diegetic prompts
(external, e.g. "Write about the adventures of the fox."). With this lens, we
study how 129 crowd workers on Prolific write short texts with different user
interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with
GPT-3): When the interface offered multiple suggestions and provided an option
for non-diegetic prompting, participants preferred choosing from multiple
suggestions over controlling them via non-diegetic prompts. When participants
provided non-diegetic prompts it was to ask for inspiration, topics or facts.
Single suggestions in particular were guided both with diegetic and
non-diegetic information. This work informs human-AI interaction with
generative models by revealing that (1) writing non-diegetic prompts requires
effort, (2) people combine diegetic and non-diegetic prompting, and (3) they
use their draft (i.e. diegetic information) and suggestion timing to
strategically guide LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures, 3 tables, ACM CHI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighborhood Contrastive <span class="highlight-title">Transformer</span> for Change Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbin Tu, Liang Li, Li Su, Ke Lu, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change captioning is to describe the semantic change between a pair of
similar images in natural language. It is more challenging than general image
captioning, because it requires capturing fine-grained change information while
being immune to irrelevant viewpoint changes, and solving syntax ambiguity in
change descriptions. In this paper, we propose a neighborhood contrastive
transformer to improve the model's perceiving ability for various changes under
different scenes and cognition ability for complex syntax structure.
Concretely, we first design a neighboring feature aggregating to integrate
neighboring context into each feature, which helps quickly locate the
inconspicuous changes under the guidance of conspicuous referents. Then, we
devise a common feature distilling to compare two images at neighborhood level
and extract common properties from each image, so as to learn effective
contrastive information between them. Finally, we introduce the explicit
dependencies between words to calibrate the transformer decoder, which helps
better understand complex syntax structure during training. Extensive
experimental results demonstrate that the proposed method achieves the
state-of-the-art performance on three public datasets with different change
scenarios. The code is available at https://github.com/tuyunbin/NCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPA-CLIP: Integrating Phonetic Priors into Vision and Language
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihaya Matsuhira, Marc A. Kastner, Takahiro Komamizu, Takatsugu Hirayama, Keisuke Doman, Yasutomo Kawanishi, Ichiro Ide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale Vision and Language (V\&L) pretraining has become the
standard backbone of many multimedia systems. While it has shown remarkable
performance even in unseen situations, it often performs in ways not intuitive
to humans. Particularly, they usually do not consider the pronunciation of the
input, which humans would utilize to understand language, especially when it
comes to unknown words. Thus, this paper inserts phonetic prior into
Contrastive Language-Image Pretraining (CLIP), one of the V\&L pretrained
models, to make it consider the pronunciation similarity among its
pronunciation inputs. To achieve this, we first propose a phoneme embedding
that utilizes the phoneme relationships provided by the International Phonetic
Alphabet (IPA) chart as a phonetic prior. Next, by distilling the frozen CLIP
text encoder, we train a pronunciation encoder employing the IPA-based
embedding. The proposed model named IPA-CLIP comprises this pronunciation
encoder and the original CLIP encoders (image and text). Quantitative
evaluation reveals that the phoneme distribution on the embedding space
represents phonetic relationships more accurately when using the proposed
phoneme embedding. Furthermore, in some multimodal retrieval tasks, we confirm
that the proposed pronunciation encoder enhances the performance of the text
encoder and that the pronunciation encoder handles nonsense words in a more
phonetic manner than the text encoder. Finally, qualitative evaluation verifies
the correlation between the pronunciation encoder and human perception
regarding pronunciation similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IFAN: An Explainability-Focused Interaction Framework for Humans and NLP
  Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Mosca, Daryna Dementieva, Tohid Ebrahim Ajdari, Maximilian Kummeth, Kirill Gringauz, Georg Groh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability and human oversight are fundamental pillars of deploying
complex NLP models into real-world applications. However, applying
explainability and human-in-the-loop methods requires technical proficiency.
Despite existing toolkits for model understanding and analysis, options to
integrate human feedback are still limited. We propose IFAN, a framework for
real-time explanation-based interaction with NLP models. Through IFAN's
interface, users can provide feedback to selected model explanations, which is
then integrated through adapter layers to align the model with human rationale.
We show the system to be effective in debiasing a hate speech classifier with
minimal performance loss. IFAN also offers a visual admin system and API to
manage models (and datasets) as well as control access rights. A demo is live
at https://ifan.ml/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Demo 2023 Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Zero-Shot Functional Compositionality of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyeol Yu, Myeongho Jeong, Jamin Shin, Hyeongdon Moon, Juneyoung Park, Seungtaek Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Pre-trained Language Models (PLM) have become the most desirable
starting point in the field of NLP, as they have become remarkably good at
solving many individual tasks. Despite such success, in this paper, we argue
that current paradigms of working with PLMs are neglecting a critical aspect of
modeling human intelligence: functional compositionality. Functional
compositionality - the ability to compose learned tasks - has been a
long-standing challenge in the field of AI (and many other fields) as it is
considered one of the hallmarks of human intelligence. An illustrative example
of such is cross-lingual summarization, where a bilingual person
(English-French) could directly summarize an English document into French
sentences without having to translate the English document or summary into
French explicitly. We discuss why this matter is an important open problem that
requires further attention from the field. Then, we show that current PLMs
(e.g., GPT-2 and T5) don't have functional compositionality yet and it is far
from human-level generalizability. Finally, we suggest several research
directions that could push the field towards zero-shot functional
compositionality of language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourcing on Sensitive Data with Privacy-Preserving Text Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nina Mouhammad, Johannes Daxenberger, Benjamin Schiller, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most tasks in NLP require labeled data. Data labeling is often done on
crowdsourcing platforms due to scalability reasons. However, publishing data on
public platforms can only be done if no privacy-relevant information is
included. Textual data often contains sensitive information like person names
or locations. In this work, we investigate how removing personally identifiable
information (PII) as well as applying differential privacy (DP) rewriting can
enable text with privacy-relevant information to be used for crowdsourcing. We
find that DP-rewriting before crowdsourcing can preserve privacy while still
leading to good label quality for certain tasks and data. PII-removal led to
good label quality in all examined tasks, however, there are no privacy
guarantees given.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only
  Training <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Linchao Zhu, Longyin Wen, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong
zero-shot transfer capability in many discriminative tasks. Their adaptation to
zero-shot image-conditioned text generation tasks has drawn increasing
interest. Prior arts approach to zero-shot captioning by either utilizing the
existing large language models (e.g., GPT-2) or pre-training the
encoder-decoder network in an end-to-end manner. In this work, we propose a
simple framework, named DeCap, for zero-shot captioning. We introduce a
lightweight visual-aware language decoder. This decoder is both data-efficient
and computation-efficient: 1) it only requires the text data for training,
easing the burden on the collection of paired data. 2) it does not require
end-to-end training. When trained with text-only data, the decoder takes the
text embedding extracted from the off-the-shelf CLIP encoder as a prefix
embedding. The challenge is that the decoder is trained on the text corpus but
at the inference stage, it needs to generate captions based on visual inputs.
The modality gap issue is widely observed in multi-modal contrastive models
that prevents us from directly taking the visual embedding as the prefix
embedding. We propose a training-free mechanism to reduce the modality gap. We
project the visual embedding into the CLIP text embedding space, while the
projected embedding retains the information of the visual input. Taking the
projected embedding as the prefix embedding, the decoder generates high-quality
descriptions that match the visual input. The experiments show that DeCap
outperforms other zero-shot captioning methods and unpaired captioning methods
on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023. Code is available at
  https://github.com/dhg-wei/DeCap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NxPlain: Web-based Tool for Discovery of Latent Concepts <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Tamim Jaban, Musab Husaini, Ummar Abbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of deep neural networks in various domains has seen an
increased need for the interpretability of these models, especially in
scenarios where fairness and trust are as important as model performance. A lot
of independent work is being carried out to: i) analyze what linguistic and
non-linguistic knowledge is learned within these models, and ii) highlight the
salient parts of the input. We present NxPlain, a web application that provides
an explanation of a model's prediction using latent concepts. NxPlain discovers
latent concepts learned in a deep NLP model, provides an interpretation of the
knowledge learned in the model, and explains its predictions based on the used
concepts. The application allows users to browse through the latent concepts in
an intuitive order, letting them efficiently scan through the most salient
concepts with a global corpus level view and a local sentence-level view. Our
tool is useful for debugging, unraveling model bias, and for highlighting
spurious correlations in a model. A hosted demo is available here:
https://nxplain.qcri.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code
  Understanding, <span class="highlight-title">Generation</span>, Translation and Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to solve problems is a hallmark of intelligence and has been an
enduring goal in AI. AI systems that can create programs as solutions to
problems or assist developers in writing programs can increase productivity and
make programming more accessible. Recently, pre-trained large language models
have shown impressive abilities in generating new codes from natural language
descriptions, repairing buggy codes, translating codes between languages, and
retrieving relevant code segments. However, the evaluation of these models has
often been performed in a scattered way on only one or two specific tasks, in a
few languages, at a partial granularity (e.g., function) level and in many
cases without proper training data. Even more concerning is that in most cases
the evaluation of generated codes has been done in terms of mere lexical
overlap rather than actual execution whereas semantic similarity (or
equivalence) of two code segments depends only on their ``execution
similarity'', i.e., being able to get the same output for a given input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCLIP: Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing with Hierarchy-aware
  Attention <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large-scale contrastive vision-language pretraining (CLIP) has
benefited both visual recognition and multimodal content understanding. The
concise design brings CLIP the advantage in inference efficiency against other
vision-language models with heavier cross-attention fusion layers, making it a
popular choice for a wide spectrum of downstream tasks. However, CLIP does not
explicitly capture the hierarchical nature of high-level and fine-grained
semantics conveyed in images and texts, which is arguably critical to
vision-language understanding and reasoning. To this end, we equip both the
visual and language branches in CLIP with hierarchy-aware attentions, namely
Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies
layer-by-layer from both images and texts in an unsupervised manner. As a
result, such hierarchical aggregation significantly improves the cross-modal
alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative
analysis on its unsupervised hierarchy induction during inference, as well as
extensive quantitative experiments on both visual recognition and
vision-language downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GlobalNER: Incorporating Non-local Information into Named Entity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiao-Wei Hsu, Keh-Yih Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, many Natural Language Processing (NLP) tasks see the demand for
incorporating knowledge external to the local information to further improve
the performance. However, there is little related work on Named Entity
Recognition (NER), which is one of the foundations of NLP. Specifically, no
studies were conducted on the query generation and re-ranking for retrieving
the related information for the purpose of improving NER. This work
demonstrates the effectiveness of a DNN-based query generation method and a
mention-aware re-ranking architecture based on BERTScore particularly for NER.
In the end, a state-of-the-art performance of 61.56 micro-f1 score on WNUT17
dataset is achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenICL: An Open-Source Framework for In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, In-context Learning (ICL) has gained increasing attention
and emerged as the new paradigm for large language model (LLM) evaluation.
Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained
models to unseen tasks without any parameter updates. However, the
implementation of ICL is sophisticated due to the diverse retrieval and
inference methods involved, as well as the varying pre-processing requirements
for different models, datasets, and tasks. A unified and flexible framework for
ICL is urgently needed to ease the implementation of the aforementioned
components. To facilitate ICL research, we introduce OpenICL, an open-source
toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly
flexible architecture that users can easily combine different components to
suit their needs. It also provides various state-of-the-art retrieval and
inference methods to streamline the process of adapting ICL to cutting-edge
research. The effectiveness of OpenICL has been validated on a wide range of
NLP tasks, including classification, QA, machine translation, and semantic
parsing. As a side-product, we found OpenICL to be an efficient yet robust tool
for LLMs evaluation. OpenICL is released at
https://github.com/Shark-NLP/OpenICL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic <span class="highlight-title">Prompt</span>ing: A Unified Framework for <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjun Yang, Wei Cheng, Xujiang Zhao, Linda Petzold, Haifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been demonstrated that prompt tuning is highly effective in
efficiently eliciting knowledge from language models (LMs). However, the prompt
tuning still lags behind fine-tuning, especially when the LMs are small.
P-tuning v2 (Liu et al., 2021b) makes it comparable with finetuning by adding
continuous prompts for every layer of the pre-trained model. However,
prepending fixed soft prompts for all instances, regardless of their
discrepancy, is doubtful. In particular, the inserted prompt position, length,
and the representations of prompts for diversified instances through different
tasks could all affect the prompt tuning performance. To fill this gap, we
propose dynamic prompting (DP): the position, length, and prompt representation
can all be dynamically optimized with respect to different tasks and instances.
We conduct comprehensive experiments on the SuperGlue benchmark to validate our
hypothesis and demonstrate substantial improvements. We also derive a unified
framework for supporting our dynamic prompting strategy. In particular, we use
a simple learning network and Gumble- Softmax for learning instance-dependent
guidance. Experimental results show that simple instance-level position-aware
soft prompts can improve the classification accuracy of up to 6 points on
average on five datasets, reducing its gap with fine-tuning. Besides, we also
prove its universal usefulness under full-data, few-shot, and multitask
regimes. Combining them together can even further unleash the power of DP,
narrowing the distance between finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitask <span class="highlight-title">Prompt</span> Tuning Enables Parameter-Efficient Transfer Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning, in which a base pretrained model is adapted to each task via
conditioning on learned prompt vectors, has emerged as a promising approach for
efficiently adapting large language models to multiple downstream tasks.
However, existing methods typically learn soft prompt vectors from scratch, and
it has not been clear how to exploit the rich cross-task knowledge with prompt
vectors in a multitask learning setting. We propose multitask prompt tuning
(MPT), which first learns a single transferable prompt by distilling knowledge
from multiple task-specific source prompts. We then learn multiplicative low
rank updates to this shared prompt to efficiently adapt it to each downstream
target task. Extensive experiments on 23 NLP datasets demonstrate that our
proposed approach outperforms the state-of-the-art methods, including the full
finetuning baseline in some cases, despite only tuning 0.035% as many
task-specific parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Project page: https://zhenwang9102.github.io/mpt.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Grained Self-Interpretable Symbolic-Neural Model For
  Single/Multi-Labeled Text Classification <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Xinyu Kong, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks based on layer-stacking architectures have historically
suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic
models function with clear interpretability, but how to combine them with
neural networks to enhance their performance remains to be explored. In this
paper, we try to marry these two systems for text classification via a
structured language model. We propose a Symbolic-Neural model that can learn to
explicitly predict class labels of text spans from a constituency tree without
requiring any access to span-level gold labels. As the structured language
model learns to predict constituency trees in a self-supervised manner, only
raw texts and sentence-level labels are required as training data, which makes
it essentially a general constituent-level self-interpretable classification
model. Our experiments demonstrate that our approach could achieve good
prediction accuracy in downstream tasks. Meanwhile, the predicted span labels
are consistent with human rationales to a certain degree.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with
  Variational Information Bottleneck and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingshan Chang, Min Yang, Qingshan Jiang, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature on aspect-based sentiment analysis (ABSA) has been overwhelmed
by deep neural networks, yielding state-of-the-art results for ABSA. However,
these deep models are susceptible to learning spurious correlations between
input features and output labels, which in general suffer from poor robustness
and generalization. In this paper, we propose a novel Contrastive Variational
Information Bottleneck framework (called CVIB) to reduce spurious correlations
for ABSA. The proposed CVIB framework is composed of an original network and a
self-pruned network, and these two networks are optimized simultaneously via
contrastive learning. Concretely, we employ the Variational Information
Bottleneck (VIB) principle to learn an informative and compressed network
(self-pruned network) from the original network, which discards the superfluous
patterns or spurious correlations between input features and prediction labels.
Then, self-pruning contrastive learning is devised to pull together
semantically similar positive pairs and push away dissimilar pairs, where the
representations of the anchor learned by the original and self-pruned networks
respectively are regarded as a positive pair while the representations of two
different sentences within a mini-batch are treated as a negative pair.
Extensive experiments on five benchmark ABSA datasets demonstrate that our CVIB
method achieves better performance than the strong competitors in terms of
overall prediction performance, robustness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in
  Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bixing Yan, Shaoling Chen, Yuxuan He, Zhihan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language understanding(NLU) is challenging for finance due to the
lack of annotated data and the specialized language in that domain. As a
result, researchers have proposed to use pre-trained language model and
multi-task learning to learn robust representations. However, aggressive
fine-tuning often causes over-fitting and multi-task learning may favor tasks
with significantly larger amounts data, etc. To address these problems, in this
paper, we investigate model-agnostic meta-learning algorithm(MAML) in
low-resource financial NLU tasks. Our contribution includes: 1. we explore the
performance of MAML method with multiple types of tasks: GLUE datasets, SNLI,
Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML method
with multiple single-type tasks: a real scenario stock price prediction problem
with twitter text data. Our models achieve the state-of-the-art performance
according to the experimental results, which demonstrate that our method can
adapt fast and well to low-resource situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Zero-Shot Human Models for Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-resolution Interpretation and Diagnostics Tool for Natural
  Language Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Jalali, Nengfeng Zhou, Yufei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing explainability methods for Natural Language Processing (NLP)
models is a challenging task, for two main reasons. First, the high
dimensionality of the data (large number of tokens) results in low coverage and
in turn small contributions for the top tokens, compared to the overall model
performance. Second, owing to their textual nature, the input variables, after
appropriate transformations, are effectively binary (presence or absence of a
token in an observation), making the input-output relationship difficult to
understand. Common NLP interpretation techniques do not have flexibility in
resolution, because they usually operate at word-level and provide fully local
(message level) or fully global (over all messages) summaries. The goal of this
paper is to create more flexible model explainability summaries by segments of
observation or clusters of words that are semantically related to each other.
In addition, we introduce a root cause analysis method for NLP models, by
analyzing representative False Positive and False Negative examples from
different segments. At the end, we illustrate, using a Yelp review data set
with three segments (Restaurant, Hotel, and Beauty), that exploiting
group/cluster structures in words and/or messages can aid in the interpretation
of decisions made by NLP models and can be utilized to assess the model's
sensitivity or bias towards gender, syntax, and word meanings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 0 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guilt Detection in Text: A Step Towards Understanding Complex Emotions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Gafar Manuel Meque, Nisar Hussain, Grigori Sidorov, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel Natural Language Processing (NLP) task called Guilt
detection, which focuses on detecting guilt in text. We identify guilt as a
complex and vital emotion that has not been previously studied in NLP, and we
aim to provide a more fine-grained analysis of it. To address the lack of
publicly available corpora for guilt detection, we created VIC, a dataset
containing 4622 texts from three existing emotion detection datasets that we
binarized into guilt and no-guilt classes. We experimented with traditional
machine learning methods using bag-of-words and term frequency-inverse document
frequency features, achieving a 72% f1 score with the highest-performing model.
Our study provides a first step towards understanding guilt in text and opens
the door for future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-stage Pipeline for Multilingual Dialect Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Vaidya, Aditya Kane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialect Identification is a crucial task for localizing various Large
Language Models. This paper outlines our approach to the VarDial 2023 shared
task. Here we have to identify three or two dialects from three languages each
which results in a 9-way classification for Track-1 and 6-way classification
for Track-2 respectively. Our proposed approach consists of a two-stage system
and outperforms other participants' systems and previous works in this domain.
We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase
is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Based Zero-Shot
  Object Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Sashank Dorbala, James F. Mullen Jr., Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LGX, a novel algorithm for Object Goal Navigation in a
"language-driven, zero-shot manner", where an embodied agent navigates to an
arbitrarily described target object in a previously unexplored environment. Our
approach leverages the capabilities of Large Language Models (LLMs) for making
navigational decisions by mapping the LLMs implicit knowledge about the
semantic context of the environment into sequential inputs for robot motion
planning. Simultaneously, we also conduct generalized target object detection
using a pre-trained Vision-Language grounding model. We achieve
state-of-the-art zero-shot object navigation results on RoboTHOR with a success
rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP
on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot
navigation and present an analysis of the various semantic factors affecting
model output. Finally, we showcase the benefits of our approach via real-world
experiments that indicate the superior performance of LGX when navigating to
and detecting visually unique objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spelling convention sensitivity in neural language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Nielsen, Christo Kirov, Brian Roark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine whether large neural language models, trained on very large
collections of varied English text, learn the potentially long-distance
dependency of British versus American spelling conventions, i.e., whether
spelling is consistently one or the other within model-generated strings. In
contrast to long-distance dependencies in non-surface underlying structure
(e.g., syntax), spelling consistency is easier to measure both in LMs and the
text corpora used to train them, which can provide additional insight into
certain observed model behaviors. Using a set of probe words unique to either
British or American English, we first establish that training corpora exhibit
substantial (though not total) consistency. A large T5 language model does
appear to internalize this consistency, though only with respect to observed
lexical items (not nonce words with British/American spelling patterns). We
further experiment with correcting for biases in the training data by
fine-tuning T5 on synthetic data that has been debiased, and find that
finetuned T5 remains only somewhat sensitive to spelling consistency. Further
experiments show GPT2 to be similarly limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Portraits: Recording Foundation Model Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Marone, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are trained on increasingly immense and opaque datasets.
Even while these models are now key in AI system building, it can be difficult
to answer the straightforward question: has the model already encountered a
given example during training? We therefore propose a widespread adoption of
Data Portraits: artifacts that record training data and allow for downstream
inspection. First we outline the properties of such an artifact and discuss how
existing solutions can be used to increase transparency. We then propose and
implement a solution based on data sketching, stressing fast and space
efficient querying. Using our tool, we document a popular large language
modeling corpus (the Pile) and show that our solution enables answering
questions about test set leakage and model plagiarism. Our tool is lightweight
and fast, costing only 3% of the dataset size in overhead. We release a demo of
our tools at dataportraits.org and call on dataset and model creators to
release Data Portraits as a complement to current documentation practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depression Detection Using Digital Traces on Social Media: A
  <span class="highlight-title">Knowledge</span>-aware Deep Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenli Zhang, Jiaheng Xie, Xiang Liu, Zhu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression is a common disease worldwide. It is difficult to diagnose and
continues to be underdiagnosed. Because depressed patients constantly share
their symptoms, major life events, and treatments on social media, researchers
are turning to user-generated digital traces on social media for depression
detection. Such methods have distinct advantages in combating depression
because they can facilitate innovative approaches to fight depression and
alleviate its social and economic burden. However, most existing studies lack
effective means to incorporate established medical domain knowledge in
depression detection or suffer from feature extraction difficulties that impede
greater performance. Following the design science research paradigm, we propose
a Deep Knowledge-aware Depression Detection (DKDD) framework to accurately
detect social media users at risk of depression and explain the critical
factors that contribute to such detection. Extensive empirical studies with
real-world data demonstrate that, by incorporating domain knowledge, our method
outperforms existing state-of-the-art methods. Our work has significant
implications for IS research in knowledge-aware machine learning, digital
traces utilization, and NLP research in IS. Practically, by providing early
detection and explaining the critical factors, DKDD can supplement clinical
depression screening and enable large-scale evaluations of a population's
mental health status.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at INFORMS 2022 Data Science Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">ChatGPT</span> is on the horizon: Could a large language model be all we need
  for Intelligent Transportation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, Shengxuan Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, developed by OpenAI, is one of the largest Large Language Models
(LLM) with over 175 billion parameters. ChatGPT has demonstrated the impressive
capabilities of LLM, particularly in the field of natural language processing
(NLP). With the emergence of the discussion and application of LLM in various
research or engineering domains, it is time to envision how LLM may
revolutionize the way we approach intelligent transportation systems. This
paper explores the future applications of LLM in addressing key transportation
problems. By leveraging LLM and a cross-modal encoder, an intelligent system
can handle traffic data from various modalities and execute transportation
operations through a single LLM. NLP, combined with cross-modal processing, is
investigated with its potential applications in transportation. To demonstrate
this potential, a smartphone-based crash report auto-generation and analysis
framework is presented as a use case. Despite the potential benefits,
challenges related to data privacy, data quality, and model bias must be
considered. Overall, the use of LLM in intelligent transport systems holds
promise for more efficient, intelligent, and sustainable transportation systems
that improve the lives of people around the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ wav2vec and its current potential to Automatic Speech Recognition in
  German for the usage in Digital History: A comparative assessment of
  available ASR-technologies for the use in cultural heritage contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Fleck, Wolfgang Göderle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this case study we trained and published a state-of-the-art open-source
model for Automatic Speech Recognition (ASR) for German to evaluate the current
potential of this technology for the use in the larger context of Digital
Humanities and cultural heritage indexation. Along with this paper we publish
our wav2vec2 based speech to text model while we evaluate its performance on a
corpus of historical recordings we assembled compared against commercial
cloud-based and proprietary services. While our model achieves moderate
results, we see that proprietary cloud services fare significantly better. As
our results show, recognition rates over 90 percent can currently be achieved,
however, these numbers drop quickly once the recordings feature limited audio
quality or use of non-every day or outworn language. A big issue is the high
variety of different dialects and accents in the German language. Nevertheless,
this paper highlights that the currently available quality of recognition is
high enough to address various use cases in the Digital Humanities. We argue
that ASR will become a key technology for the documentation and analysis of
audio-visual sources and identify an array of important questions that the DH
community and cultural heritage stakeholders will have to address in the near
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using
  Distant Supervision <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Feng, Piji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are
important to study ancient Chinese, but the amount of ancient Chinese WSG and
POS tagging data is still rare. In this paper, we propose a novel augmentation
method of ancient Chinese WSG and POS tagging data using distant supervision
over parallel corpus. However, there are still mislabeled and unlabeled ancient
Chinese words inevitably in distant supervision. To address this problem, we
take advantage of the memorization effects of deep neural networks and a small
amount of annotated data to get a model with much knowledge and a little noise,
and then we use this model to relabel the ancient Chinese sentences in parallel
corpus. Experiments show that the model trained over the relabeled data
outperforms the model trained over the data generated from distant supervision
and the annotated data. Our code is available at
https://github.com/farlit/ACDS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-Free Attentive Scoring for Speaker Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05642v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05642v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Pelecanos, Quan Wang, Yiling Huang, Ignacio Lopez Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel study of parameter-free attentive scoring for
speaker verification. Parameter-free scoring provides the flexibility of
comparing speaker representations without the need of an accompanying
parametric scoring model. Inspired by the attention component in Transformer
neural networks, we propose a variant of the scaled dot product attention
mechanism to compare enrollment and test segment representations. In addition,
this work explores the effect on performance of (i) different types of
normalization, (ii) independent versus tied query/key estimation, (iii) varying
the number of key-value pairs and (iv) pooling multiple enrollment utterance
statistics. Experimental results for a 4 task average show that a simple
parameter-free attentive scoring mechanism can improve the average EER by 10%
over the best cosine similarity baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TalkToModel: Explaining Machine Learning Models with Interactive Natural
  Language <span class="highlight-title">Conversation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04154v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04154v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, Sameer Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have become more complex, making them
harder to understand. To this end, researchers have proposed several techniques
to explain model predictions. However, practitioners struggle to use these
explainability techniques because they often do not know which one to choose
and how to interpret the results of the explanations. In this work, we address
these challenges by introducing TalkToModel: an interactive dialogue system for
explaining machine learning models through conversations. Specifically,
TalkToModel comprises of three key components: 1) a natural language interface
for engaging in conversations, making ML model explainability highly
accessible, 2) a dialogue engine that adapts to any tabular model and dataset,
interprets natural language, maps it to appropriate explanations, and generates
text responses, and 3) an execution component that constructs the explanations.
We carried out extensive quantitative and human subject evaluations of
TalkToModel. Overall, we found the conversational system understands user
inputs on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In real-world evaluations
with humans, 73% of healthcare workers (e.g., doctors and nurses) agreed they
would use TalkToModel over baseline point-and-click systems for explainability
in a disease prediction task, and 85% of ML professionals agreed TalkToModel
was easier to use for computing explanations. Our findings demonstrate that
TalkToModel is more effective for model explainability than existing systems,
introducing a new category of explainability tools for practitioners. Code &
demo released here: https://github.com/dylan-slack/TalkToModel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print; comments welcome! Reach out to dslack@uci.edu v3 update
  title and abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Exemplars for In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained language models (LMs) have shown impressive In-Context
Learning (ICL) ability, where the model learns to do an unseen task via a
prompt consisting of input-output examples as the demonstration, without any
parameter updates. The performance of ICL is highly dominated by the quality of
the selected in-context examples. However, previous selection methods are
mostly based on simple heuristics, leading to sub-optimal performance. In this
work, we formulate in-context example selection as a subset selection problem.
We propose CEIL (Compositional Exemplars for In-context Learning), which is
instantiated by Determinantal Point Processes (DPPs) to model the interaction
between the given input and in-context examples, and optimized through a
carefully-designed contrastive learning objective to obtain preference from
LMs. We validate CEIL on 12 classification and generation datasets from 7
distinct NLP tasks, including sentiment analysis, paraphrase detection, natural
language inference, commonsense reasoning, open-domain question answering, code
generation, and semantic parsing. Extensive experiments demonstrate not only
the state-of-the-art performance but also the transferability and
compositionality of CEIL, shedding new light on effective and efficient
in-context learning. Our code is released at
https://github.com/HKUNLP/icl-ceil.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Categorical Archive of <span class="highlight-title">ChatGPT</span> Failures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03494v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03494v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Borji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Memorization Across Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07646v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07646v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
  We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes more complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Hyena Hierarchy: Towards Larger Convolutional Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, <span class="highlight-author">Yoshua Bengio</span>, Stefano Ermon, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have relied heavily on the use of large
Transformers due to their ability to learn at scale. However, the core building
block of Transformers, the attention operator, exhibits quadratic cost in
sequence length, limiting the amount of context accessible. Existing
subquadratic methods based on low-rank and sparse approximations need to be
combined with dense attention layers to match Transformers, indicating a gap in
capability. In this work, we propose Hyena, a subquadratic drop-in replacement
for attention constructed by interleaving implicitly parametrized long
convolutions and data-controlled gating. In recall and reasoning tasks on
sequences of thousands to hundreds of thousands of tokens, Hyena improves
accuracy by more than 50 points over operators relying on state-spaces and
other implicit and explicit methods, matching attention-based models. We set a
new state-of-the-art for dense-attention-free architectures on language
modeling in standard datasets (WikiText103 and The Pile), reaching Transformer
quality with a 20% reduction in training compute required at sequence length
2K. Hyena operators are twice as fast as highly optimized attention at sequence
length 8K, and 100x faster at sequence length 64K.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Additional results (PG-19, LAMBADA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepStruct: <span class="highlight-title">Pretrain</span>ing of Language Models for Structure Prediction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for improving the structural understanding abilities of
language models. Unlike previous approaches that finetune the models with
task-specific augmentation, we pretrain language models on a collection of
task-agnostic corpora to generate structures from text. Our structure
pretraining enables zero-shot transfer of the learned knowledge that models
have about the structure tasks. We study the performance of this approach on 28
datasets, spanning 10 structure prediction tasks including open information
extraction, joint entity and relation extraction, named entity recognition,
relation classification, semantic role labeling, event extraction, coreference
resolution, factual probe, intent detection, and dialogue state tracking. We
further enhance the pretraining with the task-specific training sets. We show
that a 10B parameter language model transfers non-trivially to most tasks and
obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverse scaling can become U-shaped 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02011v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02011v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Wei, Najoung Kim, Yi Tay, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up language models has been empirically shown to improve performance
and unlock emergent abilities. Conversely, observing worse performance as a
function of scale ("inverse scaling") would indicate that scaling encourages
behaviors that are misaligned with human preferences. The Inverse Scaling Prize
(McKenzie et al. 2022) identified eleven such inverse scaling tasks, evaluated
on models of up to 280B parameters and up to 500 zettaFLOPs of training
compute. This paper takes a closer look at these inverse scaling tasks. We
evaluate models of up to 540B parameters, trained on five times more compute
than those evaluated in the Inverse Scaling Prize. With this increased range of
model sizes and training compute, only four out of the eleven tasks remain
inverse scaling. Six out of the eleven tasks exhibit what we call "U-shaped
scaling" -- performance decreases up to a certain model size, and then
increases again up to the largest model evaluated (the one remaining task
displays positive scaling). U-shaped scaling suggests that the inverse scaling
trend observed in McKenzie et al. (2022) may not continue to hold for larger
models, and adds further support to the claim that sufficiently large models
unlock emergent abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v4 reports correct results on Round 2 tasks and includes results for
  additional one-shot evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a survey of the state of the art of hybrid language
models architectures and strategies for "complex" question-answering (QA, CQA,
CPS). Very large language models are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, tasks, methods,
sensitive data, performance, human approval and versatile feedback... This
survey extends findings from the robust community edited research papers BIG,
BLOOM and HELM which open source, benchmark and analyze limits and challenges
of large language models in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). It identifies the key
elements used with Large Language Models (LLM) to solve complex questions or
problems. Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of language models in complex QA. Hybridizing these models with
different components could allow to overcome these different limits and go much
further. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form QA,
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, QA explainability and truthfulness, time dimension.
Therefore we review current solutions and promising strategies, using elements
such as hybrid LLM architectures, human-in-the-loop reinforcement learning,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, and others. We analyze existing solutions and provide an
overview of the current research and trends in the area of complex QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiViz: Towards Visualizing and Understanding Multimodal Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The promise of multimodal models for real-world applications has inspired
research in visualizing and understanding their internal mechanics with the end
goal of empowering stakeholders to visualize model behavior, perform model
debugging, and promote trust in machine learning models. However, modern
multimodal models are typically black-box neural networks, which makes it
challenging to understand their internal mechanics. How can we visualize the
internal modeling of multimodal interactions in these models? Our paper aims to
fill this gap by proposing MultiViz, a method for analyzing the behavior of
multimodal models by scaffolding the problem of interpretability into 4 stages:
(1) unimodal importance: how each modality contributes towards downstream
modeling and prediction, (2) cross-modal interactions: how different modalities
relate with each other, (3) multimodal representations: how unimodal and
cross-modal interactions are represented in decision-level features, and (4)
multimodal prediction: how decision-level features are composed to make a
prediction. MultiViz is designed to operate on diverse modalities, models,
tasks, and research areas. Through experiments on 8 trained models across 6
real-world tasks, we show that the complementary stages in MultiViz together
enable users to (1) simulate model predictions, (2) assign interpretable
concepts to features, (3) perform error analysis on model misclassifications,
and (4) use insights from error analysis to debug models. MultiViz is publicly
available, will be regularly updated with new interpretation tools and metrics,
and welcomes inputs from the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Code available at: https://github.com/pliang279/MultiViz</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic
  Analysis For DDIM-Type Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitan Chen, Giannis Daras, Alexandros G. Dimakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a framework for non-asymptotic analysis of deterministic samplers
used for diffusion generative modeling. Several recent works have analyzed
stochastic samplers using tools like Girsanov's theorem and a chain rule
variant of the interpolation argument. Unfortunately, these techniques give
vacuous bounds when applied to deterministic samplers. We give a new
operational interpretation for deterministic sampling by showing that one step
along the probability flow ODE can be expressed as two steps: 1) a restoration
step that runs gradient ascent on the conditional log-likelihood at some
infinitesimally previous time, and 2) a degradation step that runs the forward
process using noise pointing back towards the current iterate. This perspective
allows us to extend denoising diffusion implicit models to general, non-linear
forward processes. We then develop the first polynomial convergence bounds for
these samplers under mild conditions on the data distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Globally Optimal Training of Neural Networks with Threshold Activation
  Functions <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tolga Ergen, Halil Ibrahim Gulluk, Jonathan Lacotte, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Threshold activation functions are highly preferable in neural networks due
to their efficiency in hardware implementations. Moreover, their mode of
operation is more interpretable and resembles that of biological neurons.
However, traditional gradient based algorithms such as Gradient Descent cannot
be used to train the parameters of neural networks with threshold activations
since the activation function has zero gradient except at a single
non-differentiable point. To this end, we study weight decay regularized
training problems of deep neural networks with threshold activations. We first
show that regularized deep threshold network training problems can be
equivalently formulated as a standard convex optimization problem, which
parallels the LASSO method, provided that the last hidden layer width exceeds a
certain threshold. We also derive a simplified convex optimization formulation
when the dataset can be shattered at a certain layer of the network. We
corroborate our theoretical results with various numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Humanoid Locomotion with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a sim-to-real learning-based approach for real-world humanoid
locomotion. Our controller is a causal Transformer trained by autoregressive
prediction of future actions from the history of observations and actions. We
hypothesize that the observation-action history contains useful information
about the world that a powerful Transformer model can use to adapt its behavior
in-context, without updating its weights. We do not use state estimation,
dynamics models, trajectory optimization, reference trajectories, or
pre-computed gait libraries. Our controller is trained with large-scale
model-free reinforcement learning on an ensemble of randomized environments in
simulation and deployed to the real world in a zero-shot fashion. We evaluate
our approach in high-fidelity simulation and successfully deploy it to the real
robot as well. To the best of our knowledge, this is the first demonstration of
a fully learning-based method for real-world full-sized humanoid locomotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://humanoid-transformer.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SUREL+: Moving from Walks to Sets for Scalable Subgraph-based Graph
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoteng Yin, Muhan Zhang, Jianguo Wang, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subgraph-based graph representation learning (SGRL) has recently emerged as a
powerful tool in many prediction tasks on graphs due to its advantages in model
expressiveness and generalization ability. Most previous SGRL models face
computational issues associated with the high cost of extracting subgraphs for
each training or testing query. Recently, SUREL has been proposed as a new
framework to accelerate SGRL, which samples random walks offline and joins
these walks as subgraphs online for prediction. Due to the reusability of
sampled walks across different queries, SUREL achieves state-of-the-art
performance in both scalability and prediction accuracy. However, SUREL still
suffers from high computational overhead caused by node redundancy in sampled
walks. In this work, we propose a novel framework SUREL+ that upgrades SUREL by
using node sets instead of walks to represent subgraphs. This set-based
representation avoids node duplication by definition, but the sizes of node
sets can be irregular. To address this issue, we design a dedicated sparse data
structure to efficiently store and fast index node sets, and provide a
specialized operator to join them in parallel batches. SUREL+ is modularized to
support multiple types of set samplers, structural features, and neural
encoders to complement the loss of structural information due to the reduction
from walks to sets. Extensive experiments have been performed to validate
SUREL+ in the prediction tasks of links, relation types, and higher-order
patterns. SUREL+ achieves 3-11$\times$ speedups of SUREL while maintaining
comparable or even better prediction performance; compared to other SGRL
baselines, SUREL+ achieves $\sim$20$\times$ speedups and significantly improves
the prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaLM-E: An Embodied Multimodal Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement
  Learning <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikayel Samvelyan, Akbir Khan, Michael Dennis, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Roberta Raileanu, Tim Rocktäschel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-ended learning methods that automatically generate a curriculum of
increasingly challenging tasks serve as a promising avenue toward generally
capable reinforcement learning agents. Existing methods adapt curricula
independently over either environment parameters (in single-agent settings) or
co-player policies (in multi-agent settings). However, the strengths and
weaknesses of co-players can manifest themselves differently depending on
environmental features. It is thus crucial to consider the dependency between
the environment and co-player when shaping a curriculum in multi-agent domains.
In this work, we use this insight and extend Unsupervised Environment Design
(UED) to multi-agent environments. We then introduce Multi-Agent Environment
Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED
approach for two-player zero-sum settings. MAESTRO efficiently produces
adversarial, joint curricula over both environments and co-players and attains
minimax-regret guarantees at Nash equilibrium. Our experiments show that
MAESTRO outperforms a number of strong baselines on competitive two-player
games, spanning discrete and continuous control settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Stay or Not to Stay in the <span class="highlight-title">Pre-train</span> Basin: Insights on Ensembling in
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ildus Sadrtdinov, Dmitrii Pozdeev, Dmitry Vetrov, Ekaterina Lobacheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning and ensembling are two popular techniques for improving the
performance and robustness of neural networks. Due to the high cost of
pre-training, ensembles of models fine-tuned from a single pre-trained
checkpoint are often used in practice. Such models end up in the same basin of
the loss landscape and thus have limited diversity. In this work, we study if
it is possible to improve ensembles trained from a single pre-trained
checkpoint by better exploring the pre-train basin or a close vicinity outside
of it. We show that while exploration of the pre-train basin may be beneficial
for the ensemble, leaving the basin results in losing the benefits of transfer
learning and degradation of the ensemble quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALMOST: Adversarial Learning to Mitigate Oracle-less ML Attacks via
  Synthesis Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Animesh Basak Chowdhury, Lilas Alrahis, Luca Collini, Johann Knechtel, Ramesh Karri, Siddharth Garg, Ozgur Sinanoglu, Benjamin Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oracle-less machine learning (ML) attacks have broken various logic locking
schemes. Regular synthesis, which is tailored for area-power-delay
optimization, yields netlists where key-gate localities are vulnerable to
learning. Thus, we call for security-aware logic synthesis. We propose ALMOST,
a framework for adversarial learning to mitigate oracle-less ML attacks via
synthesis tuning. ALMOST uses a simulated-annealing-based synthesis recipe
generator, employing adversarially trained models that can predict
state-of-the-art attacks' accuracies over wide ranges of recipes and key-gate
localities. Experiments on ISCAS benchmarks confirm the attacks' accuracies
drops to around 50\% for ALMOST-synthesized circuits, all while not undermining
design optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Design Automation Conference (DAC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Skill Acquisition for Complex Manipulation Tasks in Obstructed
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yamada, Jack Collins, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data efficiency in robotic skill acquisition is crucial for operating robots
in varied small-batch assembly settings. To operate in such environments,
robots must have robust obstacle avoidance and versatile goal conditioning
acquired from only a few simple demonstrations. Existing approaches, however,
fall short of these requirements. Deep reinforcement learning (RL) enables a
robot to learn complex manipulation tasks but is often limited to small task
spaces in the real world due to sample inefficiency and safety concerns. Motion
planning (MP) can generate collision-free paths in obstructed environments, but
cannot solve complex manipulation tasks and requires goal states often
specified by a user or object-specific pose estimator. In this work, we propose
a system for efficient skill acquisition that leverages an object-centric
generative model (OCGM) for versatile goal identification to specify a goal for
MP combined with RL to solve complex manipulation tasks in obstructed
environments. Specifically, OCGM enables one-shot target object identification
and re-identification in new scenes, allowing MP to guide the robot to the
target object while avoiding obstacles. This is combined with a skill
transition network, which bridges the gap between terminal states of MP and
feasible start states of a sample-efficient RL policy. The experiments
demonstrate that our OCGM-based one-shot goal identification provides
competitive accuracy to other baseline approaches and that our modular
framework outperforms competitive baselines, including a state-of-the-art RL
algorithm, by a significant margin for complex manipulation tasks in obstructed
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Scene Embeddings for Gradient-Based Motion Planning in Latent
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yamada, Chia-Man Hung, Jack Collins, Ioannis Havoutis, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning framed as optimisation in structured latent spaces has
recently emerged as competitive with traditional methods in terms of planning
success while significantly outperforming them in terms of computational speed.
However, the real-world applicability of recent work in this domain remains
limited by the need to express obstacle information directly in state-space,
involving simple geometric primitives. In this work we address this challenge
by leveraging learned scene embeddings together with a generative model of the
robot manipulator to drive the optimisation process. In addition, we introduce
an approach for efficient collision checking which directly regularises the
optimisation undertaken for planning. Using simulated as well as real-world
experiments, we demonstrate that our approach, AMP-LS, is able to successfully
plan in novel, complex scenes while outperforming traditional planning
baselines in terms of computation speed by an order of magnitude. We show that
the resulting system is fast enough to enable closed-loop planning in
real-world dynamic scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://amp-ls.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Activity Prediction Models in Drug Discovery with the Ability
  to Understand Human Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Seidl, Andreu Vall, Sepp Hochreiter, Günter Klambauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activity and property prediction models are the central workhorses in drug
discovery and materials sciences, but currently they have to be trained or
fine-tuned for new tasks. Without training or fine-tuning, scientific language
models could be used for such low-data tasks through their announced zero- and
few-shot capabilities. However, their predictive quality at activity prediction
is lacking. In this work, we envision a novel type of activity prediction model
that is able to adapt to new prediction tasks at inference time, via
understanding textual information describing the task. To this end, we propose
a new architecture with separate modules for chemical and natural language
inputs, and a contrastive pre-training objective on data from large biochemical
databases. In extensive experiments, we show that our method CLAMP yields
improved predictive performance on few-shot learning benchmarks and zero-shot
problems in drug discovery. We attribute the advances of our method to the
modularized architecture and to our pre-training objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages + 18 pages appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thompson Sampling for Linear Bandit Problems with Normal-Gamma Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Björn Lindenberg, Karl-Olof Lindahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider Thompson sampling for linear bandit problems with finitely many
independent arms, where rewards are sampled from normal distributions that are
linearly dependent on unknown parameter vectors and with unknown variance.
Specifically, with a Bayesian formulation we consider multivariate normal-gamma
priors to represent environment uncertainty for all involved parameters. We
show that our chosen sampling prior is a conjugate prior to the reward model
and derive a Bayesian regret bound for Thompson sampling under the condition
that the 5/2-moment of the variance distribution exist.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic Synthesis of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Whitehouse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks adapt very well to distributed and continuous
representations, but struggle to learn and generalize from small amounts of
data. Symbolic systems commonly achieve data efficient generalization by
exploiting modularity to benefit from local and discrete features of a
representation. These features allow symbolic programs to be improved one
module at a time and to experience combinatorial growth in the values they can
successfully process. However, it is difficult to design components that can be
used to form symbolic abstractions and which are highly-overparametrized like
neural networks, as the adjustment of parameters makes the semantics of modules
unstable. I present Graph-based Symbolically Synthesized Neural Networks
(G-SSNNs), a form of neural network whose topology and parameters are informed
by the output of a symbolic program. I demonstrate that by developing symbolic
abstractions at a population level, and applying gradient-based optimization to
such neural models at an individual level, I can elicit reliable patterns of
improved generalization with small quantities of data known to contain local
and discrete features. The paradigm embodied by G-SSNNs offers a route towards
the communal development of compact and composable abstractions which can be
flexibly repurposed for a variety of tasks and high-dimensional media. In
future work, I hope to pursue these benefits by exploring more ambitious G-SSNN
designs based on more complex classes of symbolic programs. The code and data
associated with the reported results are publicly available at
https://github.com/shlomenu/symbolically_synthesized_networks .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bounds for $γ$-Regret via the Decision-Estimation Coefficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margalit Glasgow, Alexander Rakhlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this note, we give a new lower bound for the $\gamma$-regret in bandit
problems, the regret which arises when comparing against a benchmark that is
$\gamma$ times the optimal solution, i.e., $\mathsf{Reg}_{\gamma}(T) = \sum_{t
= 1}^T \gamma \max_{\pi} f(\pi) - f(\pi_t)$. The $\gamma$-regret arises in
structured bandit problems where finding an exact optimum of $f$ is
intractable. Our lower bound is given in terms of a modification of the
constrained Decision-Estimation Coefficient (DEC) of~\citet{foster2023tight}
(and closely related to the original offset DEC of
\citet{foster2021statistical}), which we term the $\gamma$-DEC. When restricted
to the traditional regret setting where $\gamma = 1$, our result removes the
logarithmic factors in the lower bound of \citet{foster2023tight}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keep It Simple: CNN Model Complexity Studies for Interference
  Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiwo Oyedare, Vijay K. Shah, Daniel J. Jakubisin, Jeffrey H. Reed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing number of devices using the wireless spectrum makes it important
to find ways to minimize interference and optimize the use of the spectrum.
Deep learning models, such as convolutional neural networks (CNNs), have been
widely utilized to identify, classify, or mitigate interference due to their
ability to learn from the data directly. However, there have been limited
research on the complexity of such deep learning models. The major focus of
deep learning-based wireless classification literature has been on improving
classification accuracy, often at the expense of model complexity. This may not
be practical for many wireless devices, such as, internet of things (IoT)
devices, which usually have very limited computational resources and cannot
handle very complex models. Thus, it becomes important to account for model
complexity when designing deep learning-based models for interference
classification. To address this, we conduct an analysis of CNN based wireless
classification that explores the trade-off amongst dataset size, CNN model
complexity, and classification accuracy under various levels of classification
difficulty: namely, interference classification, heterogeneous transmitter
classification, and homogeneous transmitter classification. Our study, based on
three wireless datasets, shows that a simpler CNN model with fewer parameters
can perform just as well as a more complex model, providing important insights
into the use of CNNs in computationally constrained applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time series anomaly detection with sequence reconstruction based
  state-space model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Wang, Keli Wang, Boyu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in digitization has led to availability of multivariate time
series data in various domains, in order to monitor operations in real time.
Identifying abnormal data pattern and detect potential failures in these
scenarios are important yet rather difficult tasks. We propose a novel
unsupervised anomaly detection method for time series data. Our approach uses
sequence encoder and decoder to represent the mapping between time series and
hidden state, and learns bidirectional dynamics simultaneously by leveraging
backward and forward temporal information in the training process. We further
regularize the state space to place constraints on states of normal samples,
and use Mahalanobis distance to evaluate abnormality level. Results on
synthetic and real-world datasets show the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal contrastive pretraining has been utilized to train multimodal
representation models, like CLIP, on vast amounts of paired image-text data.
However, previous studies have highlighted the susceptibility of such models to
backdoor attacks. Specifically, when training on backdoored examples, CLIP
learns spurious correlations between the embedded backdoor trigger and the
target label, aligning their representations in the joint embedding space. With
injecting only a few poisoned examples e.g., 75 examples in the 3M pretraining
data, the model's behavior can be significantly manipulated, thus making it
hard to detect or unlearn such correlations. To address this issue, we propose
CleanCLIP, a finetuning framework that weakens the learned spurious
associations introduced by backdoor attacks by re-aligning the representations
for individual modalities independently. CleanCLIP can be employed for both
unsupervised finetuning on paired image-text data and for supervised finetuning
on labeled image data. We demonstrate that unsupervised finetuning with a
combination of multimodal contrastive and unimodal self-supervised objectives
for individual modalities can significantly reduce the impact of the backdoor
attack. Additionally, supervised finetuning on task-specific labeled data of
the individual modality, such as image data, removes the backdoor trigger from
the CLIP vision encoder. Empirically, we show that CleanCLIP maintains model
performance on benign examples while mitigating the impact of a range of
backdoor attacks on multimodal contrastive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Backdoor Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henger Li, Chen Wu, Senchun Zhu, Zizhan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a federated learning (FL) system, malicious participants can easily embed
backdoors into the aggregated model while maintaining the model's performance
on the main task. To this end, various defenses, including training stage
aggregation-based defenses and post-training mitigation defenses, have been
proposed recently. While these defenses obtain reasonable performance against
existing backdoor attacks, which are mainly heuristics based, we show that they
are insufficient in the face of more advanced attacks. In particular, we
propose a general reinforcement learning-based backdoor attack framework where
the attacker first trains a (non-myopic) attack policy using a simulator built
upon its local data and common knowledge on the FL system, which is then
applied during actual FL training. Our attack framework is both adaptive and
flexible and achieves strong attack performance and durability even under
state-of-the-art defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weight Perturbation Can Help Fairness under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimeng Jiang, Xiaotian Han, Hongye Jin, Guanchu Wang, Na Zou, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in machine learning has attracted increasing attention in recent
years. The fairness methods improving algorithmic fairness for in-distribution
data may not perform well under distribution shift. In this paper, we first
theoretically demonstrate the inherent connection between distribution shift,
data perturbation, and weight perturbation. Subsequently, we analyze the
sufficient conditions to guarantee fairness (i.e., low demographic parity) for
the target dataset, including fairness for the source dataset, and low
prediction difference between the source and target dataset for each sensitive
attribute group. Motivated by these sufficient conditions, we propose robust
fairness regularization (RFR) by considering the worst case within the weight
perturbation ball for each sensitive attribute group. In this way, the
maximization problem can be simplified as two forward and two backward
propagations for each update of model parameters. We evaluate the effectiveness
of our proposed RFR algorithm on synthetic and real distribution shifts across
various datasets. Experimental results demonstrate that RFR achieves better
fairness-accuracy trade-off performance compared with several baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiGeN: Hierarchical Multi-Resolution Graph Generative Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Karami, Jun Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real world domains, most graphs naturally exhibit a hierarchical
structure. However, data-driven graph generation is yet to effectively capture
such structures. To address this, we propose a novel approach that recursively
generates community structures at multiple resolutions, with the generated
structures conforming to training data distribution at each level of the
hierarchy. The graphs generation is designed as a sequence of coarse-to-fine
generative models allowing for parallel generation of all sub-structures,
resulting in a high degree of scalability. Furthermore, we model the output
distribution of edges with a more expressive multinomial distribution and
derive a recursive factorization for this distribution, making it a suitable
choice for graph generative models. This allows for the generation of graphs
with integer-valued edge weights. Our method achieves state-of-the-art
performance in both accuracy and efficiency on multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Wasserstein Believer: Learning Belief Updates for Partially
  Observable Environments through Reliable Latent Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Avalos, Florent Delgrange, Ann Nowé, Guillermo A. Pérez, Diederik M. Roijers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially Observable Markov Decision Processes (POMDPs) are useful tools to
model environments where the full state cannot be perceived by an agent. As
such the agent needs to reason taking into account the past observations and
actions. However, simply remembering the full history is generally intractable
due to the exponential growth in the history space. Keeping a probability
distribution that models the belief over what the true state is can be used as
a sufficient statistic of the history, but its computation requires access to
the model of the environment and is also intractable. Current state-of-the-art
algorithms use Recurrent Neural Networks (RNNs) to compress the
observation-action history aiming to learn a sufficient statistic, but they
lack guarantees of success and can lead to suboptimal policies. To overcome
this, we propose the Wasserstein-Belief-Updater (WBU), an RL algorithm that
learns a latent model of the POMDP and an approximation of the belief update.
Our approach comes with theoretical guarantees on the quality of our
approximation ensuring that our outputted beliefs allow for learning the
optimal value function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Faith</span>fulness-Aware Decoding Strategies for Abstractive Summarization <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in understanding and improving faithfulness in
abstractive summarization, the question of how decoding strategies affect
faithfulness is less studied. We present a systematic study of the effect of
generation techniques such as beam search and nucleus sampling on faithfulness
in abstractive summarization. We find a consistent trend where beam search with
large beam sizes produces the most faithful summaries while nucleus sampling
generates the least faithful ones. We propose two faithfulness-aware generation
methods to further improve faithfulness over current generation techniques: (1)
ranking candidates generated by beam search using automatic faithfulness
metrics and (2) incorporating lookahead heuristics that produce a faithfulness
score on the future summary. We show that both generation methods significantly
improve faithfulness across two datasets as evaluated by four automatic
faithfulness metrics and human evaluation. To reduce computational cost, we
demonstrate a simple distillation approach that allows the model to generate
faithful summaries with just greedy decoding. Our code is publicly available at
https://github.com/amazon-science/faithful-summarization-generation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 (17 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep symbolic regression for physics guided by units constraints: toward
  the automated discovery of physical laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wassim Tenachi, Rodrigo Ibata, Foivos I. Diakogiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic Regression is the study of algorithms that automate the search for
analytic expressions that fit data. While recent advances in deep learning have
generated renewed interest in such approaches, efforts have not been focused on
physics, where we have important additional constraints due to the units
associated with our data. Here we present $\Phi$-SO, a Physical Symbolic
Optimization framework for recovering analytical symbolic expressions from
physics data using deep reinforcement learning techniques by learning units
constraints. Our system is built, from the ground up, to propose solutions
where the physical units are consistent by construction. This is useful not
only in eliminating physically impossible solutions, but because it restricts
enormously the freedom of the equation generator, thus vastly improving
performance. The algorithm can be used to fit noiseless data, which can be
useful for instance when attempting to derive an analytical property of a
physical model, and it can also be used to obtain analytical approximations to
noisy data. We showcase our machinery on a panel of examples from astrophysics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 2 tables. Submitted to ApJ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Rates between Stochastic and Adversarial Online Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Sachs, Hedi Hadiji, Tim van Erven, Cristobal Guzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic and adversarial data are two widely studied settings in online
learning. But many optimization tasks are neither i.i.d. nor fully adversarial,
which makes it of fundamental interest to get a better theoretical
understanding of the world between these extremes. In this work we establish
novel regret bounds for online convex optimization in a setting that
interpolates between stochastic i.i.d. and fully adversarial losses. By
exploiting smoothness of the expected losses, these bounds replace a dependence
on the maximum gradient length by the variance of the gradients, which was
previously known only for linear losses. In addition, they weaken the i.i.d.
assumption by allowing, for example, adversarially poisoned rounds, which were
previously considered in the related expert and bandit settings. In the fully
i.i.d. case, our regret bounds match the rates one would expect from results in
stochastic acceleration, and we also recover the optimal stochastically
accelerated rates via online-to-batch conversion. In the fully adversarial case
our bounds gracefully deteriorate to match the minimax regret. We further
provide lower bounds showing that our regret upper bounds are tight for all
intermediate regimes in terms of the stochastic variance and the adversarial
variation of the loss gradients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of 'Between Stochastic and Adversarial Online Convex
  Optimization: Improved Regret Bounds via Smoothness' by the same authors.
  arXiv admin note: text overlap with arXiv:2202.07554</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Online Algorithm for Chance Constrained Resource Allocation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Chen, Zengde Deng, Yinzhi Zhou, Zaiyi Chen, Yujie Chen, Haoyuan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the online stochastic resource allocation problem (RAP)
with chance constraints. The online RAP is a 0-1 integer linear programming
problem where the resource consumption coefficients are revealed column by
column along with the corresponding revenue coefficients. When a column is
revealed, the corresponding decision variables are determined instantaneously
without future information. Moreover, in online applications, the resource
consumption coefficients are often obtained by prediction. To model their
uncertainties, we take the chance constraints into the consideration. To the
best of our knowledge, this is the first time chance constraints are introduced
in the online RAP problem. Assuming that the uncertain variables have known
Gaussian distributions, the stochastic RAP can be transformed into a
deterministic but nonlinear problem with integer second-order cone constraints.
Next, we linearize this nonlinear problem and analyze the performance of
vanilla online primal-dual algorithm for solving the linearized stochastic RAP.
Under mild technical assumptions, the optimality gap and constraint violation
are both on the order of $\sqrt{n}$. Then, to further improve the performance
of the algorithm, several modified online primal-dual algorithms with heuristic
corrections are proposed. Finally, extensive numerical experiments on both
synthetic and real data demonstrate the applicability and effectiveness of our
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures. Accepted to ICASSP 2023. arXiv admin note:
  substantial text overlap with arXiv:2203.16818</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Fairness of Deep Learning Uncertainty Estimates in
  Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Mehta, Changjian Shui, Tal Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning (DL) models have shown great success in many medical
image analysis tasks, deployment of the resulting models into real clinical
contexts requires: (1) that they exhibit robustness and fairness across
different sub-populations, and (2) that the confidence in DL model predictions
be accurately expressed in the form of uncertainties. Unfortunately, recent
studies have indeed shown significant biases in DL models across demographic
subgroups (e.g., race, sex, age) in the context of medical image analysis,
indicating a lack of fairness in the models. Although several methods have been
proposed in the ML literature to mitigate a lack of fairness in DL models, they
focus entirely on the absolute performance between groups without considering
their effect on uncertainty estimation. In this work, we present the first
exploration of the effect of popular fairness models on overcoming biases
across subgroups in medical image analysis in terms of bottom-line performance,
and their effects on uncertainty quantification. We perform extensive
experiments on three different clinically relevant tasks: (i) skin lesion
classification, (ii) brain tumour segmentation, and (iii) Alzheimer's disease
clinical score regression. Our results indicate that popular ML methods, such
as data-balancing and distributionally robust optimization, succeed in
mitigating fairness issues in terms of the model performances for some of the
tasks. However, this can come at the cost of poor uncertainty estimates
associated with the model predictions. This tradeoff must be mitigated if
fairness models are to be adopted in medical image analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at MIDL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Rates for Non-Log-Concave Sampling and Log-Partition
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Holzmüller, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from Gibbs distributions $p(x) \propto \exp(-V(x)/\varepsilon)$ and
computing their log-partition function are fundamental tasks in statistics,
machine learning, and statistical physics. However, while efficient algorithms
are known for convex potentials $V$, the situation is much more difficult in
the non-convex case, where algorithms necessarily suffer from the curse of
dimensionality in the worst case. For optimization, which can be seen as a
low-temperature limit of sampling, it is known that smooth functions $V$ allow
faster convergence rates. Specifically, for $m$-times differentiable functions
in $d$ dimensions, the optimal rate for algorithms with $n$ function
evaluations is known to be $O(n^{-m/d})$, where the constant can potentially
depend on $m, d$ and the function to be optimized. Hence, the curse of
dimensionality can be alleviated for smooth functions at least in terms of the
convergence rate. Recently, it has been shown that similarly fast rates can
also be achieved with polynomial runtime $O(n^{3.5})$, where the exponent $3.5$
is independent of $m$ or $d$. Hence, it is natural to ask whether similar rates
for sampling and log-partition computation are possible, and whether they can
be realized in polynomial time with an exponent independent of $m$ and $d$. We
show that the optimal rates for sampling and log-partition computation are
sometimes equal and sometimes faster than for optimization. We then analyze
various polynomial-time sampling algorithms, including an extension of a recent
promising optimization approach, and find that they sometimes exhibit
interesting behavior but no near-optimal rates. Our results also give further
insights on the relation between sampling, log-partition, and optimization
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Plots can be reproduced using the code at
  https://github.com/dholzmueller/sampling_experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Hybrid Networks: an interplay between quantum and classical
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Kordzanganeh, Daria Kosichkina, Alexey Melnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum neural networks represent a new machine learning paradigm that has
recently attracted much attention due to its potential promise. Under certain
conditions, these models approximate the distribution of their dataset with a
truncated Fourier series. The trigonometric nature of this fit could result in
angle-embedded quantum neural networks struggling to fit the non-harmonic
features in a given dataset. Moreover, the interpretability of neural networks
remains a challenge. In this work, we introduce a new, interpretable class of
hybrid quantum neural networks that pass the inputs of the dataset in parallel
to 1) a classical multi-layered perceptron and 2) a variational quantum
circuit, and then the outputs of the two are linearly combined. We observe that
the quantum neural network creates a smooth sinusoidal foundation base on the
training set, and then the classical perceptrons fill the non-harmonic gaps in
the landscape. We demonstrate this claim on two synthetic datasets sampled from
periodic distributions with added protrusions as noise. The training results
indicate that the parallel hybrid network architecture could improve the
solution optimality on periodic datasets with additional noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Reinforcement Learning via Probabilistic Logic Shields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Chi Yang, Giuseppe Marra, Gavin Rens, Luc De Raedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe Reinforcement learning (Safe RL) aims at learning optimal policies while
staying safe. A popular solution to Safe RL is shielding, which uses a logical
safety specification to prevent an RL agent from taking unsafe actions.
However, traditional shielding techniques are difficult to integrate with
continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic
Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses
probabilistic logic programming to model logical safety constraints as
differentiable functions. Therefore, PLPG can be seamlessly applied to any
policy gradient algorithm while still providing the same convergence
guarantees. In our experiments, we show that PLPG learns safer and more
rewarding policies compared to other state-of-the-art shielding techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Differentiable Causal Discovery via Adaptive Sample Reweighting <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Zhang, Fangfu Liu, Wenchang Ma, Zhibo Cai, Xiang Wang, Tat-seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under stringent model type and variable distribution assumptions,
differentiable score-based causal discovery methods learn a directed acyclic
graph (DAG) from observational data by evaluating candidate graphs over an
average score function. Despite great success in low-dimensional linear
systems, it has been observed that these approaches overly exploit
easier-to-fit samples, thus inevitably learning spurious edges. Worse still,
inherent mostly in these methods the common homogeneity assumption can be
easily violated, due to the widespread existence of heterogeneous data in the
real world, resulting in performance vulnerability when noise distributions
vary. We propose a simple yet effective model-agnostic framework to boost
causal discovery performance by dynamically learning the adaptive weights for
the Reweighted Score function, ReScore for short, where the weights tailor
quantitatively to the importance degree of each sample. Intuitively, we
leverage the bilevel optimization scheme to \wx{alternately train a standard
DAG learner and reweight samples -- that is, upweight the samples the learner
fails to fit and downweight the samples that the learner easily extracts the
spurious information from. Extensive experiments on both synthetic and
real-world datasets are carried out to validate the effectiveness of ReScore.
We observe consistent and significant boosts in structure learning performance.
Furthermore, we visualize that ReScore concurrently mitigates the influence of
spurious edges and generalizes to heterogeneous data. Finally, we perform the
theoretical analysis to guarantee the structure identifiability and the weight
adaptive properties of ReScore in linear systems. Our codes are available at
https://github.com/anzhang314/ReScore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaPhysiCa: OOD Robustness in Physics-informed Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S Chandra Mouli, Muhammad Ashraful Alam, Bruno Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental challenge in physics-informed machine learning (PIML) is the
design of robust PIML methods for out-of-distribution (OOD) forecasting tasks.
These OOD tasks require learning-to-learn from observations of the same (ODE)
dynamical system with different unknown ODE parameters, and demand accurate
forecasts even under out-of-support initial conditions and out-of-support ODE
parameters. In this work we propose a solution for such tasks, which we define
as a meta-learning procedure for causal structure discovery (including
invariant risk minimization). Using three different OOD tasks, we empirically
observe that the proposed approach significantly outperforms existing
state-of-the-art PIML and deep learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Algebraic Perspective on Lipschitz Neural Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Araujo, Aaron Havens, Blaise Delattre, Alexandre Allauzen, Bin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Important research efforts have focused on the design and training of neural
networks with a controlled Lipschitz constant. The goal is to increase and
sometimes guarantee the robustness against adversarial attacks. Recent
promising techniques draw inspirations from different backgrounds to design
1-Lipschitz neural networks, just to name a few: convex potential layers derive
from the discretization of continuous dynamical systems,
Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling.
However, it is today important to consider the recent and promising
contributions in the field under a common theoretical lens to better design new
and improved layers. This paper introduces a novel algebraic perspective
unifying various types of 1-Lipschitz neural networks, including the ones
previously mentioned, along with methods based on orthogonality and spectral
methods. Interestingly, we show that many existing techniques can be derived
and generalized via finding analytical solutions of a common semidefinite
programming (SDP) condition. We also prove that AOL biases the scaled weight to
the ones which are close to the set of orthogonal matrices in a certain
mathematical manner. Moreover, our algebraic condition, combined with the
Gershgorin circle theorem, readily leads to new and diverse parameterizations
for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers
(SLL), allows us to design non-trivial yet efficient generalization of convex
potential layers. Finally, the comprehensive set of experiments on image
classification shows that SLLs outperform previous approaches on certified
robust accuracy. Code is available at
https://github.com/araujoalexandre/Lipschitz-SLL-Networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Spotlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Control with Inherent Lyapunov Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjae Min, Spencer M. Richards, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in learning-based control leverage deep function
approximators, such as neural networks, to model the evolution of controlled
dynamical systems over time. However, the problem of learning a dynamics model
and a stabilizing controller persists, since the synthesis of a stabilizing
feedback law for known nonlinear systems is a difficult task, let alone for
complex parametric representations that must be fit to data. To this end, we
propose a method for jointly learning parametric representations of a nonlinear
dynamics model and a stabilizing controller from data. To do this, our approach
simultaneously learns a parametric Lyapunov function which intrinsically
constrains the dynamics model to be stabilizable by the learned controller. In
addition to the stabilizability of the learned dynamics guaranteed by our novel
construction, we show that the learned controller stabilizes the true dynamics
under certain assumptions on the fidelity of the learned dynamics. Finally, we
demonstrate the efficacy of our method on a variety of simulated nonlinear
dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SC-Block: Supervised Contrastive Blocking within Entity Resolution
  Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Brinkmann, Roee Shraga, Christian Bizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of entity resolution is to identify records in multiple datasets
that represent the same real-world entity. However, comparing all records
across datasets can be computationally intensive, leading to long runtimes. To
reduce these runtimes, entity resolution pipelines are constructed of two
parts: a blocker that applies a computationally cheap method to select
candidate record pairs, and a matcher that afterwards identifies matching pairs
from this set using more expensive methods. This paper presents SC-Block, a
blocking method that utilizes supervised contrastive learning for positioning
records in the embedding space, and nearest neighbour search for candidate set
building. We benchmark SC-Block against eight state-of-the-art blocking
methods. In order to relate the training time of SC-Block to the reduction of
the overall runtime of the entity resolution pipeline, we combine SC-Block with
four matching methods into complete pipelines. For measuring the overall
runtime, we determine candidate sets with 98% pair completeness and pass them
to the matcher. The results show that SC-Block is able to create smaller
candidate sets and pipelines with SC-Block execute 1.5 to 2 times faster
compared to pipelines with other blockers, without sacrificing F1 score.
Blockers are often evaluated using relatively small datasets which might lead
to runtime effects resulting from a large vocabulary size being overlooked. In
order to measure runtimes in a more challenging setting, we introduce a new
benchmark dataset that requires large numbers of product offers to be blocked.
On this large-scale benchmark dataset, pipelines utilizing SC-Block and the
best-performing matcher execute 8 times faster than pipelines utilizing another
blocker with the same matcher reducing the runtime from 2.5 hours to 18
minutes, clearly compensating for the 5 minutes required for training SC-Block.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmark of Data Preprocessing Methods for Imbalanced Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radovan Haluška, Jan Brabec, Tomáš Komárek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Severe class imbalance is one of the main conditions that make machine
learning in cybersecurity difficult. A variety of dataset preprocessing methods
have been introduced over the years. These methods modify the training dataset
by oversampling, undersampling or a combination of both to improve the
predictive performance of classifiers trained on this dataset. Although these
methods are used in cybersecurity occasionally, a comprehensive, unbiased
benchmark comparing their performance over a variety of cybersecurity problems
is missing. This paper presents a benchmark of 16 preprocessing methods on six
cybersecurity datasets together with 17 public imbalanced datasets from other
domains. We test the methods under multiple hyperparameter configurations and
use an AutoML system to train classifiers on the preprocessed datasets, which
reduces potential bias from specific hyperparameter or classifier choices.
Special consideration is also given to evaluating the methods using appropriate
performance measures that are good proxies for practical performance in
real-world cybersecurity systems. The main findings of our study are: 1) Most
of the time, a data preprocessing method that improves classification
performance exists. 2) Baseline approach of doing nothing outperformed a large
portion of methods in the benchmark. 3) Oversampling methods generally
outperform undersampling methods. 4) The most significant performance gains are
brought by the standard SMOTE algorithm and more complicated methods provide
mainly incremental improvements at the cost of often worse computational
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 2022 IEEE International Conference on Big Data (Big
  Data) 2022. Extended version with full results in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Environment Invariant Linear Least Squares 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Fan, Cong Fang, Yihong Gu, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers a multiple environments linear regression model in which
data from multiple experimental settings are collected. The joint distribution
of the response variable and covariate may vary across different environments,
yet the conditional expectation of $y$ given the unknown set of important
variables are invariant across environments. Such a statistical model is
related to the problem of endogeneity, causal inference, and transfer learning.
The motivation behind it is illustrated by how the goals of prediction and
attribution are inherent in estimating the true parameter and the important
variable set. We construct a novel {\it environment invariant linear least
squares (EILLS)} objective function, a multiple-environment version of linear
least squares that leverages the above conditional expectation invariance
structure and heterogeneity among different environments to determine the true
parameter. Our proposed method is applicable without any additional structural
knowledge and can identify the true parameter under a near-minimal
identification condition. We establish non-asymptotic $\ell_2$ error bounds on
the estimation error for the EILLS estimator in the presence of spurious
variables. Moreover, we further show that the EILLS estimator is able to
eliminate all endogenous variables and the $\ell_0$ penalized EILLS estimator
can achieve variable selection consistency in high-dimensional regimes. These
non-asymptotic results demonstrate the sample efficiency of the EILLS estimator
and its capability to circumvent the curse of endogeneity in an algorithmic
manner without any prior structural knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Regression in Extreme Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Huet, Stephan Clémençon, Anne Sabourin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the classic regression problem, the value of a real-valued random variable
$Y$ is to be predicted based on the observation of a random vector $X$, taking
its values in $\mathbb{R}^d$ with $d\geq 1$ say. The statistical learning
problem consists in building a predictive function $\hat{f}:\mathbb{R}^d\to
\mathbb{R}$ based on independent copies of the pair $(X,Y)$ so that $Y$ is
approximated by $\hat{f}(X)$ with minimum error in the mean-squared sense.
Motivated by various applications, ranging from environmental sciences to
finance or insurance, special attention is paid here to the case of extreme
(i.e. very large) observations $X$. Because of their rarity, they contribute in
a negligible manner to the (empirical) error and the predictive performance of
empirical quadratic risk minimizers can be consequently very poor in extreme
regions. In this paper, we develop a general framework for regression in the
extremes. It is assumed that $X$'s conditional distribution given $Y$ belongs
to a non parametric class of heavy-tailed probability distributions. It is then
shown that an asymptotic notion of risk can be tailored to summarize
appropriately predictive performance in extreme regions of the input space. It
is also proved that minimization of an empirical and non asymptotic version of
this 'extreme risk', based on a fraction of the largest observations solely,
yields regression functions with good generalization capacity. In addition,
numerical results providing strong empirical evidence of the relevance of the
approach proposed are displayed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (main paper), 10 pages (appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A neural network based model for multi-dimensional nonlinear Hawkes
  processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sobin Joseph, Shashi Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Neural Network for Nonlinear Hawkes processes
(NNNH), a non-parametric method based on neural networks to fit nonlinear
Hawkes processes. Our method is suitable for analyzing large datasets in which
events exhibit both mutually-exciting and inhibitive patterns. The NNNH
approach models the individual kernels and the base intensity of the nonlinear
Hawkes process using feed forward neural networks and jointly calibrates the
parameters of the networks by maximizing the log-likelihood function. We
utilize Stochastic Gradient Descent to search for the optimal parameters and
propose an unbiased estimator for the gradient, as well as an efficient
computation method. We demonstrate the flexibility and accuracy of our method
through numerical experiments on both simulated and real-world data, and
compare it with state-of-the-art methods. Our results highlight the
effectiveness of the NNNH method in accurately capturing the complexities of
nonlinear Hawkes processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning Based Self-play and State Stacking Techniques for
  Noisy Air Combat Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Semih Tasbas, Safa Onur Sahin, Nazim Kemal Ure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has recently proven itself as a powerful
instrument for solving complex problems and even surpassed human performance in
several challenging applications. This signifies that RL algorithms can be used
in the autonomous air combat problem, which has been studied for many years.
The complexity of air combat arises from aggressive close-range maneuvers and
agile enemy behaviors. In addition to these complexities, there may be
uncertainties in real-life scenarios due to sensor errors, which prevent
estimation of the actual position of the enemy. In this case, autonomous
aircraft should be successful even in the noisy environments. In this study, we
developed an air combat simulation, which provides noisy observations to the
agents, therefore, make the air combat problem even more challenging. Thus, we
present a state stacking method for noisy RL environments as a noise reduction
technique. In our extensive set of experiments, the proposed method
significantly outperforms the baseline algorithms in terms of the winning
ratio, where the performance improvement is even more pronounced in the high
noise levels. In addition, we incorporate a self-play scheme to our training
process by periodically updating the enemy with a frozen copy of the training
agent. By this way, the training agent performs air combat simulations to an
enemy with smarter strategies, which improves the performance and robustness of
the agents. In our simulations, we demonstrate that the self-play scheme
provides important performance gains compared to the classical RL training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Images Are Counterfactual Samples for Robust Fine-tuning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are challenged by the distribution shift between the
training data and test data. Recently, the large models pre-trained on diverse
data demonstrate unprecedented robustness to various distribution shifts.
However, fine-tuning on these models can lead to a trade-off between
in-distribution (ID) performance and out-of-distribution (OOD) robustness.
Existing methods for tackling this trade-off do not explicitly address the OOD
robustness problem. In this paper, based on causal analysis on the
aforementioned problems, we propose a novel fine-tuning method, which use
masked images as counterfactual samples that help improving the robustness of
the fine-tuning model. Specifically, we mask either the semantics-related or
semantics-unrelated patches of the images based on class activation map to
break the spurious correlation, and refill the masked patches with patches from
other images. The resulting counterfactual samples are used in feature-based
distillation with the pre-trained model. Extensive experiments verify that
regularizing the fine-tuning with the proposed masked images can achieve a
better trade-off between ID and OOD, surpassing previous methods on the OOD
performance. Our code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Neural Networks as 2-D systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gramlich, Patricia Pauli, Carsten W. Scherer, Frank Allgöwer, Christian Ebenbauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel representation of convolutional Neural Networks
(CNNs) in terms of 2-D dynamical systems. To this end, the usual description of
convolutional layers with convolution kernels, i.e., the impulse responses of
linear filters, is realized in state space as a linear time-invariant 2-D
system. The overall convolutional Neural Network composed of convolutional
layers and nonlinear activation functions is then viewed as a 2-D version of a
Lur'e system, i.e., a linear dynamical system interconnected with static
nonlinear components. One benefit of this 2-D Lur'e system perspective on CNNs
is that we can use robust control theory much more efficiently for Lipschitz
constant estimation than previously possible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Clustering with a Constraint for Topological Invariance based on
  Symmetric InfoNCE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Yuichiro Wada, Hiroki Waida, Kaito Goto, Yusaku Hino, Takafumi Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the scenario of deep clustering, in which the available prior
knowledge is limited. In this scenario, few existing state-of-the-art deep
clustering methods can perform well for both non-complex topology and complex
topology datasets. To address the problem, we propose a constraint utilizing
symmetric InfoNCE, which helps an objective of deep clustering method in the
scenario train the model so as to be efficient for not only non-complex
topology but also complex topology datasets. Additionally, we provide several
theoretical explanations of the reason why the constraint can enhances
performance of deep clustering methods. To confirm the effectiveness of the
proposed constraint, we introduce a deep clustering method named MIST, which is
a combination of an existing deep clustering method and our constraint. Our
numerical experiments via MIST demonstrate that the constraint is effective. In
addition, MIST outperforms other state-of-the-art deep clustering methods for
most of the commonly used ten benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Critical Points and Convergence Analysis of Generative Deep Linear
  Networks Trained with Bures-Wasserstein <span class="highlight-title">Loss</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Bréchet, Katerina Papagiannouli, Jing An, Guido Montúfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a deep matrix factorization model of covariance matrices trained
with the Bures-Wasserstein distance. While recent works have made important
advances in the study of the optimization problem for overparametrized low-rank
matrix approximation, much emphasis has been placed on discriminative settings
and the square loss. In contrast, our model considers another interesting type
of loss and connects with the generative setting. We characterize the critical
points and minimizers of the Bures-Wasserstein distance over the space of
rank-bounded matrices. For low-rank matrices the Hessian of this loss can
theoretically blow up, which creates challenges to analyze convergence of
optimizaton methods. We establish convergence results for gradient flow using a
smooth perturbative version of the loss and convergence results for finite step
size gradient descent under certain assumptions on the initial weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding Energy-based Models via Contrastive Latent Variables <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hankook Lee, Jongheon Jeong, Sejun Park, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An energy-based model (EBM) is a popular generative framework that offers
both explicit density and architectural flexibility, but training them is
difficult since it is often unstable and time-consuming. In recent years,
various training techniques have been developed, e.g., better divergence
measures or stabilization in MCMC sampling, but there often exists a large gap
between EBMs and other generative frameworks like GANs in terms of generation
quality. In this paper, we propose a novel and effective framework for
improving EBMs via contrastive representation learning (CRL). To be specific,
we consider representations learned by contrastive methods as the true
underlying latent variable. This contrastive latent variable could guide EBMs
to understand the data structure better, so it can improve and accelerate EBM
training significantly. To enable the joint training of EBM and CRL, we also
design a new class of latent-variable EBMs for learning the joint density of
data and the contrastive latent variable. Our experimental results demonstrate
that our scheme achieves lower FID scores, compared to prior-art EBM methods
(e.g., additionally using variational autoencoders or diffusion techniques),
even with significantly faster and more memory-efficient training. We also show
conditional and compositional generation abilities of our latent-variable EBMs
as their additional benefits, even without explicit conditional training. The
code is available at https://github.com/hankook/CLEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023 (Spotlight). The code is available at
  https://github.com/hankook/CLEL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling strategies for on-device low-complexity source separation with
  Conv-Tasnet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Nabih Ali, Francesco Paissan, Daniele Falavigna, Alessio Brutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, several very effective neural approaches for single-channel speech
separation have been presented in the literature. However, due to the size and
complexity of these models, their use on low-resource devices, e.g. for hearing
aids, and earphones, is still a challenge and established solutions are not
available yet. Although approaches based on either pruning or compressing
neural models have been proposed, the design of a model architecture suitable
for a certain application domain often requires heuristic procedures not easily
portable to different low-resource platforms. Given the modular nature of the
well-known Conv-Tasnet speech separation architecture, in this paper we
consider three parameters that directly control the overall size of the model,
namely: the number of residual blocks, the number of repetitions of the
separation blocks and the number of channels in the depth-wise convolutions,
and experimentally evaluate how they affect the speech separation performance.
In particular, experiments carried out on the Libri2Mix show that the number of
dilated 1D-Conv blocks is the most critical parameter and that the usage of
extra-dilation in the residual blocks allows reducing the performance drop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCLIP: Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing with Hierarchy-aware
  Attention <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large-scale contrastive vision-language pretraining (CLIP) has
benefited both visual recognition and multimodal content understanding. The
concise design brings CLIP the advantage in inference efficiency against other
vision-language models with heavier cross-attention fusion layers, making it a
popular choice for a wide spectrum of downstream tasks. However, CLIP does not
explicitly capture the hierarchical nature of high-level and fine-grained
semantics conveyed in images and texts, which is arguably critical to
vision-language understanding and reasoning. To this end, we equip both the
visual and language branches in CLIP with hierarchy-aware attentions, namely
Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies
layer-by-layer from both images and texts in an unsupervised manner. As a
result, such hierarchical aggregation significantly improves the cross-modal
alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative
analysis on its unsupervised hierarchy induction during inference, as well as
extensive quantitative experiments on both visual recognition and
vision-language downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching for Effective Neural Network Architectures for Heart Murmur
  Detection from Phonocardiogram <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wen, Jingsu Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aim: The George B. Moody PhysioNet Challenge 2022 raised problems of heart
murmur detection and related abnormal cardiac function identification from
phonocardiograms (PCGs). This work describes the novel approaches developed by
our team, Revenger, to solve these problems.
  Methods: PCGs were resampled to 1000 Hz, then filtered with a Butterworth
band-pass filter of order 3, cutoff frequencies 25 - 400 Hz, and z-score
normalized. We used the multi-task learning (MTL) method via hard parameter
sharing to train one neural network (NN) model for all the Challenge tasks. We
performed neural architecture searching among a set of network backbones,
including multi-branch convolutional neural networks (CNNs), SE-ResNets,
TResNets, simplified wav2vec2, etc.
  Based on a stratified splitting of the subjects, 20% of the public data was
left out as a validation set for model selection. The AdamW optimizer was
adopted, along with the OneCycle scheduler, to optimize the model weights.
  Results: Our murmur detection classifier received a weighted accuracy score
of 0.736 (ranked 14th out of 40 teams) and a Challenge cost score of 12944
(ranked 19th out of 39 teams) on the hidden validation set.
  Conclusion: We provided a practical solution to the problems of detecting
heart murmurs and providing clinical diagnosis suggestions from PCGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, Computing in Cardiology 2022, URL:
  https://github.com/DeepPSP/cinc2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning multi-scale local conditional probability models of images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Kadkhodaie, Florentin Guth, Stéphane Mallat, Eero P Simoncelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks can learn powerful prior probability models for images,
as evidenced by the high-quality generations obtained with recent score-based
diffusion methods. But the means by which these networks capture complex global
statistical structure, apparently without suffering from the curse of
dimensionality, remain a mystery. To study this, we incorporate diffusion
methods into a multi-scale decomposition, reducing dimensionality by assuming a
stationary local Markov model for wavelet coefficients conditioned on
coarser-scale coefficients. We instantiate this model using convolutional
neural networks (CNNs) with local receptive fields, which enforce both the
stationarity and Markov properties. Global structures are captured using a CNN
with receptive fields covering the entire (but small) low-pass image. We test
this model on a dataset of face images, which are highly non-stationary and
contain large-scale geometric structures. Remarkably, denoising,
super-resolution, and image synthesis results all demonstrate that these
structures can be captured with significantly smaller conditioning
neighborhoods than required by a Markov model implemented in the pixel domain.
Our results show that score estimation for large complex images can be reduced
to low-dimensional Markov conditional models across scales, alleviating the
curse of dimensionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KDSM: An uplift modeling framework based on <span class="highlight-title">knowledge</span> distillation and
  sample matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Sun, Qianying Li, Guanxiang Wang, Sihao Xu, Yitong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uplift modeling aims to estimate the treatment effect on individuals, widely
applied in the e-commerce platform to target persuadable customers and maximize
the return of marketing activities. Among the existing uplift modeling methods,
tree-based methods are adept at fitting increment and generalization, while
neural-network-based models excel at predicting absolute value and precision,
and these advantages have not been fully explored and combined. Also, the lack
of counterfactual sample pairs is the root challenge in uplift modeling. In
this paper, we proposed an uplift modeling framework based on Knowledge
Distillation and Sample Matching (KDSM). The teacher model is the uplift
decision tree (UpliftDT), whose structure is exploited to construct
counterfactual sample pairs, and the pairwise incremental prediction is treated
as another objective for the student model. Under the idea of multitask
learning, the student model can achieve better performance on generalization
and even surpass the teacher. Extensive offline experiments validate the
universality of different combinations of teachers and student models and the
superiority of KDSM measured against the baselines. In online A/B testing, the
cost of each incremental room night is reduced by 6.5\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Confidence Calibration for Failure Prediction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Zhu, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable confidence estimation for the predictions is important in many
safety-critical applications. However, modern deep neural networks are often
overconfident for their incorrect predictions. Recently, many calibration
methods have been proposed to alleviate the overconfidence problem. With
calibrated confidence, a primary and practical purpose is to detect
misclassification errors by filtering out low-confidence predictions (known as
failure prediction). In this paper, we find a general, widely-existed but
actually-neglected phenomenon that most confidence calibration methods are
useless or harmful for failure prediction. We investigate this problem and
reveal that popular confidence calibration methods often lead to worse
confidence separation between correct and incorrect samples, making it more
difficult to decide whether to trust a prediction or not. Finally, inspired by
the natural connection between flat minima and confidence separation, we
propose a simple hypothesis: flat minima is beneficial for failure prediction.
We verify this hypothesis via extensive experiments and further boost the
performance by combining two different flat minima techniques. Our code is
available at https://github.com/Impression2805/FMFP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at
  https://github.com/Impression2805/FMFP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Parametric Outlier Synthesis <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leitian Tao, Xuefeng Du, Xiaojin Zhu, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is indispensable for safely deploying
machine learning models in the wild. One of the key challenges is that models
lack supervision signals from unknown data, and as a result, can produce
overconfident predictions on OOD data. Recent work on outlier synthesis modeled
the feature space as parametric Gaussian distribution, a strong and restrictive
assumption that might not hold in reality. In this paper, we propose a novel
framework, Non-Parametric Outlier Synthesis (NPOS), which generates artificial
OOD training data and facilitates learning a reliable decision boundary between
ID and OOD data. Importantly, our proposed synthesis approach does not make any
distributional assumption on the ID embeddings, thereby offering strong
flexibility and generality. We show that our synthesis approach can be
mathematically interpreted as a rejection sampling framework. Extensive
experiments show that NPOS can achieve superior OOD detection performance,
outperforming the competitive rivals by a significant margin. Code is publicly
available at https://github.com/deeplearning-wisc/npos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Primal and Dual Analysis of Entropic Fictitious Play for Finite-sum
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Nitanda, Kazusato Oko, Denny Wu, Nobuhito Takenouchi, Taiji Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The entropic fictitious play (EFP) is a recently proposed algorithm that
minimizes the sum of a convex functional and entropy in the space of measures
-- such an objective naturally arises in the optimization of a two-layer neural
network in the mean-field regime. In this work, we provide a concise
primal-dual analysis of EFP in the setting where the learning problem exhibits
a finite-sum structure. We establish quantitative global convergence guarantees
for both the continuous-time and discrete-time dynamics based on properties of
a proximal Gibbs measure introduced in Nitanda et al. (2022). Furthermore, our
primal-dual framework entails a memory-efficient particle-based implementation
of the EFP update, and also suggests a connection to gradient boosting methods.
We illustrate the efficiency of our novel implementation in experiments
including neural network optimization and image synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centroid Distance Distillation for Effective Rehearsal in Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daofeng Liu, Fan Lyu, Linyan Li, Zhenping Xia, Fuyuan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rehearsal, retraining on a stored small data subset of old tasks, has been
proven effective in solving catastrophic forgetting in continual learning.
However, due to the sampled data may have a large bias towards the original
dataset, retraining them is susceptible to driving continual domain drift of
old tasks in feature space, resulting in forgetting. In this paper, we focus on
tackling the continual domain drift problem with centroid distance
distillation. First, we propose a centroid caching mechanism for sampling data
points based on constructed centroids to reduce the sample bias in rehearsal.
Then, we present a centroid distance distillation that only stores the centroid
distance to reduce the continual domain drift. The experiments on four
continual learning datasets show the superiority of the proposed method, and
the continual domain drift can be reduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Positional Encoding via Random Feature Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Eliasof, Fabrizio Frasca, Beatrice Bevilacqua, Eran Treister, Gal Chechik, Haggai Maron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two main families of node feature augmentation schemes have been explored for
enhancing GNNs: random features and spectral positional encoding. Surprisingly,
however, there is still no clear understanding of the relation between these
two augmentation schemes. Here we propose a novel family of positional encoding
schemes which draws a link between the above two approaches and improves over
both. The new approach, named Random Feature Propagation (RFP), is inspired by
the power iteration method and its generalizations. It concatenates several
intermediate steps of an iterative algorithm for computing the dominant
eigenvectors of a propagation matrix, starting from random node features.
Notably, these propagation steps are based on graph-dependent propagation
operators that can be either predefined or learned. We explore the theoretical
and empirical benefits of RFP. First, we provide theoretical justifications for
using random features, for incorporating early propagation steps, and for using
multiple random initializations. Then, we empirically demonstrate that RFP
significantly outperforms both spectral PE and random features in multiple node
classification and graph classification benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Topological Distance Measure between Multi-Fields for Classification
  and Analysis of Shapes and Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashwanth Ramamurthi, Amit Chattopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distance measures play an important role in shape classification and data
analysis problems. Topological distances based on Reeb graphs and persistence
diagrams have been employed to obtain effective algorithms in shape matching
and scalar data analysis. In the current paper, we propose an improved distance
measure between two multi-fields by computing a multi-dimensional Reeb graph
(MDRG) each of which captures the topology of a multi-field through a hierarchy
of Reeb graphs in different dimensions. A hierarchy of persistence diagrams is
then constructed by computing a persistence diagram corresponding to each Reeb
graph of the MDRG. Based on this representation, we propose a novel distance
measure between two MDRGs by extending the bottleneck distance between two Reeb
graphs. We show that the proposed measure satisfies the pseudo-metric and
stability properties. We examine the effectiveness of the proposed multi-field
topology-based measure on two different applications: (1) shape classification
and (2) detection of topological features in a time-varying multi-field data.
In the shape classification problem, the performance of the proposed measure is
compared with the well-known topology-based measures in shape matching. In the
second application, we consider a time-varying volumetric multi-field data from
the field of computational chemistry where the goal is to detect the site of
stable bond formation between Pt and CO molecules. We demonstrate the ability
of the proposed distance in classifying each of the sites as occurring before
and after the bond stabilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The $α$-divergence Improves the Entropy Production Estimation via
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euijoon Kwon, Yongjoo Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen a surge of interest in the algorithmic estimation of
stochastic entropy production (EP) from the trajectory data via machine
learning. A crucial element of such algorithms is the identification of a loss
function whose minimization guarantees the accurate EP estimation. In this
study, we show that there exists a host of loss functions, namely those
implementing a variational representation of the $\alpha$-divergence, which can
be used for the EP estimation. Among these loss functions, the one
corresponding to $\alpha = -0.5$ exhibits the most robust performance against
strong nonequilibrium driving or slow dynamics, which adversely affects the
existing method based on the Kullback-Leibler divergence ($\alpha = 0$). To
corroborate our findings, we present an exactly solvable simplification of the
EP estimation problem, whose loss function landscape and stochastic properties
demonstrate the optimality of $\alpha = -0.5$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics
  at Single-Precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candi Zheng, Wang Yang, Shiyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing extended hydrodynamics equations valid for both dense and rarefied
gases remains a great challenge. A systematical solution for this challenge is
the moment method describing both dense and rarefied gas behaviors with moments
of gas molecule velocity distributions. Among moment methods, the maximal
entropy moment method (MEM) stands out for its well-posedness and stability,
which utilizes velocity distributions with maximized entropy. However, finding
such distributions requires solving an ill-conditioned and
computation-demanding optimization problem. This problem causes numerical
overflow and breakdown when the numerical precision is insufficient, especially
for flows like high-speed shock waves. It also prevents modern GPUs from
accelerating optimization with their enormous single floating-point precision
computation power. This paper aims to stabilize MEM, making it practical for
simulating very strong normal shock waves on modern GPUs at single precision.
We propose the gauge transformations for MEM, making the optimization less
ill-conditioned. We also tackle numerical overflow and breakdown by adopting
the canonical form of distribution and Newton's modified optimization method.
With these techniques, we achieved a single-precision GPU simulation of a Mach
10 shock wave with 35 moments MEM, surpassing the previous double-precision
results of Mach 4. Moreover, we argued that over-refined spatial mesh degrades
both the accuracy and stability of MEM. Overall, this paper makes the maximal
entropy moment method practical for simulating very strong normal shock waves
on modern GPUs at single-precision, with significant stability improvement
compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspectives on the Social Impacts of Reinforcement Learning with Human
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Kaili-May Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Is it possible for machines to think like humans? And if it is, how should we
go about teaching them to do so? As early as 1950, Alan Turing stated that we
ought to teach machines in the way of teaching a child. Reinforcement learning
with human feedback (RLHF) has emerged as a strong candidate toward allowing
agents to learn from human feedback in a naturalistic manner. RLHF is distinct
from traditional reinforcement learning as it provides feedback from a human
teacher in addition to a reward signal. It has been catapulted into public view
by multiple high-profile AI applications, including OpenAI's ChatGPT,
DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are
already overturning our understanding of how AI interacts with humanity. The
wide applicability and burgeoning success of RLHF strongly motivate the need to
evaluate its social impacts. In light of recent developments, this paper
considers an important question: can RLHF be developed and used without
negatively affecting human societies? Our objectives are threefold: to provide
a systematic study of the social effects of RLHF; to identify key social and
ethical issues of RLHF; and to discuss social impacts for stakeholders.
Although text-based applications of RLHF have received much attention, it is
crucial to consider when evaluating its social implications the diverse range
of areas to which it may be deployed. We describe seven primary ways in which
RLHF-based technologies will affect society by positively transforming human
experiences with AI. This paper ultimately proposes that RLHF has potential to
net positively impact areas of misinformation, AI value-alignment, bias, AI
access, cross-cultural dialogue, industry, and workforce. As RLHF raises
concerns that echo those of existing AI technologies, it will be important for
all to be aware and intentional in the adoption of RLHF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Small
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whilst the partial differential equations that govern the dynamics of our
world have been studied in great depth for centuries, solving them for complex,
high-dimensional conditions and domains still presents an incredibly large
mathematical and computational challenge. Analytical methods can be cumbersome
to utilise, and numerical methods can lead to errors and inaccuracies. On top
of this, sometimes we lack the information or knowledge to pose the problem
well enough to apply these kinds of methods. Here, we present a new approach to
approximating the solution to physical systems - physics-informed neural
networks. The concept of artificial neural networks is introduced, the
objective function is defined, and optimisation strategies are discussed. The
partial differential equation is then included as a constraint in the loss
function for the optimisation problem, giving the network access to knowledge
of the dynamics of the physical system it is modelling. Some intuitive examples
are displayed, and more complex applications are considered to showcase the
power of physics informed neural networks, such as in seismic imaging. Solution
error is analysed, and suggestions are made to improve convergence and/or
solution precision. Problems and limitations are also touched upon in the
conclusions, as well as some thoughts as to where physics informed neural
networks are most useful, and where they could go next.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Sketching: Centering Concepts in Early-Stage Machine Learning
  Model Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle S. Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A. Landay, Michael S. Bernstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning practitioners often end up tunneling on low-level technical
details like model architectures and performance metrics. Could early model
development instead focus on high-level questions of which factors a model
ought to pay attention to? Inspired by the practice of sketching in design,
which distills ideas to their minimal representation, we introduce model
sketching: a technical framework for iteratively and rapidly authoring
functional approximations of a machine learning model's decision-making logic.
Model sketching refocuses practitioner attention on composing high-level,
human-understandable concepts that the model is expected to reason over (e.g.,
profanity, racism, or sarcasm in a content moderation task) using zero-shot
concept instantiation. In an evaluation with 17 ML practitioners, model
sketching reframed thinking from implementation to higher-level exploration,
prompted iteration on a broader range of model designs, and helped identify
gaps in the problem formulation$\unicode{x2014}$all in a fraction of the time
ordinarily required to build a model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CHI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Very fast, approximate counterfactual explanations for decision forests <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Á. Carreira-Perpiñán, Suryabhan Singh Hada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider finding a counterfactual explanation for a classification or
regression forest, such as a random forest. This requires solving an
optimization problem to find the closest input instance to a given instance for
which the forest outputs a desired value. Finding an exact solution has a cost
that is exponential on the number of leaves in the forest. We propose a simple
but very effective approach: we constrain the optimization to only those input
space regions defined by the forest that are populated by actual data points.
The problem reduces to a form of nearest-neighbor search using a certain
distance on a certain dataset. This has two advantages: first, the solution can
be found very quickly, scaling to large forests and high-dimensional data, and
enabling interactive use. Second, the solution found is more likely to be
realistic in that it is guided towards high-density areas of input space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version of this paper appears in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Capsule Neural Network for Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Qin, Yong Liang Guan, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through advancement of the Vehicle-to-Everything (V2X) network, road safety,
energy consumption, and traffic efficiency can be significantly improved. An
accurate vehicle trajectory prediction benefits communication traffic
management and network resource allocation for the real-time application of the
V2X network. Recurrent neural networks and their variants have been reported in
recent research to predict vehicle mobility. However, the spatial attribute of
vehicle movement behavior has been overlooked, resulting in incomplete
information utilization. To bridge this gap, we put forward for the first time
a hierarchical trajectory prediction structure using the capsule neural network
(CapsNet) with three sequential components. First, the geographic information
is transformed into a grid map presentation, describing vehicle mobility
distribution spatially and temporally. Second, CapsNet serves as the core model
to embed local temporal and global spatial correlation through hierarchical
capsules. Finally, extensive experiments conducted on actual taxi mobility data
collected in Porto city (Portugal) and Singapore show that the proposed method
outperforms the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TVT has accepted this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of Deep Learning-Powered Mesh Reconstruction Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advances in hardware and rendering techniques, 3D models have
emerged everywhere in our life. Yet creating 3D shapes is arduous and requires
significant professional knowledge. Meanwhile, Deep learning has enabled
high-quality 3D shape reconstruction from various sources, making it a viable
approach to acquiring 3D shapes with minimal effort. Importantly, to be used in
common 3D applications, the reconstructed shapes need to be represented as
polygonal meshes, which is a challenge for neural networks due to the
irregularity of mesh tessellations. In this survey, we provide a comprehensive
review of mesh reconstruction methods that are powered by machine learning. We
first describe various representations for 3D shapes in the deep learning
context. Then we review the development of 3D mesh reconstruction methods from
voxels, point clouds, single images, and multi-view images. Finally, we
identify several challenges in this field and propose potential future
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding metastable skyrmionic structures via a metaheuristic
  perturbation-driven neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qichen Xu, I. P. Miranda, Manuel Pereiro, Filipp N. Rybakov, Danny Thonig, Erik Sjöqvist, Pavel Bessarab, Anders Bergman, Olle Eriksson, Pawel Herman, Anna Delin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological magnetic textures observed in experiments can, in principle, be
predicted by theoretical calculations and numerical simulations. However, such
calculations are, in general, hampered by difficulties in distinguishing
between local and global energy minima. This becomes particularly problematic
for magnetic materials that allow for a multitude of topological charges.
Finding solutions to such problems by means of classical numerical methods can
be challenging because either a good initial guess or a gigantic amount of
random sampling is required. In this study, we demonstrate an efficient way to
identify those metastable configurations by leveraging the power of gradient
descent-based optimization within the framework of a feedforward neural network
combined with a heuristic meta-search, which is driven by a random perturbation
of the neural network's input. We exemplify the power of the method by an
analysis of the Pd/Fe/Ir(111) system, an experimentally well characterized
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DR-Label: Improving GNN Models for Catalysis Systems by Label
  Deconstruction and Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wang, Chen Liang, Jiaze Wang, Furui Liu, Shaogang Hao, Dong Li, Jianye Hao, Guangyong Chen, Xiaolong Zou, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attaining the equilibrium state of a catalyst-adsorbate system is key to
fundamentally assessing its effective properties, such as adsorption energy.
Machine learning methods with finer supervision strategies have been applied to
boost and guide the relaxation process of an atomic system and better predict
its properties at the equilibrium state. In this paper, we present a novel
graph neural network (GNN) supervision and prediction strategy DR-Label. The
method enhances the supervision signal, reduces the multiplicity of solutions
in edge representation, and encourages the model to provide node predictions
that are graph structural variation robust. DR-Label first Deconstructs
finer-grained equilibrium state information to the model by projecting the
node-level supervision signal to each edge. Reversely, the model Reconstructs a
more robust equilibrium state prediction by transforming edge-level predictions
to node-level with a sphere-fitting algorithm. The DR-Label strategy was
applied to three radically distinct models, each of which displayed consistent
performance enhancements. Based on the DR-Label strategy, we further proposed
DRFormer, which achieved a new state-of-the-art performance on the Open
Catalyst 2020 (OC20) dataset and the Cu-based single-atom-alloyed CO adsorption
(SAA) dataset. We expect that our work will highlight crucial steps for the
development of a more accurate model in equilibrium state property prediction
of a catalysis system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Sampling for Fairness Testing in Deep Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tosin Ige, William Marfo, Justin Tonkinson, Sikiru Adewale, Bolanle Hafiz Matti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we focus on the usage of adversarial sampling to test for
the fairness in the prediction of deep neural network model across different
classes of image in a given dataset. While several framework had been proposed
to ensure robustness of machine learning model against adversarial attack, some
of which includes adversarial training algorithm. There is still the pitfall
that adversarial training algorithm tends to cause disparity in accuracy and
robustness among different group. Our research is aimed at using adversarial
sampling to test for fairness in the prediction of deep neural network model
across different classes or categories of image in a given dataset. We
successfully demonstrated a new method of ensuring fairness across various
group of input in deep neural network classifier. We trained our neural network
model on the original image, and without training our model on the perturbed or
attacked image. When we feed the adversarial samplings to our model, it was
able to predict the original category/ class of the image the adversarial
sample belongs to. We also introduced and used the separation of concern
concept from software engineering whereby there is an additional standalone
filter layer that filters perturbed image by heavily removing the noise or
attack before automatically passing it to the network for classification, we
were able to have accuracy of 93.3%. Cifar-10 dataset have ten categories of
dataset, and so, in order to account for fairness, we applied our hypothesis
across each categories of dataset and were able to get a consistent result and
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, International Journal of Advanced Computer
  Science and Application</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Angel-PTM: A Scalable and Economical Large-scale <span class="highlight-title">Pre-train</span>ing System in
  Tencent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Nie, Yi Liu, Fangcheng Fu, Jinbao Xue, Dian Jiao, Xupeng Miao, Yangyu Tao, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the unprecedented achievements of large-scale
pre-trained models, especially the Transformer models. Many products and
services in Tencent Inc., such as WeChat, QQ, and Tencent Advertisement, have
been opted in to gain the power of pre-trained models. In this work, we present
Angel-PTM, a productive deep learning system designed for pre-training and
fine-tuning Transformer models. Angel-PTM can train extremely large-scale
models with hierarchical memory efficiently. The key designs of Angel-PTM are
the fine-grained memory management via the Page abstraction and a unified
scheduling method that coordinate the computations, data movements, and
communications. Furthermore, Angel-PTM supports extreme model scaling with SSD
storage and implements the lock-free updating mechanism to address the SSD I/O
bandwidth bottlenecks. Experimental results demonstrate that Angel-PTM
outperforms existing systems by up to 114.8% in terms of maximum model scale as
well as up to 88.9% in terms of training throughput. Additionally, experiments
on GPT3-175B and T5-MoE-1.2T models utilizing hundreds of GPUs verify the
strong scalability of Angel-PTM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Grained Self-Interpretable Symbolic-Neural Model For
  Single/Multi-Labeled Text Classification <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Xinyu Kong, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks based on layer-stacking architectures have historically
suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic
models function with clear interpretability, but how to combine them with
neural networks to enhance their performance remains to be explored. In this
paper, we try to marry these two systems for text classification via a
structured language model. We propose a Symbolic-Neural model that can learn to
explicitly predict class labels of text spans from a constituency tree without
requiring any access to span-level gold labels. As the structured language
model learns to predict constituency trees in a self-supervised manner, only
raw texts and sentence-level labels are required as training data, which makes
it essentially a general constituent-level self-interpretable classification
model. Our experiments demonstrate that our approach could achieve good
prediction accuracy in downstream tasks. Meanwhile, the predicted span labels
are consistent with human rationales to a certain degree.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian inference with finitely wide neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Ken Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analytic inference, e.g. predictive distribution being in closed form,
may be an appealing benefit for machine learning practitioners when they treat
wide neural networks as Gaussian process in Bayesian setting. The realistic
widths, however, are finite and cause weak deviation from the Gaussianity under
which partial marginalization of random variables in a model is
straightforward. On the basis of multivariate Edgeworth expansion, we propose a
non-Gaussian distribution in differential form to model a finite set of outputs
from a random neural network, and derive the corresponding marginal and
conditional properties. Thus, we are able to derive the non-Gaussian posterior
distribution in Bayesian regression task. In addition, in the bottlenecked deep
neural networks, a weight space representation of deep Gaussian process, the
non-Gaussianity is investigated through the marginal kernel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Knowledge</span>-embedded meta-learning model for lift coefficient prediction
  of airfoils 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hairun Xie, Jing Wang, Miao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerodynamic performance evaluation is an important part of the aircraft
aerodynamic design optimization process; however, traditional methods are
costly and time-consuming. Despite the fact that various machine learning
methods can achieve high accuracy, their application in engineering is still
difficult due to their poor generalization performance and "black box" nature.
In this paper, a knowledge-embedded meta learning model, which fully integrates
data with the theoretical knowledge of the lift curve, is developed to obtain
the lift coefficients of an arbitrary supercritical airfoil under various angle
of attacks. In the proposed model, a primary network is responsible for
representing the relationship between the lift and angle of attack, while the
geometry information is encoded into a hyper network to predict the unknown
parameters involved in the primary network. Specifically, three models with
different architectures are trained to provide various interpretations.
Compared to the ordinary neural network, our proposed model can exhibit better
generalization capability with competitive prediction accuracy. Afterward,
interpretable analysis is performed based on the Integrated Gradients and
Saliency methods. Results show that the proposed model can tend to assess the
influence of airfoil geometry to the physical characteristics. Furthermore, the
exceptions and shortcomings caused by the proposed model are analysed and
discussed in detail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in
  Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bixing Yan, Shaoling Chen, Yuxuan He, Zhihan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language understanding(NLU) is challenging for finance due to the
lack of annotated data and the specialized language in that domain. As a
result, researchers have proposed to use pre-trained language model and
multi-task learning to learn robust representations. However, aggressive
fine-tuning often causes over-fitting and multi-task learning may favor tasks
with significantly larger amounts data, etc. To address these problems, in this
paper, we investigate model-agnostic meta-learning algorithm(MAML) in
low-resource financial NLU tasks. Our contribution includes: 1. we explore the
performance of MAML method with multiple types of tasks: GLUE datasets, SNLI,
Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML method
with multiple single-type tasks: a real scenario stock price prediction problem
with twitter text data. Our models achieve the state-of-the-art performance
according to the experimental results, which demonstrate that our method can
adapt fast and well to low-resource situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eCDANs: Efficient Temporal Causal Discovery from Autocorrelated and
  Non-stationary Data (Student Abstract) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hasan Ferdous, Uzma Hasan, Md Osman Gani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional temporal causal discovery (CD) methods suffer from high
dimensionality, fail to identify lagged causal relationships, and often ignore
dynamics in relations. In this study, we present a novel constraint-based CD
approach for autocorrelated and non-stationary time series data (eCDANs)
capable of detecting lagged and contemporaneous causal relationships along with
temporal changes. eCDANs addresses high dimensionality by optimizing the
conditioning sets while conducting conditional independence (CI) tests and
identifies the changes in causal relations by introducing a surrogate variable
to represent time dependency. Experiments on synthetic and real-world data show
that eCDANs can identify time influence and outperform the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribution-Scores and Causal Counterfactuals as Explanations in
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leopoldo Bertossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this expository article we highlight the relevance of explanations for
artificial intelligence, in general, and for the newer developments in {\em
explainable AI}, referring to origins and connections of and among different
approaches. We describe in simple terms, explanations in data management and
machine learning that are based on attribution-scores, and counterfactuals as
found in the area of causality. We elaborate on the importance of logical
reasoning when dealing with counterfactuals, and their use for score
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted as chapter contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Autoencoders for Collective Corruption Removal <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taihui Li, Hengkang Wang, Peng Le, XianE Tang, Ju Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust PCA is a standard tool for learning a linear subspace in the presence
of sparse corruption or rare outliers. What about robustly learning manifolds
that are more realistic models for natural data, such as images? There have
been several recent attempts to generalize robust PCA to manifold settings. In
this paper, we propose $\ell_1$- and scaling-invariant $\ell_1/\ell_2$-robust
autoencoders based on a surprisingly compact formulation built on the intuition
that deep autoencoders perform manifold learning. We demonstrate on several
standard image datasets that the proposed formulation significantly outperforms
all previous methods in collectively removing sparse corruption, without clean
images for training. Moreover, we also show that the learned manifold
structures can be generalized to unseen data samples effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Analytics of Neuron Vulnerability to Adversarial Attacks on
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Li, Junpeng Wang, Takanori Fujiwara, Kwan-Liu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks on a convolutional neural network (CNN) -- injecting
human-imperceptible perturbations into an input image -- could fool a
high-performance CNN into making incorrect predictions. The success of
adversarial attacks raises serious concerns about the robustness of CNNs, and
prevents them from being used in safety-critical applications, such as medical
diagnosis and autonomous driving. Our work introduces a visual analytics
approach to understanding adversarial attacks by answering two questions: (1)
which neurons are more vulnerable to attacks and (2) which image features do
these vulnerable neurons capture during the prediction? For the first question,
we introduce multiple perturbation-based measures to break down the attacking
magnitude into individual CNN neurons and rank the neurons by their
vulnerability levels. For the second, we identify image features (e.g., cat
ears) that highly stimulate a user-selected neuron to augment and validate the
neuron's responsibility. Furthermore, we support an interactive exploration of
a large number of neurons by aiding with hierarchical clustering based on the
neurons' roles in the prediction. To this end, a visual analytics system is
designed to incorporate visual reasoning for interpreting adversarial attacks.
We validate the effectiveness of our system through multiple case studies as
well as feedback from domain experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Special Issue on Human-Centered Explainable AI, ACM
  Transactions on Interactive Intelligent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Dominant Periodicity Detection for Time Series with Missing Data <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingsong Wen, Linxiao Yang, Liang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Periodicity detection is an important task in time series analysis, but still
a challenging problem due to the diverse characteristics of time series data
like abrupt trend change, outlier, noise, and especially block missing data. In
this paper, we propose a robust and effective periodicity detection algorithm
for time series with block missing data. We first design a robust trend filter
to remove the interference of complicated trend patterns under missing data.
Then, we propose a robust autocorrelation function (ACF) that can handle
missing values and outliers effectively. We rigorously prove that the proposed
robust ACF can still work well when the length of the missing block is less
than $1/3$ of the period length. Last, by combining the time-frequency
information, our algorithm can generate the period length accurately. The
experimental results demonstrate that our algorithm outperforms existing
periodicity detection algorithms on real-world time series datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Zero-Shot Human Models for Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expressivity of Shallow and Deep Neural Networks for Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Shapira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the number of neurons that a ReLU neural network needs to
approximate multivariate monomials. We establish an exponential lower bound for
the complexity of any shallow network that approximates the product function
$\vec{x} \to \prod_{i=1}^d x_i$ on a general compact domain. Furthermore, we
prove that this lower bound does not hold for normalized O(1)-Lipschitz
monomials (or equivalently, by restricting to the unit cube). These results
suggest shallow ReLU networks suffer from the curse of dimensionality when
expressing functions with a Lipschitz parameter scaling with the dimension of
the input, and that the expressive power of neural networks lies in their depth
rather than the overall complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Equivariant Diffusion for Target-Aware Molecule <span class="highlight-title">Generation</span> and
  Affinity Prediction <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, Jianzhu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rich data and powerful machine learning models allow us to design drugs for a
specific protein target \textit{in silico}. Recently, the inclusion of 3D
structures during targeted drug design shows superior performance to other
target-free models as the atomic interaction in the 3D space is explicitly
modeled. However, current 3D target-aware models either rely on the voxelized
atom densities or the autoregressive sampling process, which are not
equivariant to rotation or easily violate geometric constraints resulting in
unrealistic structures. In this work, we develop a 3D equivariant diffusion
model to solve the above challenges. To achieve target-aware molecule design,
our method learns a joint generative process of both continuous atom
coordinates and categorical atom types with a SE(3)-equivariant network.
Moreover, we show that our model can serve as an unsupervised feature extractor
to estimate the binding affinity under proper parameterization, which provides
an effective way for drug screening. To evaluate our model, we propose a
comprehensive framework to evaluate the quality of sampled molecules from
different dimensions. Empirical studies show our model could generate molecules
with more realistic 3D structures and better affinities towards the protein
targets, and improve binding affinity ranking and prediction without
retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Need for Objective Task-based Evaluation of Deep Learning-Based
  Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Yu, Md Ashequr Rahman, Richard Laforest, Thomas H. Schindler, Robert J. Gropler, Richard L. Wahl, Barry A. Siegel, Abhinav K. Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence-based methods have generated substantial interest in
nuclear medicine. An area of significant interest has been using deep-learning
(DL)-based approaches for denoising images acquired with lower doses, shorter
acquisition times, or both. Objective evaluation of these approaches is
essential for clinical application. DL-based approaches for denoising
nuclear-medicine images have typically been evaluated using fidelity-based
figures of merit (FoMs) such as RMSE and SSIM. However, these images are
acquired for clinical tasks and thus should be evaluated based on their
performance in these tasks. Our objectives were to (1) investigate whether
evaluation with these FoMs is consistent with objective clinical-task-based
evaluation; (2) provide a theoretical analysis for determining the impact of
denoising on signal-detection tasks; (3) demonstrate the utility of virtual
clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a
DL-based method for denoising myocardial perfusion SPECT (MPS) images was
conducted. The impact of DL-based denoising was evaluated using fidelity-based
FoMs and AUC, which quantified performance on detecting perfusion defects in
MPS images as obtained using a model observer with anthropomorphic channels.
Based on fidelity-based FoMs, denoising using the considered DL-based method
led to significantly superior performance. However, based on ROC analysis,
denoising did not improve, and in fact, often degraded detection-task
performance. The results motivate the need for objective task-based evaluation
of DL-based denoising approaches. Further, this study shows how VCTs provide a
mechanism to conduct such evaluations using VCTs. Finally, our theoretical
treatment reveals insights into the reasons for the limited performance of the
denoising approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic Data Generator for Adaptive Interventions in Global Health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Rastogi, Juan Francisco Garamendi, Ana Fernández del Río, Anna Guitart, Moiz Hassan Khan, Dexian Tang, África Periáñez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence and digital health have the potential to transform
global health. However, having access to representative data to test and
validate algorithms in realistic production environments is essential. We
introduce HealthSyn, an open-source synthetic data generator of user behavior
for testing reinforcement learning algorithms in the context of mobile health
interventions. The generator utilizes Markov processes to generate diverse user
actions, with individual user behavioral patterns that can change in reaction
to personalized interventions (i.e., reminders, recommendations, and
incentives). These actions are translated into actual logs using an ML-purposed
data schema specific to the mobile health application functionality included
with HealthKit, and open-source SDK. The logs can be fed to pipelines to obtain
user metrics. The generated data, which is based on real-world behaviors and
simulation techniques, can be used to develop, test, and evaluate, both ML
algorithms in research and end-to-end operational RL-based intervention
delivery frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Explanation Disparities as a Fairness Diagnostic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter W. Chang, Leor Fishman, Seth Neel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a flurry of research focusing on the fairness
of machine learning models, and in particular on quantifying and eliminating
bias against protected subgroups. One line of work generalizes the notion of
protected subgroups beyond simple discrete classes by introducing the notion of
a "rich subgroup", and seeks to train models that are calibrated or equalize
error rates with respect to these richer subgroup classes. Largely
orthogonally, local model explanation methods have been developed that given a
classifier h and test point x, attribute influence for the prediction h(x) to
the individual features of x. This raises a natural question: Do local model
explanation methods attribute different feature importance values on average
across different protected subgroups, and can we detect these disparities
efficiently? If the model places high weight on a given feature in a specific
protected subgroup, but not on the dataset overall (or vice versa), this could
be a potential indicator of bias in the predictive model or the underlying data
generating process, and is at the very least a useful diagnostic that signals
the need for a domain expert to delve deeper. In this paper, we formally
introduce the notion of feature importance disparity (FID) in the context of
rich subgroups, design oracle-efficent algorithms to identify large FID
subgroups, and conduct a thorough empirical analysis that establishes auditing
for FID as an important method to investigate dataset bias. Our experiments
show that across 4 datasets and 4 common feature importance methods our
algorithms find (feature, subgroup) pairs that simultaneously: (i) have
subgroup feature importance that is often an order of magnitude different than
the importance on the dataset as a whole (ii) generalize out of sample, and
(iii) yield interesting discussions about potential bias inherent in these
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures. Appendix: 8 pages, 4 figures. Replacement info:
  minor changes to match metadata abstract to paper abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Transfer Learning and State Inference for Soft Robots via a
  Semi-supervised Sequential Variational Bayes Framework <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shageenderan Sapai, Junn Yong Loo, Ze Yang Ding, Chee Pin Tan, Raphael CW Phan, Vishnu Monn Baskaran, Surya Girinatha Nurzaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, data-driven models such as deep neural networks have shown to be
promising tools for modelling and state inference in soft robots. However,
voluminous amounts of data are necessary for deep models to perform
effectively, which requires exhaustive and quality data collection,
particularly of state labels. Consequently, obtaining labelled state data for
soft robotic systems is challenged for various reasons, including difficulty in
the sensorization of soft robots and the inconvenience of collecting data in
unstructured environments. To address this challenge, in this paper, we propose
a semi-supervised sequential variational Bayes (DSVB) framework for transfer
learning and state inference in soft robots with missing state labels on
certain robot configurations. Considering that soft robots may exhibit distinct
dynamics under different robot configurations, a feature space transfer
strategy is also incorporated to promote the adaptation of latent features
across multiple configurations. Unlike existing transfer learning approaches,
our proposed DSVB employs a recurrent neural network to model the nonlinear
dynamics and temporal coherence in soft robot data. The proposed framework is
validated on multiple setup configurations of a pneumatic-based soft robot
finger. Experimental results on four transfer scenarios demonstrate that DSVB
performs effective transfer learning and accurate state inference amidst
missing state labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Conference on Robotics and Automation
  (ICRA) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Score-based Continuous-time Discrete Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, Hanjun Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based modeling through stochastic differential equations (SDEs) has
provided a new perspective on diffusion models, and demonstrated superior
performance on continuous data. However, the gradient of the log-likelihood
function, i.e., the score function, is not properly defined for discrete
spaces. This makes it non-trivial to adapt \textcolor{\cdiff}{the score-based
modeling} to categorical data. In this paper, we extend diffusion models to
discrete variables by introducing a stochastic jump process where the reverse
process denoises via a continuous-time Markov chain. This formulation admits an
analytical simulation during backward sampling. To learn the reverse process,
we extend score matching to general categorical data and show that an unbiased
estimator can be obtained via simple matching of the conditional marginal
distributions. We demonstrate the effectiveness of the proposed method on a set
of synthetic and real-world music and image benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Domain Coverage for Vehicles with Second-Order Dynamics via
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05952v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05952v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhao, Razvan C. Fetecau, Mo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative autonomous multi-agent systems covering a specified area have
many potential applications, such as UAV search and rescue, forest fire
fighting, and real-time high-resolution monitoring. Traditional approaches for
such coverage problems involve designing a model-based control policy based on
sensor data. However, designing model-based controllers is challenging, and the
state-of-the-art classical control policy still exhibits a large degree of
sub-optimality. In this paper, we present a reinforcement learning (RL)
approach for the multi-agent efficient domain coverage problem involving agents
with second-order dynamics. Our approach is based on the Multi-Agent Proximal
Policy Optimization Algorithm (MAPPO). Our proposed network architecture
includes the incorporation of LSTM and self-attention, which allows the trained
policy to adapt to a variable number of agents. Our trained policy
significantly outperforms the state-of-the-art classical control policy. We
demonstrate our proposed method in a variety of simulated experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version is submitted to IEEE / RSJ International Conference on
  Intelligent Robots and Systems, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span> Meets Boundary Value Inverse Problems <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14977v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14977v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruchi Guo, Shuhao Cao, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Transformer-based deep direct sampling method is proposed for electrical
impedance tomography, a well-known severely ill-posed nonlinear boundary value
inverse problem. A real-time reconstruction is achieved by evaluating the
learned inverse operator between carefully designed data and the reconstructed
images. An effort is made to give a specific example to a fundamental question:
whether and how one can benefit from the theoretical structure of a
mathematical problem to develop task-oriented and structure-conforming deep
neural networks? Specifically, inspired by direct sampling methods for inverse
problems, the 1D boundary data in different frequencies are preprocessed by a
partial differential equation-based feature map to yield 2D harmonic extensions
as different input channels. Then, by introducing learnable non-local kernels,
the direct sampling is recast to a modified attention mechanism. The new method
achieves superior accuracy over its predecessors and contemporary operator
learners and shows robustness to noises in benchmarks. This research shall
strengthen the insights that, despite being invented for natural language
processing tasks, the attention mechanism offers great flexibility to be
modified in conformity with the a priori mathematical knowledge, which
ultimately leads to the design of more physics-compatible neural architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures. Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All are Worth Words: A ViT Backbone for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12152v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12152v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViT) have shown promise in various vision tasks while
the U-Net based on a convolutional neural network (CNN) remains dominant in
diffusion models. We design a simple and general ViT-based architecture (named
U-ViT) for image generation with diffusion models. U-ViT is characterized by
treating all inputs including the time, condition and noisy image patches as
tokens and employing long skip connections between shallow and deep layers. We
evaluate U-ViT in unconditional and class-conditional image generation, as well
as text-to-image generation tasks, where U-ViT is comparable if not superior to
a CNN-based U-Net of a similar size. In particular, latent diffusion models
with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional
image generation on ImageNet 256x256, and 5.48 in text-to-image generation on
MS-COCO, among methods without accessing large external datasets during the
training of generative models. Our results suggest that, for diffusion-based
image modeling, the long skip connection is crucial while the down-sampling and
up-sampling operators in CNN-based U-Net are not always necessary. We believe
that U-ViT can provide insights for future research on backbones in diffusion
models and benefit generative modeling on large scale cross-modality datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating a Terrain-Robustness Benchmark for Legged Locomotion: A
  Prototype via Terrain Authoring and Active Learning <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Zhang, Lizhi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrain-aware locomotion has become an emerging topic in legged robotics.
However, it is hard to generate diverse, challenging, and realistic
unstructured terrains in simulation, which limits the way researchers evaluate
their locomotion policies. In this paper, we prototype the generation of a
terrain dataset via terrain authoring and active learning, and the learned
samplers can stably generate diverse high-quality terrains. We expect the
generated dataset to make a terrain-robustness benchmark for legged locomotion.
The dataset, the code implementation, and some policy evaluations are released
at https://bit.ly/3bn4j7f.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures. IEEE ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Multivariate Time-Series Forecasting: Adversarial Attacks and
  Defense Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linbo Liu, Youngsuk Park, Trong Nghia Hoang, Hilaf Hasson, Jun Huan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the threats of adversarial attack on multivariate
probabilistic forecasting models and viable defense mechanisms. Our studies
discover a new attack pattern that negatively impact the forecasting of a
target time series via making strategic, sparse (imperceptible) modifications
to the past observations of a small number of other time series. To mitigate
the impact of such attack, we have developed two defense strategies. First, we
extend a previously developed randomized smoothing technique in classification
to multivariate forecasting scenarios. Second, we develop an adversarial
training algorithm that learns to create adversarial examples and at the same
time optimizes the forecasting model to improve its robustness against such
adversarial simulation. Extensive experiments on real-world datasets confirm
that our attack schemes are powerful and our defense algorithms are more
effective compared with baseline defense mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise2Music: Text-conditioned Music <span class="highlight-title">Generation</span> with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan, Zhifeng Chen, Wei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Noise2Music, where a series of diffusion models is trained to
generate high-quality 30-second music clips from text prompts. Two types of
diffusion models, a generator model, which generates an intermediate
representation conditioned on text, and a cascader model, which generates
high-fidelity audio conditioned on the intermediate representation and possibly
the text, are trained and utilized in succession to generate high-fidelity
music. We explore two options for the intermediate representation, one using a
spectrogram and the other using audio with lower fidelity. We find that the
generated audio is not only able to faithfully reflect key elements of the text
prompt such as genre, tempo, instruments, mood, and era, but goes beyond to
ground fine-grained semantics of the prompt. Pretrained large language models
play a key role in this story -- they are used to generate paired text for the
audio of the training set and to extract embeddings of the text prompts
ingested by the diffusion models.
  Generated examples: https://google-research.github.io/noise2music
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Falsification before Extrapolation in Causal Effect Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshan Hussain, Michael Oberst, Ming-Chieh Shih, David Sontag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized Controlled Trials (RCTs) represent a gold standard when developing
policy guidelines. However, RCTs are often narrow, and lack data on broader
populations of interest. Causal effects in these populations are often
estimated using observational datasets, which may suffer from unobserved
confounding and selection bias. Given a set of observational estimates (e.g.
from multiple studies), we propose a meta-algorithm that attempts to reject
observational estimates that are biased. We do so using validation effects,
causal effects that can be inferred from both RCT and observational data. After
rejecting estimators that do not pass this test, we generate conservative
confidence intervals on the extrapolated causal effects for subgroups not
observed in the RCT. Under the assumption that at least one observational
estimator is asymptotically normal and consistent for both the validation and
extrapolated effects, we provide guarantees on the coverage probability of the
intervals output by our algorithm. To facilitate hypothesis testing in settings
where causal effect transportation across datasets is necessary, we give
conditions under which a doubly-robust estimator of group average treatment
effects is asymptotically normal, even when flexible machine learning methods
are used for estimation of nuisance parameters. We illustrate the properties of
our approach on semi-synthetic and real world datasets, and show that it
compares favorably to standard meta-analysis techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Neural Information Processing Systems, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Certified Robustness Against Real-World Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoze Wu, Teruhiro Tagomori, Alexander Robey, Fengjun Yang, Nikolai Matni, George Pappas, Hamed Hassani, Corina Pasareanu, Clark Barrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of certifying the robustness of deep neural networks
against real-world distribution shifts. To do so, we bridge the gap between
hand-crafted specifications and realistic deployment settings by proposing a
novel neural-symbolic verification framework, in which we train a generative
model to learn perturbations from data and define specifications with respect
to the output of the learned model. A unique challenge arising from this
setting is that existing verifiers cannot tightly approximate sigmoid
activations, which are fundamental to many state-of-the-art generative models.
To address this challenge, we propose a general meta-algorithm for handling
sigmoid activations which leverages classical notions of counter-example-guided
abstraction refinement. The key idea is to "lazily" refine the abstraction of
sigmoid functions to exclude spurious counter-examples found in the previous
abstraction, thus guaranteeing progress in the verification process while
keeping the state-space small. Experiments on the MNIST and CIFAR-10 datasets
show that our framework significantly outperforms existing methods on a range
of challenging distribution shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SatML'23. Keywords: certified robustness, distribution shift,
  generative models, S-shaped activations, CEGAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RRWaveNet: A Compact End-to-End Multi-Scale Residual CNN for Robust PPG
  Respiratory Rate Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pongpanut Osathitporn, Guntitat Sawadwuthikul, Punnawish Thuwajit, Kawisara Ueafuea, Thee Mateepithaktham, Narin Kunaseth, Tanut Choksatchawathi, Proadpran Punyabukkana, Emmanuel Mignot, Theerawit Wilaiprasitporn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory rate (RR) is an important biomarker as RR changes can reflect
severe medical events such as heart disease, lung disease, and sleep disorders.
Unfortunately, standard manual RR counting is prone to human error and cannot
be performed continuously. This study proposes a method for continuously
estimating RR, RRWaveNet. The method is a compact end-to-end deep learning
model which does not require feature engineering and can use low-cost raw
photoplethysmography (PPG) as input signal. RRWaveNet was tested
subject-independently and compared to baseline in four datasets (BIDMC,
CapnoBase, WESAD, and SensAI) and using three window sizes (16, 32, and 64
seconds). RRWaveNet outperformed current state-of-the-art methods with mean
absolute errors at optimal window size of 1.66 \pm 1.01, 1.59 \pm 1.08, 1.92
\pm 0.96 and 1.23 \pm 0.61 breaths per minute for each dataset. In remote
monitoring settings, such as in the WESAD and SensAI datasets, we apply
transfer learning to improve the performance using two other ICU datasets as
pretraining datasets, reducing the MAE by up to 21$\%$. This shows that this
model allows accurate and practical estimation of RR on affordable and wearable
devices. Our study also shows feasibility of remote RR monitoring in the
context of telemedicine and at home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Falsification of Internal and External Validity in Observational Studies
  via Conditional Moment Restrictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshan Hussain, Ming-Chieh Shih, Michael Oberst, Ilker Demirel, David Sontag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized Controlled Trials (RCT)s are relied upon to assess new treatments,
but suffer from limited power to guide personalized treatment decisions. On the
other hand, observational (i.e., non-experimental) studies have large and
diverse populations, but are prone to various biases (e.g. residual
confounding). To safely leverage the strengths of observational studies, we
focus on the problem of falsification, whereby RCTs are used to validate causal
effect estimates learned from observational data. In particular, we show that,
given data from both an RCT and an observational study, assumptions on internal
and external validity have an observable, testable implication in the form of a
set of Conditional Moment Restrictions (CMRs). Further, we show that expressing
these CMRs with respect to the causal effect, or "causal contrast", as opposed
to individual counterfactual means, provides a more reliable falsification
test. In addition to giving guarantees on the asymptotic properties of our
test, we demonstrate superior power and type I error of our approach on
semi-synthetic and real world datasets. Our approach is interpretable, allowing
a practitioner to visualize which subgroups in the population lead to
falsification of an observational study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Artificial Intelligence and Statistics 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TalkToModel: Explaining Machine Learning Models with Interactive Natural
  Language <span class="highlight-title">Conversation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04154v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04154v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, Sameer Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have become more complex, making them
harder to understand. To this end, researchers have proposed several techniques
to explain model predictions. However, practitioners struggle to use these
explainability techniques because they often do not know which one to choose
and how to interpret the results of the explanations. In this work, we address
these challenges by introducing TalkToModel: an interactive dialogue system for
explaining machine learning models through conversations. Specifically,
TalkToModel comprises of three key components: 1) a natural language interface
for engaging in conversations, making ML model explainability highly
accessible, 2) a dialogue engine that adapts to any tabular model and dataset,
interprets natural language, maps it to appropriate explanations, and generates
text responses, and 3) an execution component that constructs the explanations.
We carried out extensive quantitative and human subject evaluations of
TalkToModel. Overall, we found the conversational system understands user
inputs on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In real-world evaluations
with humans, 73% of healthcare workers (e.g., doctors and nurses) agreed they
would use TalkToModel over baseline point-and-click systems for explainability
in a disease prediction task, and 85% of ML professionals agreed TalkToModel
was easier to use for computing explanations. Our findings demonstrate that
TalkToModel is more effective for model explainability than existing systems,
introducing a new category of explainability tools for practitioners. Code &
demo released here: https://github.com/dylan-slack/TalkToModel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print; comments welcome! Reach out to dslack@uci.edu v3 update
  title and abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Semi-supervised Medical Image Segmentation with
  Anatomical-aware Contrastive Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown great promise over annotation scarcity
problems in the context of medical image segmentation. Existing approaches
typically assume a balanced class distribution for both labeled and unlabeled
medical images. However, medical image data in reality is commonly imbalanced
(i.e., multi-class label imbalance), which naturally yields blurry contours and
usually incorrectly labels rare objects. Moreover, it remains unclear whether
all negative samples are equally negative. In this work, we present ACTION, an
Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised
medical image segmentation. Specifically, we first develop an iterative
contrastive distillation algorithm by softly labeling the negatives rather than
binary supervision between positive and negative pairs. We also capture more
semantically similar features from the randomly chosen negative set compared to
the positives to enforce the diversity of the sampled data. Second, we raise a
more important question: Can we really handle imbalanced samples to yield
better performance? Hence, the key innovation in ACTION is to learn global
semantic relationship across the entire dataset and local anatomical features
among the neighbouring pixels with minimal additional memory footprint. During
the training, we introduce anatomical contrast by actively sampling a sparse
set of hard negative pixels, which can generate smoother segmentation
boundaries and more accurate predictions. Extensive experiments across two
benchmark datasets and different unlabeled settings show that ACTION
significantly outperforms the current state-of-the-art semi-supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Shapley Explanation via Contributive Cooperator Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanchu Wang, Yu-Neng Chuang, Mengnan Du, Fan Yang, Quan Zhou, Pushkar Tripathi, Xuanting Cai, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Source Survival Domain Adaptation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammar Shaker, Carolin Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survival analysis is the branch of statistics that studies the relation
between the characteristics of living entities and their respective survival
times, taking into account the partial information held by censored cases. A
good analysis can, for example, determine whether one medical treatment for a
group of patients is better than another. With the rise of machine learning,
survival analysis can be modeled as learning a function that maps studied
patients to their survival times. To succeed with that, there are three crucial
issues to be tackled. First, some patient data is censored: we do not know the
true survival times for all patients. Second, data is scarce, which led past
research to treat different illness types as domains in a multi-task setup.
Third, there is the need for adaptation to new or extremely rare illness types,
where little or no labels are available. In contrast to previous multi-task
setups, we want to investigate how to efficiently adapt to a new survival
target domain from multiple survival source domains. For this, we introduce a
new survival metric and the corresponding discrepancy measure between survival
distributions. These allow us to define domain adaptation for survival analysis
while incorporating censored data, which would otherwise have to be dropped.
Our experiments on two cancer data sets reveal a superb performance on target
domains, a better treatment recommendation, and a weight matrix with a
plausible explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37th AAAI Conference on Artificial Intelligence, 2023. Includes
  Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedExP: Speeding Up Federated Averaging via Extrapolation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Averaging (FedAvg) remains the most popular algorithm for Federated
Learning (FL) optimization due to its simple implementation, stateless nature,
and privacy guarantees combined with secure aggregation. Recent work has sought
to generalize the vanilla averaging in FedAvg to a generalized gradient descent
step by treating client updates as pseudo-gradients and using a server step
size. While the use of a server step size has been shown to provide performance
improvement theoretically, the practical benefit of the server step size has
not been seen in most existing works. In this work, we present FedExP, a method
to adaptively determine the server step size in FL based on dynamically varying
pseudo-gradients throughout the FL process. We begin by considering the
overparameterized convex regime, where we reveal an interesting similarity
between FedAvg and the Projection Onto Convex Sets (POCS) algorithm. We then
show how FedExP can be motivated as a novel extension to the extrapolation
mechanism that is used to speed up POCS. Our theoretical analysis later also
discusses the implications of FedExP in underparameterized and non-convex
settings. Experimental results show that FedExP consistently converges faster
than FedAvg and competing baselines on a range of realistic FL datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023. V2 fixes minor typos and cleans up proofs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automata Cascades: Expressivity and Sample Complexity <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Ronca, Nadezda Alexandrovna Knorozova, Giuseppe De Giacomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Every automaton can be decomposed into a cascade of basic prime automata.
This is the Prime Decomposition Theorem by Krohn and Rhodes. Guided by this
theory, we propose automata cascades as a structured, modular, way to describe
automata as complex systems made of many components, each implementing a
specific functionality. Any automaton can serve as a component; using specific
components allows for a fine-grained control of the expressivity of the
resulting class of automata; using prime automata as components implies
specific expressivity guarantees. Moreover, specifying automata as cascades
allows for describing the sample complexity of automata in terms of their
components. We show that the sample complexity is linear in the number of
components and the maximum complexity of a single component, modulo logarithmic
factors. This opens to the possibility of learning automata representing large
dynamical systems consisting of many parts interacting with each other. It is
in sharp contrast with the established understanding of the sample complexity
of automata, described in terms of the overall number of states and input
letters, which implies that it is only possible to learn automata where the
number of states is linear in the amount of data available. Instead our results
show that one can learn automata with a number of states that is exponential in
the amount of data available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version with appendix of a paper with the same title that
  appears in the proceedings of AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Interpretability Methods and Perturbation Artifacts in
  Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Brocki, Neo Christopher Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite excellent performance of deep neural networks (DNNs) in image
classification, detection, and prediction, characterizing how DNNs make a given
decision remains an open problem, resulting in a number of interpretability
methods. Post-hoc interpretability methods primarily aim to quantify the
importance of input features with respect to the class probabilities. However,
due to the lack of ground truth and the existence of interpretability methods
with diverse operating characteristics, evaluating these methods is a crucial
challenge. A popular approach to evaluate interpretability methods is to
perturb input features deemed important for a given prediction and observe the
decrease in accuracy. However, perturbation itself may introduce artifacts,
since perturbed images may be out-of-distribution (OOD). In this paper, we have
conducted computational experiments to estimate the contribution of
perturbation artifacts and developed a method to estimate the fidelity of
interpretability methods. We demonstrate that, while perturbation artifacts
indeed exist, we can minimize and characterize their impact on fidelity
estimation by utilizing model accuracy curves from perturbing input features
according to the Most Import First (MIF) and Least Import First (LIF) orders.
Using the ResNet-50 trained on the ImageNet, we demonstrate the proposed
fidelity estimation of four popular post-hoc interpretability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Exemplars for In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained language models (LMs) have shown impressive In-Context
Learning (ICL) ability, where the model learns to do an unseen task via a
prompt consisting of input-output examples as the demonstration, without any
parameter updates. The performance of ICL is highly dominated by the quality of
the selected in-context examples. However, previous selection methods are
mostly based on simple heuristics, leading to sub-optimal performance. In this
work, we formulate in-context example selection as a subset selection problem.
We propose CEIL (Compositional Exemplars for In-context Learning), which is
instantiated by Determinantal Point Processes (DPPs) to model the interaction
between the given input and in-context examples, and optimized through a
carefully-designed contrastive learning objective to obtain preference from
LMs. We validate CEIL on 12 classification and generation datasets from 7
distinct NLP tasks, including sentiment analysis, paraphrase detection, natural
language inference, commonsense reasoning, open-domain question answering, code
generation, and semantic parsing. Extensive experiments demonstrate not only
the state-of-the-art performance but also the transferability and
compositionality of CEIL, shedding new light on effective and efficient
in-context learning. Our code is released at
https://github.com/HKUNLP/icl-ceil.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lifting the Information Ratio: An Information-Theoretic Analysis of
  Thompson Sampling for Contextual Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gergely Neu, Julia Olkhovskaya, Matteo Papini, Ludovic Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Bayesian regret of the renowned Thompson Sampling algorithm in
contextual bandits with binary losses and adversarially-selected contexts. We
adapt the information-theoretic perspective of \cite{RvR16} to the contextual
setting by considering a lifted version of the information ratio defined in
terms of the unknown model parameter instead of the optimal action or optimal
policy as done in previous works on the same setting. This allows us to bound
the regret in terms of the entropy of the prior distribution through a
remarkably simple proof, and with no structural assumptions on the likelihood
or the prior. The extension to priors with infinite entropy only requires a
Lipschitz assumption on the log-likelihood. An interesting special case is that
of logistic bandits with $d$-dimensional parameters, $K$ actions, and Lipschitz
logits, for which we provide a $\widetilde{O}(\sqrt{dKT})$ regret upper-bound
that does not depend on the smallest slope of the sigmoid link function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-free Contextual Dynamic Pricing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.07340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.07340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyun Luo, Will Wei Sun, and Yufeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual dynamic pricing aims to set personalized prices based on
sequential interactions with customers. At each time period, a customer who is
interested in purchasing a product comes to the platform. The customer's
valuation for the product is a linear function of contexts, including product
and customer features, plus some random market noise. The seller does not
observe the customer's true valuation, but instead needs to learn the valuation
by leveraging contextual information and historical binary purchase feedbacks.
Existing models typically assume full or partial knowledge of the random noise
distribution. In this paper, we consider contextual dynamic pricing with
unknown random noise in the valuation model. Our distribution-free pricing
policy learns both the contextual function and the market noise simultaneously.
A key ingredient of our method is a novel perturbed linear bandit framework,
where a modified linear upper confidence bound algorithm is proposed to balance
the exploration of market noise and the exploitation of the current knowledge
for better pricing. We establish the regret upper bound and a matching lower
bound of our policy in the perturbed linear bandit framework and prove a
sub-linear regret bound in the considered pricing problem. Finally, we
demonstrate the superior performance of our policy on simulations and a
real-life auto-loan dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Mathematics of Operations Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TPC: Transformation-Specific Smoothing for Point Cloud Models <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenda Chu, Linyi Li, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud models with neural network architectures have achieved great
success and have been widely used in safety-critical applications, such as
Lidar-based recognition systems in autonomous vehicles. However, such models
are shown vulnerable to adversarial attacks which aim to apply stealthy
semantic transformations such as rotation and tapering to mislead model
predictions. In this paper, we propose a transformation-specific smoothing
framework TPC, which provides tight and scalable robustness guarantees for
point cloud models against semantic transformation attacks. We first categorize
common 3D transformations into three categories: additive (e.g., shearing),
composable (e.g., rotation), and indirectly composable (e.g., tapering), and we
present generic robustness certification strategies for all categories
respectively. We then specify unique certification protocols for a range of
specific semantic transformations and their compositions. Extensive experiments
on several common 3D transformations show that TPC significantly outperforms
the state of the art. For example, our framework boosts the certified accuracy
against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$
to 83.8$\%$. Codes and models are available at
https://github.com/Qianhewu/Point-Cloud-Smoothing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Which Factors are associated with Open Access Publishing? A Springer
  Nature Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fakhri Momeni, Stefan Dietze, Philipp Mayr, Kristin Biesenbender, Isabella Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Access (OA) facilitates access to articles. But, authors or funders
often must pay the publishing costs preventing authors who do not receive
financial support from participating in OA publishing and citation advantage
for OA articles. OA may exacerbate existing inequalities in the publication
system rather than overcome them. To investigate this, we studied 522,411
articles published by Springer Nature. Employing correlation and regression
analyses, we describe the relationship between authors affiliated with
countries from different income levels, their choice of publishing model, and
the citation impact of their papers. A machine learning classification method
helped us to explore the importance of different features in predicting the
publishing model. The results show that authors eligible for APC waivers
publish more in gold-OA journals than others. In contrast, authors eligible for
an APC discount have the lowest ratio of OA publications, leading to the
assumption that this discount insufficiently motivates authors to publish in
gold-OA journals. We found a strong correlation between the journal rank and
the publishing model in gold-OA journals, whereas the OA option is mostly
avoided in hybrid journals. Also, results show that the countries' income
level, seniority, and experience with OA publications are the most predictive
factors for OA publishing in hybrid journals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is it enough to optimize CNN architectures on ImageNet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.09108v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.09108v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Tuggener, Jürgen Schmidhuber, Thilo Stadelmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification performance based on ImageNet is the de-facto standard metric
for CNN development. In this work we challenge the notion that CNN architecture
design solely based on ImageNet leads to generally effective convolutional
neural network (CNN) architectures that perform well on a diverse set of
datasets and application domains. To this end, we investigate and ultimately
improve ImageNet as a basis for deriving such architectures. We conduct an
extensive empirical study for which we train $500$ CNN architectures, sampled
from the broad AnyNetX design space, on ImageNet as well as $8$ additional well
known image classification benchmark datasets from a diverse array of
application domains. We observe that the performances of the architectures are
highly dataset dependent. Some datasets even exhibit a negative error
correlation with ImageNet across all architectures. We show how to
significantly increase these correlations by utilizing ImageNet subsets
restricted to fewer classes. These contributions can have a profound impact on
the way we design future CNN architectures and help alleviate the tilt we see
currently in our community with respect to over-reliance on one dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ User-friendly introduction to PAC-Bayes bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11216v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11216v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Alquier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aggregated predictors are obtained by making a set of basic predictors vote
according to some weights, that is, to some probability distribution.
  Randomized predictors are obtained by sampling in a set of basic predictors,
according to some prescribed probability distribution.
  Thus, aggregated and randomized predictors have in common that they are not
defined by a minimization problem, but by a probability distribution on the set
of predictors. In statistical learning theory, there is a set of tools designed
to understand the generalization ability of such procedures: PAC-Bayesian or
PAC-Bayes bounds.
  Since the original PAC-Bayes bounds of D. McAllester, these tools have been
considerably improved in many directions (we will for example describe a
simplified version of the localization technique of O. Catoni that was missed
by the community, and later rediscovered as "mutual information bounds"). Very
recently, PAC-Bayes bounds received a considerable attention: for example there
was workshop on PAC-Bayes at NIPS 2017, "(Almost) 50 Shades of Bayesian
Learning: PAC-Bayesian trends and insights", organized by B. Guedj, F. Bach and
P. Germain. One of the reason of this recent success is the successful
application of these bounds to neural networks by G. Dziugaite and D. Roy.
  An elementary introduction to PAC-Bayes theory is still missing. This is an
attempt to provide such an introduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Reinforcement Learning Approach for Scheduling Problems With Improved
  Generalization Through Order Swapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Vivekanandan, Samuel Wirth, Patrick Karlbauer, Noah Klarmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scheduling of production resources (such as associating jobs to machines)
plays a vital role for the manufacturing industry not only for saving energy
but also for increasing the overall efficiency. Among the different job
scheduling problems, the JSSP is addressed in this work. JSSP falls into the
category of NP-hard COP, in which solving the problem through exhaustive search
becomes unfeasible. Simple heuristics such as FIFO, LPT and metaheuristics such
as Taboo search are often adopted to solve the problem by truncating the search
space. The viability of the methods becomes inefficient for large problem sizes
as it is either far from the optimum or time consuming. In recent years, the
research towards using DRL to solve COP has gained interest and has shown
promising results in terms of solution quality and computational efficiency. In
this work, we provide an novel approach to solve the JSSP examining the
objectives generalization and solution effectiveness using DRL. In particular,
we employ the PPO algorithm that adopts the policy-gradient paradigm that is
found to perform well in the constrained dispatching of jobs. We incorporated
an OSM in the environment to achieve better generalized learning of the
problem. The performance of the presented approach is analyzed in depth by
using a set of available benchmark instances and comparing our results with the
work of other groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Permutation Invariant Training for Universal Sound
  Separation <span class="chip">ICASSP-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilian Postolache, Jordi Pons, Santiago Pascual, Joan Serrà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal sound separation consists of separating mixes with arbitrary sounds
of different types, and permutation invariant training (PIT) is used to train
source agnostic models that do so. In this work, we complement PIT with
adversarial losses but find it challenging with the standard formulation used
in speech source separation. We overcome this challenge with a novel
I-replacement context-based adversarial loss, and by training with multiple
discriminators. Our experiments show that by simply improving the loss (keeping
the same model and dataset) we obtain a non-negligible improvement of 1.4 dB
SI-SNRi in the reverberant FUSS dataset. We also find adversarial PIT to be
effective at reducing spectral holes, ubiquitous in mask-based separation
models, which highlights the potential relevance of adversarial losses for
source separation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo page: http://jordipons.me/apps/adversarialPIT/, Accepted at
  ICASSP-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization of Convolutional Neural Network Using the Linearly
  Decreasing Weight Particle Swarm Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2001.05670v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2001.05670v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Serizawa, H. Fujita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network (CNN) is one of the most frequently used deep
learning techniques. Various forms of models have been proposed and im-proved
for learning at CNN. When learning with CNN, it is necessary to determine the
optimal hyperparameters. However, the number of hyperparameters is so large
that it is difficult to do it manually, so much research has been done on
automation. A method that uses metaheuristic algorithms is attracting attention
in research on hyperparameter optimization. Metaheuristic algorithms are
naturally inspired and include evolution strategies, genetic algorithms,
antcolony optimization and particle swarm optimization. In particular, particle
swarm optimization converges faster than genetic algorithms, and various models
have been proposed. In this paper, we pro-pose CNN hyperparameter optimization
with linearly decreasing weight particle swarm optimization (LDWPSO). In the
experiment, the MNIST data set and CIFAR-10 data set, which are often used as
benchmark data sets, are used. By opti-mizing CNN hyperparameters with LDWPSO,
learning the MNIST and CIFAR-10 datasets, we compare the accuracy with a
standard CNN based on LeNet-5. As a result, when using the MNIST dataset, the
baseline CNN is 94.02% at the 5th epoch, compared to 98.95% for LDWPSO CNN,
which improves accuracy. When using the CIFAR-10 dataset, the Baseline CNN is
28.07% at the 10th epoch, compared to 69.37% for the LDWPSO CNN, which greatly
improves accuracy. This paper is presented at the 36th Annual Conference of the
Japanese Society for Artificial In-telligence. The final version is available
at the following URL: https://doi.org/10.11517/pjsai.JSAI2022.0_2S4IS2b03
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is presented at the 36th Annual Conference of the Japanese
  Society for Artificial In-telligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MURANA: A Generic Framework for Stochastic Variance-Reduced Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Condat, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a generic variance-reduced algorithm, which we call MUltiple
RANdomized Algorithm (MURANA), for minimizing a sum of several smooth functions
plus a regularizer, in a sequential or distributed manner. Our method is
formulated with general stochastic operators, which allow us to model various
strategies for reducing the computational complexity. For example, MURANA
supports sparse activation of the gradients, and also reduction of the
communication load via compression of the update vectors. This versatility
allows MURANA to cover many existing randomization mechanisms within a unified
framework, which also makes it possible to design new methods as special cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3rd Annual Conference on Mathematical and Scientific Machine Learning
  (MSML), Aug. 2022. PMLR 190:81-96</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoxSRC 2022: The Fourth VoxCeleb Speaker Recognition Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesung Huh, Andrew Brown, Jee-weon Jung, Joon Son Chung, Arsha Nagrani, Daniel Garcia-Romero, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper summarises the findings from the VoxCeleb Speaker Recognition
Challenge 2022 (VoxSRC-22), which was held in conjunction with INTERSPEECH
2022. The goal of this challenge was to evaluate how well state-of-the-art
speaker recognition systems can diarise and recognise speakers from speech
obtained "in the wild". The challenge consisted of: (i) the provision of
publicly available speaker recognition and diarisation data from YouTube videos
together with ground truth annotation and standardised evaluation software; and
(ii) a public challenge and hybrid workshop held at INTERSPEECH 2022. We
describe the four tracks of our challenge along with the baselines, methods,
and results. We conclude with a discussion on the new domain-transfer focus of
VoxSRC-22, and on the progression of the challenge from the previous three
editions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online certification of preference-based fairness for <span class="highlight-title">persona</span>lized
  recommender systems <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.14527v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.14527v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virginie Do, Sam Corbett-Davies, Jamal Atif, Nicolas Usunier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are facing scrutiny because of their growing impact on
the opportunities we have access to. Current audits for fairness are limited to
coarse-grained parity assessments at the level of sensitive groups. We propose
to audit for envy-freeness, a more granular criterion aligned with individual
preferences: every user should prefer their recommendations to those of other
users. Since auditing for envy requires to estimate the preferences of users
beyond their existing recommendations, we cast the audit as a new pure
exploration problem in multi-armed bandits. We propose a sample-efficient
algorithm with theoretical guarantees that it does not deteriorate user
experience. We also study the trade-offs achieved on real-world recommendation
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EF-BV: A Unified Theory of Error Feedback and Variance Reduction
  Mechanisms for Biased and Unbiased Compression in Distributed Optimization <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.04180v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.04180v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Condat, Kai Yi, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In distributed or federated optimization and learning, communication between
the different computing units is often the bottleneck and gradient compression
is widely used to reduce the number of bits sent within each communication
round of iterative methods. There are two classes of compression operators and
separate algorithms making use of them. In the case of unbiased random
compressors with bounded variance (e.g., rand-k), the DIANA algorithm of
Mishchenko et al. (2019), which implements a variance reduction technique for
handling the variance introduced by compression, is the current state of the
art. In the case of biased and contractive compressors (e.g., top-k), the EF21
algorithm of Richt\'arik et al. (2021), which instead implements an
error-feedback mechanism, is the current state of the art. These two classes of
compression schemes and algorithms are distinct, with different analyses and
proof techniques. In this paper, we unify them into a single framework and
propose a new algorithm, recovering DIANA and EF21 as particular cases. Our
general approach works with a new, larger class of compressors, which has two
parameters, the bias and the variance, and includes unbiased and biased
compressors as particular cases. This allows us to inherit the best of the two
worlds: like EF21 and unlike DIANA, biased compressors, like top-k, whose good
performance in practice is recognized, can be used. And like DIANA and unlike
EF21, independent randomness at the compressors allows to mitigate the effects
of compression, with the convergence rate improving when the number of parallel
workers is large. This is the first time that an algorithm with all these
features is proposed. We prove its linear convergence under certain conditions.
Our approach takes a step towards better understanding of two so-far distinct
worlds of communication-efficient distributed learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Different Learning Styles for Improved <span class="highlight-title">Knowledge</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usma Niyaz, Deepti R. Bathula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning style refers to a type of training mechanism adopted by an
individual to gain new knowledge. As suggested by the VARK model, humans have
different learning preferences like visual, auditory, etc., for acquiring and
effectively processing information. Inspired by this concept, our work explores
the idea of mixed information sharing with model compression in the context of
Knowledge Distillation (KD) and Mutual Learning (ML). Unlike conventional
techniques that share the same type of knowledge with all networks, we propose
to train individual networks with different forms of information to enhance the
learning process. We formulate a combined KD and ML framework with one teacher
and two student networks that share or exchange information in the form of
predictions and feature maps. Our comprehensive experiments with benchmark
classification and segmentation datasets demonstrate that with 15% compression,
the ensemble performance of networks trained with diverse forms of knowledge
outperforms the conventional techniques both quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Consist</span>ent Attack: Universal Adversarial Perturbation on Embodied Vision
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05751v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05751v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Ying, You Qiaoben, Xinning Zhou, Hang Su, Wenbo Ding, Jianyong Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied agents in vision navigation coupled with deep neural networks have
attracted increasing attention. However, deep neural networks have been shown
vulnerable to malicious adversarial noises, which may potentially cause
catastrophic failures in Embodied Vision Navigation. Among different
adversarial noises, universal adversarial perturbations (UAP), i.e., a constant
image-agnostic perturbation applied on every input frame of the agent, play a
critical role in Embodied Vision Navigation since they are
computation-efficient and application-practical during the attack. However,
existing UAP methods ignore the system dynamics of Embodied Vision Navigation
and might be sub-optimal. In order to extend UAP to the sequential decision
setting, we formulate the disturbed environment under the universal noise
$\delta$, as a $\delta$-disturbed Markov Decision Process ($\delta$-MDP). Based
on the formulation, we analyze the properties of $\delta$-MDP and propose two
novel Consistent Attack methods, named Reward UAP and Trajectory UAP, for
attacking Embodied agents, which consider the dynamic of the MDP and calculate
universal noises by estimating the disturbed distribution and the disturbed Q
function. For various victim models, our Consistent Attack can cause a
significant drop in their performance in the PointGoal task in Habitat with
different datasets and different scenes. Extensive experimental results
indicate that there exist serious potential risks for applying Embodied Vision
Navigation methods to the real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaFed: Federated Learning among Federations with Cyclic <span class="highlight-title">Knowledge</span>
  Distillation for <span class="highlight-title">Persona</span>lized Healthcare <span class="chip">IJCAI'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqiang Chen, Wang Lu, Xin Qin, Jindong Wang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has attracted increasing attention to building models
without accessing the raw user data, especially in healthcare. In real
applications, different federations can seldom work together due to possible
reasons such as data heterogeneity and distrust/inexistence of the central
server. In this paper, we propose a novel framework called MetaFed to
facilitate trustworthy FL between different federations. MetaFed obtains a
personalized model for each federation without a central server via the
proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each
federation as a meta distribution and aggregates knowledge of each federation
in a cyclic manner. The training is split into two parts: common knowledge
accumulation and personalization. Comprehensive experiments on three benchmarks
demonstrate that MetaFed without a server achieves better accuracy compared to
state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the
baseline for PAMAP2) with fewer communication costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI'22 FTL workshop innovation award; Extended version (12 pages)
  with more experiments and extensions; code at
  https://github.com/microsoft/PersonalizedFL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Modeling of Mach-Zehnder Interferometer-based Optical Matrix
  Multipliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Cem, Siqi Yan, Yunhong Ding, Darko Zibar, Francesco Da Ros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photonic integrated circuits are facilitating the development of optical
neural networks, which have the potential to be both faster and more energy
efficient than their electronic counterparts since optical signals are
especially well-suited for implementing matrix multiplications. However,
accurate programming of photonic chips for optical matrix multiplication
remains a difficult challenge. Here, we describe both simple analytical models
and data-driven models for offline training of optical matrix multipliers. We
train and evaluate the models using experimental data obtained from a
fabricated chip featuring a Mach-Zehnder interferometer mesh implementing
3-by-3 matrix multiplication. The neural network-based models outperform the
simple physics-based models in terms of prediction error. Furthermore, the
neural network models are also able to predict the spectral variations in the
matrix weights for up to 100 frequency channels covering the C-band. The use of
neural network models for programming the chip for optical matrix
multiplication yields increased performance on multiple machine learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures, submitted to Jorunal of Lightwave Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Federated Learning Benchmark for Drug-Target Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Mittone, Filip Svoboda, Marco Aldinucci, Nicholas D. Lane, Pietro Lio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aggregating pharmaceutical data in the drug-target interaction (DTI) domain
has the potential to deliver life-saving breakthroughs. It is, however,
notoriously difficult due to regulatory constraints and commercial interests.
This work proposes the application of federated learning, which we argue to be
reconcilable with the industry's constraints, as it does not require sharing of
any information that would reveal the entities' data or any other high-level
summary of it. When used on a representative GraphDTA model and the KIBA
dataset it achieves up to 15% improved performance relative to the best
available non-privacy preserving alternative. Our extensive battery of
experiments shows that, unlike in other domains, the non-IID data distribution
in the DTI datasets does not deteriorate FL performance. Additionally, we
identify a material trade-off between the benefits of adding new data, and the
cost of adding more clients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical
  Development Patterns of Preterm Infants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Xue, Fan Wang, Yuanzhuo Zhu, Hui Li, Deyu Meng, Dinggang Shen, Chunfeng Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying reliable deep learning techniques in interdisciplinary applications
needs learned models to output accurate and ({even more importantly})
explainable predictions. Existing approaches typically explicate network
outputs in a post-hoc fashion, under an implicit assumption that faithful
explanations come from accurate predictions/classifications. We have an
opposite claim that explanations boost (or even determine) classification. That
is, end-to-end learning of explanation factors to augment discriminative
representation extraction could be a more intuitive strategy to inversely
assure fine-grained explainability, e.g., in those neuroimaging and
neuroscience studies with high-dimensional data containing noisy, redundant,
and task-irrelevant information. In this paper, we propose such an explainable
geometric deep network dubbed as NeuroExplainer, with applications to uncover
altered infant cortical development patterns associated with preterm birth.
Given fundamental cortical attributes as network input, our NeuroExplainer
adopts a hierarchical attention-decoding framework to learn fine-grained
attentions and respective discriminative representations to accurately
recognize preterm infants from term-born infants at term-equivalent age.
NeuroExplainer learns the hierarchical attention-decoding modules under
subject-level weak supervision coupled with targeted regularizers deduced from
domain knowledge regarding brain development. These prior-guided constraints
implicitly maximizes the explainability metrics (i.e., fidelity, sparsity, and
stability) in network training, driving the learned network to output detailed
explanations and accurate classifications. Experimental results on the public
dHCP benchmark suggest that NeuroExplainer led to quantitatively reliable
explanation results that are qualitatively consistent with representative
neuroimaging studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some parts of the thesis are still being revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fixed-budget online adaptive learning for physics-informed neural
  networks. Towards parameterized problem inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi Nguyen Khoa Nguyen, Thibault Dairay, Raphaël Meunier, Christophe Millet, Mathilde Mougeot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) have gained much attention in
various fields of engineering thanks to their capability of incorporating
physical laws into the models. PINNs integrate the physical constraints by
minimizing the partial differential equations (PDEs) residuals on a set of
collocation points. The distribution of these collocation points appears to
have a huge impact on the performance of PINNs and the assessment of the
sampling methods for these points is still an active topic. In this paper, we
propose a Fixed-Budget Online Adaptive Learning (FBOAL) method, which
decomposes the domain into sub-domains, for training collocation points based
on local maxima and local minima of the PDEs residuals. The effectiveness of
FBOAL is demonstrated for non-parameterized and parameterized problems. The
comparison with other adaptive sampling methods is also illustrated. The
numerical results demonstrate important gains in terms of the accuracy and
computational cost of PINNs with FBOAL over the classical PINNs with
non-adaptive collocation points. We also apply FBOAL in a complex industrial
application involving coupling between mechanical and thermal fields. We show
that FBOAL is able to identify the high-gradient locations and even give better
predictions for some physical fields than the classical PINNs with collocation
points sampled on a pre-adapted finite element mesh built thanks to numerical
expert knowledge. From the present study, it is expected that the use of FBOAL
will help to improve the conventional numerical solver in the construction of
the mesh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 30 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling-free Inference for Ab-Initio Potential Energy Surface Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Gao, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, it has been shown that neural networks not only approximate the
ground-state wave functions of a single molecular system well but can also
generalize to multiple geometries. While such generalization significantly
speeds up training, each energy evaluation still requires Monte Carlo
integration which limits the evaluation to a few geometries. In this work, we
address the inference shortcomings by proposing the Potential learning from
ab-initio Networks (PlaNet) framework, in which we simultaneously train a
surrogate model in addition to the neural wave function. At inference time, the
surrogate avoids expensive Monte-Carlo integration by directly estimating the
energy, accelerating the process from hours to milliseconds. In this way, we
can accurately model high-resolution multi-dimensional energy surfaces for
larger systems that previously were unobtainable via neural wave functions.
Finally, we explore an additional inductive bias by introducing
physically-motivated restricted neural wave function models. We implement such
a function with several additional improvements in the new PESNet++ model. In
our experimental evaluation, PlaNet accelerates inference by 7 orders of
magnitude for larger molecules like ethanol while preserving accuracy. Compared
to previous energy surface networks, PESNet++ reduces energy errors by up to
74%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Categorical Archive of <span class="highlight-title">ChatGPT</span> Failures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03494v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03494v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Borji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Double Descent via Smooth Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10080v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10080v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gamba, Erik Englesson, Mårten Björkman, Hossein Azizpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of overparameterized deep networks to interpolate noisy data,
while at the same time showing good generalization performance, has been
recently characterized in terms of the double descent curve for the test error.
Common intuition from polynomial regression suggests that overparameterized
networks are able to sharply interpolate noisy data, without considerably
deviating from the ground-truth signal, thus preserving their generalization
ability. At present, a precise characterization of the relationship between
interpolation and generalization for deep networks is missing. In this work, we
quantify sharpness of fit of the training data interpolated by neural network
functions, by studying the loss landscape w.r.t.\ to the input variable locally
to each training point, over volumes around cleanly- and noisily-labelled
training samples, as we systematically increase the number of model parameters
and training epochs. Our findings show that loss sharpness in the input space
follows both model- and epoch-wise double descent, with worse peaks observed
around noisy labels. While small interpolating models sharply fit both clean
and noisy data, large interpolating models express a smooth loss landscape,
where noisy targets are predicted over large volumes around training data
points, in contrast to existing intuition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Discretized Neural Networks under Ricci Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider Discretized Neural Networks (DNNs) consisting of
low-precision weights and activations, which suffer from either infinite or
zero gradients caused by the non-differentiable discrete function in the
training process. In this case, most training-based DNNs use the standard
Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete
values. However, the STE will cause the problem of gradient mismatch, which
implies that the approximated gradient is with perturbations. We propose an
analysis that this mismatch can be viewed as a metric perturbation in a
Riemannian manifold through the lens of duality theory. To address this
problem, based on the information geometry, we construct the Linearly Nearly
Euclidean (LNE) manifold for DNNs as a background to deal with perturbations.
By introducing a partial differential equation on metrics, the Ricci flow, we
prove the dynamical stability and convergence of the LNE metric with the
$L^2$-norm perturbation. And unlike the previous perturbation theory which
gives the rate of convergence is the fractional powers, we yield the metric
perturbation under the Ricci flow can be exponentially decayed in the LNE
manifold. The experimental results on various datasets demonstrate that our
method achieves better and more stable performance for DNNs than other
representative training-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Memorization Across Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07646v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07646v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
  We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes more complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ (Certified!!) Adversarial Robustness for Free! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we show how to achieve state-of-the-art certified adversarial
robustness to 2-norm bounded perturbations by relying exclusively on
off-the-shelf pretrained models. To do so, we instantiate the denoised
smoothing approach of Salman et al. 2020 by combining a pretrained denoising
diffusion probabilistic model and a standard high-accuracy classifier. This
allows us to certify 71% accuracy on ImageNet under adversarial perturbations
constrained to be within an 2-norm of 0.5, an improvement of 14 percentage
points over the prior certified SoTA using any approach, or an improvement of
30 percentage points over denoised smoothing. We obtain these results using
only pretrained diffusion models and image classifiers, without requiring any
fine tuning or retraining of model parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coverage-centric Coreset Selection for High Pruning Rates <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haizhong Zheng, Rui Liu, Fan Lai, Atul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-shot coreset selection aims to select a representative subset of the
training data, given a pruning rate, that can later be used to train future
models while retaining high accuracy. State-of-the-art coreset selection
methods pick the highest importance examples based on an importance metric and
are found to perform well at low pruning rates. However, at high pruning rates,
they suffer from a catastrophic accuracy drop, performing worse than even
random sampling. This paper explores the reasons behind this accuracy drop both
theoretically and empirically. We first propose a novel metric to measure the
coverage of a dataset on a specific distribution by extending the classical
geometric set cover problem to a distribution cover problem. This metric helps
explain why coresets selected by SOTA methods at high pruning rates perform
poorly compared to random sampling because of worse data coverage. We then
propose a novel one-shot coreset selection method, Coverage-centric Coreset
Selection (CCS), that jointly considers overall data coverage upon a
distribution as well as the importance of each example. We evaluate CCS on five
datasets and show that, at high pruning rates (e.g., 90%), it achieves
significantly better accuracy than previous SOTA methods (e.g., at least 19.56%
higher on CIFAR10) as well as random selection (e.g., 7.04% higher on CIFAR10)
and comparable accuracy at low pruning rates. We make our code publicly
available at
https://github.com/haizhongzheng/Coverage-centric-coreset-selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Point to Which Soft Actor-Critic Converges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft actor-critic is a successful successor over soft Q-learning. While lived
under maximum entropy framework, their relationship is still unclear. In this
paper, we prove that in the limit they converge to the same solution. This is
appealing since it translates the optimization from an arduous to an easier
way. The same justification can also be applied to other regularizers such as
KL divergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Switchable Representation Learning Framework with Self-compatibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengsen Wu, Yan Bai, Yihang Lou, Xiongkun Linghu, Jianzhong He, Ling-Yu Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world visual search systems involve deployments on multiple platforms
with different computing and storage resources. Deploying a unified model that
suits the minimal-constrain platforms leads to limited accuracy. It is expected
to deploy models with different capacities adapting to the resource
constraints, which requires features extracted by these models to be aligned in
the metric space. The method to achieve feature alignments is called
``compatible learning''. Existing research mainly focuses on the one-to-one
compatible paradigm, which is limited in learning compatibility among multiple
models. We propose a \textbf{S}witchable representation learning Framework with
Self-Compatibility (SFSC). SFSC generates a series of compatible sub-models
with different capacities through one training process. The optimization of
sub-models faces gradients conflict, and we mitigate this problem from the
perspective of the magnitude and direction. We adjust the priorities of
sub-models dynamically through uncertainty estimation to co-optimize sub-models
properly. Besides, the gradients with conflicting directions are projected to
avoid mutual interference. SFSC achieves state-of-the-art performance on the
evaluated datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Unifying Fourteen Attribution Methods with Taylor
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, Ziwei Yang, Zheyang Li, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various attribution methods have been developed to explain deep neural
networks (DNNs) by inferring the attribution/importance/contribution score of
each input variable to the final output. However, existing attribution methods
are often built upon different heuristics. There remains a lack of a unified
theoretical understanding of why these methods are effective and how they are
related. To this end, for the first time, we formulate core mechanisms of
fourteen attribution methods, which were designed on different heuristics, into
the same mathematical system, i.e., the system of Taylor interactions.
Specifically, we prove that attribution scores estimated by fourteen
attribution methods can all be reformulated as the weighted sum of two types of
effects, i.e., independent effects of each individual input variable and
interaction effects between input variables. The essential difference among the
fourteen attribution methods mainly lies in the weights of allocating different
effects. Based on the above findings, we propose three principles for a fair
allocation of effects to evaluate the faithfulness of the fourteen attribution
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Centric AI: Deep Generative Differentiable Feature Selection via
  Discrete Subsetting as Continuous Embedding Space Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Xiao, Dongjie Wang, Min Wu, Pengfei Wang, Yuanchun Zhou, Yanjie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature Selection (FS), such as filter, wrapper, and embedded methods, aims
to find the optimal feature subset for a given downstream task. However, in
many real-world practices, 1) the criteria of FS vary across domains; 2) FS is
brittle when data is a high-dimensional and small sample size. Can selected
feature subsets be more generalized, accurate, and input dimensionality
agnostic? We generalize this problem into a deep differentiable feature
selection task and propose a new perspective: discrete feature subsetting as
continuous embedding space optimization. We develop a generic and principled
framework including a deep feature subset encoder, accuracy evaluator, decoder,
and gradient ascent optimizer. This framework implements four steps: 1)
features-accuracy training data preparation; 2) deep feature subset embedding;
3) gradient-optimized search; 4) feature subset reconstruction. We develop new
technical insights: reinforcement as a training data generator, ensembles of
diverse peer and exploratory feature selector knowledge for generalization, an
effective embedding from feature subsets to continuous space along with joint
optimizing reconstruction and accuracy losses to select accurate features.
Experimental results demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-Cal: An optimal test for the calibration of predictive models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01850v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01850v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghwan Lee, Xinmeng Huang, Hamed Hassani, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prediction accuracy of machine learning methods is steadily increasing,
but the calibration of their uncertainty predictions poses a significant
challenge. Numerous works focus on obtaining well-calibrated predictive models,
but less is known about reliably assessing model calibration. This limits our
ability to know when algorithms for improving calibration have a real effect,
and when their improvements are merely artifacts due to random noise in finite
datasets. In this work, we consider detecting mis-calibration of predictive
models using a finite validation dataset as a hypothesis testing problem. The
null hypothesis is that the predictive model is calibrated, while the
alternative hypothesis is that the deviation from calibration is sufficiently
large.
  We find that detecting mis-calibration is only possible when the conditional
probabilities of the classes are sufficiently smooth functions of the
predictions. When the conditional class probabilities are H\"older continuous,
we propose T-Cal, a minimax optimal test for calibration based on a debiased
plug-in estimator of the $\ell_2$-Expected Calibration Error (ECE). We further
propose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We
verify our theoretical findings with a broad range of experiments, including
with several popular deep neural net architectures and several standard
post-hoc calibration methods. T-Cal is a practical general-purpose tool, which
-- combined with classical tests for discrete-valued predictors -- can be used
to test the calibration of virtually any probabilistic classification method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The implementation of T-Cal is available at
  https://github.com/dh7401/T-Cal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Li, Jun Cao, Jiawei Zhu, Qinyao Luo, Silu He, Xuyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretasks are mainly built on mutual information estimation, which
requires data augmentation to construct positive samples with similar semantics
to learn invariant signals and negative samples with dissimilar semantics in
order to empower representation discriminability. However, an appropriate data
augmentation configuration depends heavily on lots of empirical trials such as
choosing the compositions of data augmentation techniques and the corresponding
hyperparameter settings. We propose an augmentation-free graph contrastive
learning method, invariant-discriminative graph contrastive learning (iGCL),
that does not intrinsically require negative samples. iGCL designs the
invariant-discriminative loss (ID loss) to learn invariant and discriminative
representations. On the one hand, ID loss learns invariant signals by directly
minimizing the mean square error between the target samples and positive
samples in the representation space. On the other hand, ID loss ensures that
the representations are discriminative by an orthonormal constraint forcing the
different dimensions of representations to be independent of each other. This
prevents representations from collapsing to a point or subspace. Our
theoretical analysis explains the effectiveness of ID loss from the
perspectives of the redundancy reduction criterion, canonical correlation
analysis, and information bottleneck principle. The experimental results
demonstrate that iGCL outperforms all baselines on 5 node classification
benchmark datasets. iGCL also shows superior performance for different label
ratios and is capable of resisting graph attacks, which indicates that iGCL has
excellent generalization and robustness. The source code is available at
https://github.com/lehaifeng/T-GCN/tree/master/iGCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages 8 figs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Backdoor Defense via Suppressing Model Shortcuts <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Yang, Yiming Li, Yong Jiang, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that deep neural networks (DNNs) are
vulnerable to backdoor attacks during the training process. Specifically, the
adversaries intend to embed hidden backdoors in DNNs so that malicious model
predictions can be activated through pre-defined trigger patterns. In this
paper, we explore the backdoor mechanism from the angle of the model structure.
We select the skip connection for discussions, inspired by the understanding
that it helps the learning of model `shortcuts' where backdoor triggers are
usually easier to be learned. Specifically, we demonstrate that the attack
success rate (ASR) decreases significantly when reducing the outputs of some
key skip connections. Based on this observation, we design a simple yet
effective backdoor removal method by suppressing the skip connections in
critical layers selected by our method. We also implement fine-tuning on these
layers to recover high benign accuracy and to further reduce ASR. Extensive
experiments on benchmark datasets verify the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICASSP 2023. 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BATT: Backdoor Attack with Transformation-based Triggers <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xu, Yiming Li, Yong Jiang, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are vulnerable to backdoor attacks. The backdoor
adversaries intend to maliciously control the predictions of attacked DNNs by
injecting hidden backdoors that can be activated by adversary-specified trigger
patterns during the training process. One recent research revealed that most of
the existing attacks failed in the real physical world since the trigger
contained in the digitized test samples may be different from that of the one
used for training. Accordingly, users can adopt spatial transformations as the
image pre-processing to deactivate hidden backdoors. In this paper, we explore
the previous findings from another side. We exploit classical spatial
transformations (i.e. rotation and translation) with the specific parameter as
trigger patterns to design a simple yet effective poisoning-based backdoor
attack. For example, only images rotated to a particular angle can activate the
embedded backdoor of attacked DNNs. Extensive experiments are conducted,
verifying the effectiveness of our attack under both digital and physical
settings and its resistance to existing backdoor defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICASSP 2023. 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Untargeted Backdoor Attack against Object Detection <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxiao Luo, Yiming Li, Yong Jiang, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies revealed that deep neural networks (DNNs) are exposed to
backdoor threats when training with third-party resources (such as training
samples or backbones). The backdoored model has promising performance in
predicting benign samples, whereas its predictions can be maliciously
manipulated by adversaries based on activating its backdoors with pre-defined
trigger patterns. Currently, most of the existing backdoor attacks were
conducted on the image classification under the targeted manner. In this paper,
we reveal that these threats could also happen in object detection, posing
threatening risks to many mission-critical applications ($e.g.$, pedestrian
detection and intelligent surveillance systems). Specifically, we design a
simple yet effective poison-only backdoor attack in an untargeted manner, based
on task characteristics. We show that, once the backdoor is embedded into the
target model by our attack, it can trick the model to lose detection of any
object stamped with our trigger patterns. We conduct extensive experiments on
the benchmark dataset, showing its effectiveness in both digital and
physical-world settings and its resistance to potential defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICASSP 2023. 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothness Analysis of Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.01400v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.01400v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sekitoshi Kanai, Masanori Yamada, Hiroshi Takahashi, Yuki Yamanaka, Yasutoshi Ida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are vulnerable to adversarial attacks. Recent studies
about adversarial robustness focus on the loss landscape in the parameter space
since it is related to optimization and generalization performance. These
studies conclude that the difficulty of adversarial training is caused by the
non-smoothness of the loss function: i.e., its gradient is not Lipschitz
continuous. However, this analysis ignores the dependence of adversarial
attacks on model parameters. Since adversarial attacks are optimized for
models, they should depend on the parameters. Considering this dependence, we
analyze the smoothness of the loss function of adversarial training using the
optimal attacks for the model parameter in more detail. We reveal that the
constraint of adversarial attacks is one cause of the non-smoothness and that
the smoothness depends on the types of the constraints. Specifically, the
$L_\infty$ constraint can cause non-smoothness more than the $L_2$ constraint.
Moreover, our analysis implies that if we flatten the loss function with
respect to input data, the Lipschitz constant of the gradient of adversarial
loss tends to increase. To address the non-smoothness, we show that EntropySGD
smoothens the non-smooth loss and improves the performance of adversarial
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The latest version of this article is published in IEEE Transactions
  on Neural Networks and Learning Systems (DOI: 10.1109/TNNLS.2023.3244172). 22
  pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Timothy Bennett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  If $A$ and $B$ are sets such that $A \subset B$, generalisation may be
understood as the inference from $A$ of a hypothesis sufficient to construct
$B$. One might infer any number of hypotheses from $A$, yet only some of those
may generalise to $B$. How can one know which are likely to generalise? One
strategy is to choose the shortest, equating the ability to compress
information with the ability to generalise (a proxy for intelligence). We
examine this in the context of a mathematical formalism of enactive cognition.
We show that compression is neither necessary nor sufficient to maximise
performance (measured in terms of the probability of a hypothesis
generalising). We formulate a proxy unrelated to length or simplicity, called
weakness. We show that if tasks are uniformly distributed, then there is no
choice of proxy that performs at least as well as weakness maximisation in all
tasks while performing strictly better in at least one. In other words,
weakness is the pareto optimal choice of proxy. In experiments comparing
maximum weakness and minimum description length in the context of binary
arithmetic, the former generalised at between $1.1$ and $5$ times the rate of
the latter. We argue this demonstrates that weakness is a far better proxy, and
explains why Deepmind's Apperception Engine is able to generalise effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising diffusion models for out-of-distribution detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07740v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07740v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark S. Graham, Walter H. L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution detection is crucial to the safe deployment of machine
learning systems. Currently, unsupervised out-of-distribution detection is
dominated by generative-based approaches that make use of estimates of the
likelihood or other measurements from a generative model. Reconstruction-based
methods offer an alternative approach, in which a measure of reconstruction
error is used to determine if a sample is out-of-distribution. However,
reconstruction-based approaches are less favoured, as they require careful
tuning of the model's information bottleneck - such as the size of the latent
dimension - to produce good results. In this work, we exploit the view of
denoising diffusion probabilistic models (DDPM) as denoising autoencoders where
the bottleneck is controlled externally, by means of the amount of noise
applied. We propose to use DDPMs to reconstruct an input that has been noised
to a range of noise levels, and use the resulting multi-dimensional
reconstruction error to classify out-of-distribution inputs. We validate our
approach both on standard computer-vision datasets and on higher dimension
medical datasets. Our approach outperforms not only reconstruction-based
methods, but also state-of-the-art generative-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Hyena Hierarchy: Towards Larger Convolutional Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, <span class="highlight-author">Yoshua Bengio</span>, Stefano Ermon, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have relied heavily on the use of large
Transformers due to their ability to learn at scale. However, the core building
block of Transformers, the attention operator, exhibits quadratic cost in
sequence length, limiting the amount of context accessible. Existing
subquadratic methods based on low-rank and sparse approximations need to be
combined with dense attention layers to match Transformers, indicating a gap in
capability. In this work, we propose Hyena, a subquadratic drop-in replacement
for attention constructed by interleaving implicitly parametrized long
convolutions and data-controlled gating. In recall and reasoning tasks on
sequences of thousands to hundreds of thousands of tokens, Hyena improves
accuracy by more than 50 points over operators relying on state-spaces and
other implicit and explicit methods, matching attention-based models. We set a
new state-of-the-art for dense-attention-free architectures on language
modeling in standard datasets (WikiText103 and The Pile), reaching Transformer
quality with a 20% reduction in training compute required at sequence length
2K. Hyena operators are twice as fast as highly optimized attention at sequence
length 8K, and 100x faster at sequence length 64K.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Additional results (PG-19, LAMBADA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Explicit Credit Assignment for Cooperative Multi-Agent
  Reinforcement Learning via Polarization Policy Gradient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wubing Chen, Wenbin Li, Xiao Liu, Shangdong Yang, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative multi-agent policy gradient (MAPG) algorithms have recently
attracted wide attention and are regarded as a general scheme for the
multi-agent system. Credit assignment plays an important role in MAPG and can
induce cooperation among multiple agents. However, most MAPG algorithms cannot
achieve good credit assignment because of the game-theoretic pathology known as
\textit{centralized-decentralized mismatch}. To address this issue, this paper
presents a novel method, \textit{\underline{M}ulti-\underline{A}gent
\underline{P}olarization \underline{P}olicy \underline{G}radient} (MAPPG).
MAPPG takes a simple but efficient polarization function to transform the
optimal consistency of joint and individual actions into easily realized
constraints, thus enabling efficient credit assignment in MAPG. Theoretically,
we prove that individual policies of MAPPG can converge to the global optimum.
Empirically, we evaluate MAPPG on the well-known matrix game and differential
game, and verify that MAPPG can converge to the global optimum for both
discrete and continuous action spaces. We also evaluate MAPPG on a set of
StarCraft II micromanagement tasks and demonstrate that MAPPG outperforms the
state-of-the-art MAPG algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Unpooling Layer for Graph <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinglong Guo, Dongmian Zou, Gilad Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel and trainable graph unpooling layer for effective graph
generation. Given a graph with features, the unpooling layer enlarges this
graph and learns its desired new structure and features. Since this unpooling
layer is trainable, it can be applied to graph generation either in the decoder
of a variational autoencoder or in the generator of a generative adversarial
network (GAN). We prove that the unpooled graph remains connected and any
connected graph can be sequentially unpooled from a 3-nodes graph. We apply the
unpooling layer within the GAN generator. Since the most studied instance of
graph generation is molecular generation, we test our ideas in this context.
Using the QM9 and ZINC datasets, we demonstrate the improvement obtained by
using the unpooling layer instead of an adjacency-matrix-based approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepStruct: <span class="highlight-title">Pretrain</span>ing of Language Models for Structure Prediction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for improving the structural understanding abilities of
language models. Unlike previous approaches that finetune the models with
task-specific augmentation, we pretrain language models on a collection of
task-agnostic corpora to generate structures from text. Our structure
pretraining enables zero-shot transfer of the learned knowledge that models
have about the structure tasks. We study the performance of this approach on 28
datasets, spanning 10 structure prediction tasks including open information
extraction, joint entity and relation extraction, named entity recognition,
relation classification, semantic role labeling, event extraction, coreference
resolution, factual probe, intent detection, and dialogue state tracking. We
further enhance the pretraining with the task-specific training sets. We show
that a 10B parameter language model transfers non-trivially to most tasks and
obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benford's law: what does it say on adversarial images? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.04615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.04615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João G. Zago, Fabio L. Baldissera, Eric A. Antonelo, Rodrigo T. Saad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are fragile to small perturbations in
the input images. These networks are thus prone to malicious attacks that
perturb the inputs to force a misclassification. Such slightly manipulated
images aimed at deceiving the classifier are known as adversarial images. In
this work, we investigate statistical differences between natural images and
adversarial ones. More precisely, we show that employing a proper image
transformation and for a class of adversarial attacks, the distribution of the
leading digit of the pixels in adversarial images deviates from Benford's law.
The stronger the attack, the more distant the resulting distribution is from
Benford's law. Our analysis provides a detailed investigation of this new
approach that can serve as a basis for alternative adversarial example
detection methods that do not need to modify the original CNN classifier
neither work on the raw high-dimensional pixels as features to defend against
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReLOAD: Reinforcement Learning with Optimistic Ascent-Descent for
  Last-Iterate Convergence in Constrained MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Moskovitz, Brendan O'Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh, Tom Zahavy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Reinforcement Learning (RL) has been applied to real-world
problems with increasing success. Such applications often require to put
constraints on the agent's behavior. Existing algorithms for constrained RL
(CRL) rely on gradient descent-ascent, but this approach comes with a caveat.
While these algorithms are guaranteed to converge on average, they do not
guarantee last-iterate convergence, i.e., the current policy of the agent may
never converge to the optimal solution. In practice, it is often observed that
the policy alternates between satisfying the constraints and maximizing the
reward, rarely accomplishing both objectives simultaneously. Here, we address
this problem by introducing Reinforcement Learning with Optimistic
Ascent-Descent (ReLOAD), a principled CRL method with guaranteed last-iterate
convergence. We demonstrate its empirical effectiveness on a wide variety of
CRL problems including discrete MDPs and continuous control. In the process we
establish a benchmark of challenging CRL problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Series Pattern Recognition in Smart Manufacturing Systems: A
  Literature <span class="highlight-title">Review</span> and Ontology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba A. Farahani, M. R. McCormick, Robert Gianinny, Frank Hudacheck, Ramy Harik, Zhichao Liu, Thorsten Wuest
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the inception of Industry 4.0 in 2012, emerging technologies have
enabled the acquisition of vast amounts of data from diverse sources such as
machine tools, robust and affordable sensor systems with advanced information
models, and other sources within Smart Manufacturing Systems (SMS). As a
result, the amount of data that is available in manufacturing settings has
exploded, allowing data-hungry tools such as Artificial Intelligence (AI) and
Machine Learning (ML) to be leveraged. Time-series analytics has been
successfully applied in a variety of industries, and that success is now being
migrated to pattern recognition applications in manufacturing to support higher
quality products, zero defect manufacturing, and improved customer
satisfaction. However, the diverse landscape of manufacturing presents a
challenge for successfully solving problems in industry using time-series
pattern recognition. The resulting research gap of understanding and applying
the subject matter of time-series pattern recognition in manufacturing is a
major limiting factor for adoption in industry. The purpose of this paper is to
provide a structured perspective of the current state of time-series pattern
recognition in manufacturing with a problem-solving focus. By using an ontology
to classify and define concepts, how they are structured, their properties, the
relationships between them, and considerations when applying them, this paper
aims to provide practical and actionable guidelines for application and
recommendations for advancing time-series analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 35 figures, 21 tables, submitted to Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Dependencies in Feature Importance for Time Series Predictions <span class="chip">ICLR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.14317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.14317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kin Kwan Leung, Clayton Rooke, Jonathan Smith, Saba Zuberi, Maksims Volkovs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series data introduces two key challenges for explainability methods:
firstly, observations of the same feature over subsequent time steps are not
independent, and secondly, the same feature can have varying importance to
model predictions over time. In this paper, we propose Windowed Feature
Importance in Time (WinIT), a feature removal based explainability approach to
address these issues. Unlike existing feature removal explanation methods,
WinIT explicitly accounts for the temporal dependence between different
observations of the same feature in the construction of its importance score.
Furthermore, WinIT captures the varying importance of a feature over time, by
summarizing its importance over a window of past time steps. We conduct an
extensive empirical study on synthetic and real-world data, compare against a
wide range of leading explainability methods, and explore the impact of various
evaluation strategies. Our results show that WinIT achieves significant gains
over existing methods, with more consistent performance across different
evaluation metrics. The code for our work is publicly available at
\url{https://github.com/layer6ai-labs/WinIT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations 2023 (ICLR'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much Space Has Been Explored? Measuring the Chemical Space Covered
  by Databases and Machine-<span class="highlight-title">Generate</span>d Molecules <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12542v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12542v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xie, Ziqiao Xu, Jiaqi Ma, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forming a molecular candidate set that contains a wide range of potentially
effective compounds is crucial to the success of drug discovery. While most
databases and machine-learning-based generation models aim to optimize
particular chemical properties, there is limited literature on how to properly
measure the coverage of the chemical space by those candidates included or
generated. This problem is challenging due to the lack of formal criteria to
select good measures of the chemical space. In this paper, we propose a novel
evaluation framework for measures of the chemical space based on two analyses:
an axiomatic analysis with three intuitive axioms that a good measure should
obey, and an empirical analysis on the correlation between a measure and a
proxy gold standard. Using this framework, we are able to identify #Circles, a
new measure of chemical space coverage, which is superior to existing measures
both analytically and empirically. We further evaluate how well the existing
databases and generation models cover the chemical space in terms of #Circles.
The results suggest that many generation models fail to explore a larger space
over existing databases, which leads to new opportunities for improving
generation models by encouraging exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap to Real-World Object-Centric Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, Francesco Locatello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally decompose their environment into entities at the appropriate
level of abstraction to act in the world. Allowing machine learning algorithms
to derive this decomposition in an unsupervised way has become an important
line of research. However, current methods are restricted to simulated data or
require additional information in the form of motion or depth in order to
successfully discover objects. In this work, we overcome this limitation by
showing that reconstructing features from models trained in a self-supervised
manner is a sufficient training signal for object-centric representations to
arise in a fully unsupervised way. Our approach, DINOSAUR, significantly
out-performs existing image-based object-centric learning models on simulated
data and is the first unsupervised object-centric model that scales to
real-world datasets such as COCO and PASCAL VOC. DINOSAUR is conceptually
simple and shows competitive performance compared to more involved pipelines
from the computer vision literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafan He, Heyang Zhao, Dongruo Zhou, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study reinforcement learning (RL) with linear function approximation. For
episodic time-inhomogeneous linear Markov decision processes (linear MDPs)
whose transition dynamic can be parameterized as a linear function of a given
feature mapping, we propose the first computationally efficient algorithm that
achieves the nearly minimax optimal regret $\tilde O(d\sqrt{H^3K})$, where $d$
is the dimension of the feature mapping, $H$ is the planning horizon, and $K$
is the number of episodes. Our algorithm is based on a weighted linear
regression scheme with a carefully designed weight, which depends on a new
variance estimator that (1) directly estimates the variance of the
\emph{optimal} value function, (2) monotonically decreases with respect to the
number of episodes to ensure a better estimation accuracy, and (3) uses a
rare-switching policy to update the value function estimator to control the
complexity of the estimated value function class. Our work provides a complete
answer to optimal RL with linear MDPs, and the developed algorithm and
theoretical tools may be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scale up with Order: Finding Good Data Permutations for Distributed
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Guo, Khiem Pham, Yucheng Lu, Tiancheng Yuan, Charlie F. Ruan, Christopher De Sa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient Balancing (GraB) is a recently proposed technique that finds
provably better data permutations when training models with multiple epochs
over a finite dataset. It converges at a faster rate than the widely adopted
Random Reshuffling, by minimizing the discrepancy of the gradients on
adjacently selected examples. However, GraB only operates under critical
assumptions such as small batch sizes and centralized data, leaving open the
question of how to order examples at large scale -- i.e. distributed learning
with decentralized data. To alleviate the limitation, in this paper we propose
D-GraB, an algorithm that orders the examples in a parallel setting with
negligible overhead, which enjoys linear speed up at rate
$\tilde{O}((mnT)^{-2/3})$ on smooth non-convex objectives and
$\tilde{O}((mnT)^{-2})$ under PL condition, where $n$ denotes the number of
parallel workers, $m$ denotes the number of examples per worker and $T$ denotes
the number of epochs. D-GraB benefits from both data ordering and parallelism.
Empirically, we show on various applications including GLUE, CIFAR10 and
WikiText-2 that D-GraB outperforms naive parallel GraB and Distributed Random
Reshuffling in terms of both training and validation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Subset Selection for Weak Supervision <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hunter Lang, Aravindan Vijayaraghavan, David Sontag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing weak supervision approaches use all the data covered by weak signals
to train a classifier. We show both theoretically and empirically that this is
not always optimal. Intuitively, there is a tradeoff between the amount of
weakly-labeled data and the precision of the weak labels. We explore this
tradeoff by combining pretrained data representations with the cut statistic
(Muhlenbach et al., 2004) to select (hopefully) high-quality subsets of the
weakly-labeled training data. Subset selection applies to any label model and
classifier and is very simple to plug in to existing weak supervision
pipelines, requiring just a few lines of code. We show our subset selection
method improves the performance of weak supervision for a wide range of label
models, classifiers, and datasets. Using less weakly-labeled data improves the
accuracy of weak supervision pipelines by up to 19% (absolute) on benchmark
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighborhood Contrastive <span class="highlight-title">Transformer</span> for Change Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbin Tu, Liang Li, Li Su, Ke Lu, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change captioning is to describe the semantic change between a pair of
similar images in natural language. It is more challenging than general image
captioning, because it requires capturing fine-grained change information while
being immune to irrelevant viewpoint changes, and solving syntax ambiguity in
change descriptions. In this paper, we propose a neighborhood contrastive
transformer to improve the model's perceiving ability for various changes under
different scenes and cognition ability for complex syntax structure.
Concretely, we first design a neighboring feature aggregating to integrate
neighboring context into each feature, which helps quickly locate the
inconspicuous changes under the guidance of conspicuous referents. Then, we
devise a common feature distilling to compare two images at neighborhood level
and extract common properties from each image, so as to learn effective
contrastive information between them. Finally, we introduce the explicit
dependencies between words to calibrate the transformer decoder, which helps
better understand complex syntax structure during training. Extensive
experimental results demonstrate that the proposed method achieves the
state-of-the-art performance on three public datasets with different change
scenarios. The code is available at https://github.com/tuyunbin/NCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPA-CLIP: Integrating Phonetic Priors into Vision and Language
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihaya Matsuhira, Marc A. Kastner, Takahiro Komamizu, Takatsugu Hirayama, Keisuke Doman, Yasutomo Kawanishi, Ichiro Ide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale Vision and Language (V\&L) pretraining has become the
standard backbone of many multimedia systems. While it has shown remarkable
performance even in unseen situations, it often performs in ways not intuitive
to humans. Particularly, they usually do not consider the pronunciation of the
input, which humans would utilize to understand language, especially when it
comes to unknown words. Thus, this paper inserts phonetic prior into
Contrastive Language-Image Pretraining (CLIP), one of the V\&L pretrained
models, to make it consider the pronunciation similarity among its
pronunciation inputs. To achieve this, we first propose a phoneme embedding
that utilizes the phoneme relationships provided by the International Phonetic
Alphabet (IPA) chart as a phonetic prior. Next, by distilling the frozen CLIP
text encoder, we train a pronunciation encoder employing the IPA-based
embedding. The proposed model named IPA-CLIP comprises this pronunciation
encoder and the original CLIP encoders (image and text). Quantitative
evaluation reveals that the phoneme distribution on the embedding space
represents phonetic relationships more accurately when using the proposed
phoneme embedding. Furthermore, in some multimodal retrieval tasks, we confirm
that the proposed pronunciation encoder enhances the performance of the text
encoder and that the pronunciation encoder handles nonsense words in a more
phonetic manner than the text encoder. Finally, qualitative evaluation verifies
the correlation between the pronunciation encoder and human perception
regarding pronunciation similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Question Answering Using CLIP-Guided Visual-Text Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Ye, Weikai Kong, Chenglin Yao, Jianfeng Ren, Xudong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal learning of video and text plays a key role in Video Question
Answering (VideoQA). In this paper, we propose a visual-text attention
mechanism to utilize the Contrastive Language-Image Pre-training (CLIP) trained
on lots of general domain language-image pairs to guide the cross-modal
learning for VideoQA. Specifically, we first extract video features using a
TimeSformer and text features using a BERT from the target application domain,
and utilize CLIP to extract a pair of visual-text features from the
general-knowledge domain through the domain-specific learning. We then propose
a Cross-domain Learning to extract the attention information between visual and
linguistic features across the target domain and general domain. The set of
CLIP-guided visual-text features are integrated to predict the answer. The
proposed method is evaluated on MSVD-QA and MSRVTT-QA datasets, and outperforms
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence-based Event-centric Online Video Question Answering on a
  Newly Constructed ATBS <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Kong, Shuhong Ye, Chenglin Yao, Jianfeng Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks facilitate video question answering (VideoQA), but the
real-world applications on video streams such as CCTV and live cast place
higher demands on the solver. To address the challenges of VideoQA on long
videos of unknown length, we define a new set of problems called Online
Open-ended Video Question Answering (O^2VQA). It requires an online
state-updating mechanism for the solver to decide if the collected information
is sufficient to conclude an answer. We then propose a Confidence-based
Event-centric Online Video Question Answering (CEO-VQA) model to solve this
problem. Furthermore, a dataset called Answer Target in Background Stream
(ATBS) is constructed to evaluate this newly developed online VideoQA
application. Compared to the baseline VideoQA method that watches the whole
video, the experimental results show that the proposed method achieves a
significant performance gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Butterfly: Multiple Reference Frames Feature Propagation Mechanism for
  Neural Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Wang, Haihang Ruan, Fei Xiong, Jiayu Yang, Litian Li, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using more reference frames can significantly improve the compression
efficiency in neural video compression. However, in low-latency scenarios, most
existing neural video compression frameworks usually use the previous one frame
as reference. Or a few frameworks which use the previous multiple frames as
reference only adopt a simple multi-reference frames propagation mechanism. In
this paper, we present a more reasonable multi-reference frames propagation
mechanism for neural video compression, called butterfly multi-reference frame
propagation mechanism (Butterfly), which allows a more effective feature fusion
of multi-reference frames. By this, we can generate more accurate temporal
context conditional prior for Contextual Coding Module. Besides, when the
number of decoded frames does not meet the required number of reference frames,
we duplicate the nearest reference frame to achieve the requirement, which is
better than duplicating the furthest one. Experiment results show that our
method can significantly outperform the previous state-of-the-art (SOTA), and
our neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when
compares with our base single-reference frame model with the same compression
configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DCC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compose & Embellish: Well-Structured Piano Performance <span class="highlight-title">Generation</span> via A
  Two-Stage Approach <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Lun Wu, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even with strong sequence models like Transformers, generating expressive
piano performances with long-range musical structures remains challenging.
Meanwhile, methods to compose well-structured melodies or lead sheets (melody +
chords), i.e., simpler forms of music, gained more success. Observing the
above, we devise a two-stage Transformer-based framework that Composes a lead
sheet first, and then Embellishes it with accompaniment and expressive touches.
Such a factorization also enables pretraining on non-piano data. Our objective
and subjective experiments show that Compose & Embellish shrinks the gap in
structureness between a current state of the art and real performances by half,
and improves other musical aspects such as richness and coherence as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perfectly Secure Steganography Using Minimum Entropy Coupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Schroeder de Witt, Samuel Sokota, J. Zico Kolter, Jakob Foerster, Martin Strohmeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganography is the practice of encoding secret information into innocuous
content in such a manner that an adversarial third party would not realize that
there is hidden meaning. While this problem has classically been studied in
security literature, recent advances in generative models have led to a shared
interest among security and machine learning researchers in developing scalable
steganography techniques. In this work, we show that a steganography procedure
is perfectly secure under \citet{cachin_perfect}'s information theoretic-model
of steganography if and only if it is induced by a coupling. Furthermore, we
show that, among perfectly secure procedures, a procedure is maximally
efficient if and only if it is induced by a minimum entropy coupling. These
insights yield what are, to the best of our knowledge, the first steganography
algorithms to achieve perfect security guarantees with non-trivial efficiency;
additionally, these algorithms are highly scalable. To provide empirical
validation, we compare a minimum entropy coupling-based approach to three
modern baselines -- arithmetic coding, Meteor, and adaptive dynamic grouping --
using GPT-2 and WaveRNN as communication channels. We find that the minimum
entropy coupling-based approach yields superior encoding efficiency, despite
its stronger security constraints. In aggregate, these results suggest that it
may be natural to view information-theoretic steganography through the lens of
minimum entropy coupling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiViz: Towards Visualizing and Understanding Multimodal Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The promise of multimodal models for real-world applications has inspired
research in visualizing and understanding their internal mechanics with the end
goal of empowering stakeholders to visualize model behavior, perform model
debugging, and promote trust in machine learning models. However, modern
multimodal models are typically black-box neural networks, which makes it
challenging to understand their internal mechanics. How can we visualize the
internal modeling of multimodal interactions in these models? Our paper aims to
fill this gap by proposing MultiViz, a method for analyzing the behavior of
multimodal models by scaffolding the problem of interpretability into 4 stages:
(1) unimodal importance: how each modality contributes towards downstream
modeling and prediction, (2) cross-modal interactions: how different modalities
relate with each other, (3) multimodal representations: how unimodal and
cross-modal interactions are represented in decision-level features, and (4)
multimodal prediction: how decision-level features are composed to make a
prediction. MultiViz is designed to operate on diverse modalities, models,
tasks, and research areas. Through experiments on 8 trained models across 6
real-world tasks, we show that the complementary stages in MultiViz together
enable users to (1) simulate model predictions, (2) assign interpretable
concepts to features, (3) perform error analysis on model misclassifications,
and (4) use insights from error analysis to debug models. MultiViz is publicly
available, will be regularly updated with new interpretation tools and metrics,
and welcomes inputs from the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Code available at: https://github.com/pliang279/MultiViz</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of a noisy hyperlink removal system: A semantic and
  relatedness approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazem Taghandiki, Elnaz Rezaei Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the volume of data on the web grows, the web structure graph, which is a
graph representation of the web, continues to evolve. The structure of this
graph has gradually shifted from content-based to non-content-based.
Furthermore, spam data, such as noisy hyperlinks, in the web structure graph
adversely affect the speed and efficiency of information retrieval and link
mining algorithms. Previous works in this area have focused on removing noisy
hyperlinks using structural and string approaches. However, these approaches
may incorrectly remove useful links or be unable to detect noisy hyperlinks in
certain circumstances. In this paper, a data collection of hyperlinks is
initially constructed using an interactive crawler. The semantic and
relatedness structure of the hyperlinks is then studied through semantic web
approaches and tools such as the DBpedia ontology. Finally, the removal process
of noisy hyperlinks is carried out using a reasoner on the DBpedia ontology.
Our experiments demonstrate the accuracy and ability of semantic web
technologies to remove noisy hyperlinks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AmQA: Amharic Question Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tilahun Abedissa, Ricardo Usbeck, Yaregal Assabie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) returns concise answers or answer lists from natural
language text given a context document. Many resources go into curating QA
datasets to advance robust models' development. There is a surge of QA datasets
for languages like English, however, this is not true for Amharic. Amharic, the
official language of Ethiopia, is the second most spoken Semitic language in
the world. There is no published or publicly available Amharic QA dataset.
Hence, to foster the research in Amharic QA, we present the first Amharic QA
(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia
articles. Additionally, we run an XLMR Large-based baseline model to spark
open-domain QA research interest. The best-performing baseline achieves an
F-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension
settings respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongEval-Retrieval: French-English Dynamic Test Collection for
  Continuous Web Search Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petra Galuščáková, Romain Deveaud, Gabriela Gonzalez-Saez, Philippe Mulhem, Lorraine Goeuriot, Florina Piroi, Martin Popel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LongEval-Retrieval is a Web document retrieval benchmark that focuses on
continuous retrieval evaluation. This test collection is intended to be used to
study the temporal persistence of Information Retrieval systems and will be
used as the test collection in the Longitudinal Evaluation of Model Performance
Track (LongEval) at CLEF 2023. This benchmark simulates an evolving information
system environment - such as the one a Web search engine operates in - where
the document collection, the query distribution, and relevance all move
continuously, while following the Cranfield paradigm for offline evaluation. To
do that, we introduce the concept of a dynamic test collection that is composed
of successive sub-collections each representing the state of an information
system at a given time step. In LongEval-Retrieval, each sub-collection
contains a set of queries, documents, and soft relevance assessments built from
click models. The data comes from Qwant, a privacy-preserving Web search engine
that primarily focuses on the French market. LongEval-Retrieval also provides a
'mirror' collection: it is initially constructed in the French language to
benefit from the majority of Qwant's traffic, before being translated to
English. This paper presents the creation process of LongEval-Retrieval and
provides baseline runs and analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MABNet: Master Assistant Buddy Network with Hybrid Learning for Image
  Retrieval <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Agarwal, Gyanendra Das, Saksham Aggarwal, Alexander Horsch, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval has garnered growing interest in recent times. The current
approaches are either supervised or self-supervised. These methods do not
exploit the benefits of hybrid learning using both supervision and
self-supervision. We present a novel Master Assistant Buddy Network (MABNet)
for image retrieval which incorporates both learning mechanisms. MABNet
consists of master and assistant blocks, both learning independently through
supervision and collectively via self-supervision. The master guides the
assistant by providing its knowledge base as a reference for self-supervision
and the assistant reports its knowledge back to the master by weight transfer.
We perform extensive experiments on public datasets with and without
post-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Fair Item Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Ao Sun, Sikha Pentyala, Martine De Cock, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users worldwide access massive amounts of curated data in the form of
rankings on a daily basis. The societal impact of this ease of access has been
studied and work has been done to propose and enforce various notions of
fairness in rankings. Current computational methods for fair item ranking rely
on disclosing user data to a centralized server, which gives rise to privacy
concerns for the users. This work is the first to advance research at the
conjunction of producer (item) fairness and consumer (user) privacy in rankings
by exploring the incorporation of privacy-preserving techniques; specifically,
differential privacy and secure multi-party computation. Our work extends the
equity of amortized attention ranking mechanism to be privacy-preserving, and
we evaluate its effects with respect to privacy, fairness, and ranking quality.
Our results using real-world datasets show that we are able to effectively
preserve the privacy of users and mitigate unfairness of items without making
additional sacrifices to the quality of rankings in comparison to the ranking
mechanism in the clear.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Feedback Attention Framework via Boundary-Aware Auxiliary and
  Progressive Semantic Optimization for Salient Object Detection in Optical
  Remote Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dejun Feng, Hongyu Chen, Suning Liu, Xingyu Shen, Ziyang Liao, Yakun Xie, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient object detection in optical remote sensing image (ORSI-SOD) has
gradually attracted attention thanks to the development of deep learning (DL)
and salient object detection in natural scene image (NSI-SOD). However, NSI and
ORSI are different in many aspects, such as large coverage, complex background,
and large differences in target types and scales. Therefore, a new dedicated
method is needed for ORSI-SOD. In addition, existing methods do not pay
sufficient attention to the boundary of the object, and the completeness of the
final saliency map still needs improvement. To address these issues, we propose
a novel method called Dual Feedback Attention Framework via Boundary-Aware
Auxiliary and Progressive Semantic Optimization (DFA-BASO). First, Boundary
Protection Calibration (BPC) module is proposed to reduce the loss of edge
position information during forward propagation and suppress noise in low-level
features. Second, a Dual Feature Feedback Complementary (DFFC) module is
proposed based on BPC module. It aggregates boundary-semantic dual features and
provides effective feedback to coordinate features across different layers.
Finally, a Strong Semantic Feedback Refinement (SSFR) module is proposed to
obtain more complete saliency maps. This module further refines feature
representation and eliminates feature differences through a unique feedback
mechanism. Extensive experiments on two public datasets show that DFA-BASO
outperforms 15 state-of-the-art methods. Furthermore, this paper strongly
demonstrates the true contribution of DFA-BASO to ORSI-SOD by in-depth analysis
of the visualization figure. All codes can be found at
https://github.com/YUHsss/DFA-BASO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Incremental Update for Neural Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Zhang, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RS) aim to provide personalized suggestions of items for
users against consumer over-choice. Although extensive research has been
conducted to address different aspects and challenges of RS, there still exists
a gap between academic research and industrial applications. Specifically, most
of the existing models still work in an offline manner, in which the
recommender is trained on a large static training set and evaluated on a very
restrictive testing set in a one-time process. RS will stay unchanged until the
next batch retrain is performed. We frame such RS as Batch Update Recommender
Systems (BURS). In reality, they have to face the challenges where RS are
expected to be instantly updated with new data streaming in, and generate
updated recommendations for current user activities based on the newly arrived
data. We frame such RS as Incremental Update Recommender Systems (IURS).
  In this article, we offer a systematic survey of incremental update for
neural recommender systems. We begin the survey by introducing key concepts and
formulating the task of IURS. We then illustrate the challenges in IURS
compared with traditional BURS. Afterwards, we detail the introduction of
existing literature and evaluation issues. We conclude the survey by outlining
some prominent open research issues in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Auditing Methodologies Can Impact Our Understanding of YouTube's
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarmad Chandio, Daniyal Pirwani Dar, Rishab Nithyanand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data generated by audits of social media websites have formed the basis of
our understanding of the biases presented in algorithmic content recommendation
systems. As legislators around the world are beginning to consider regulating
the algorithmic systems that drive online platforms, it is critical to ensure
the correctness of these inferred biases. However, as we will show in this
paper, doing so is a challenging task for a variety of reasons related to the
complexity of configuration parameters associated with the audits that gather
data from a specific platform.
  Focusing specifically on YouTube, we show that conducting audits to make
inferences about YouTube's recommendation systems is more methodologically
challenging than one might expect. There are many methodological decisions that
need to be considered in order to obtain scientifically valid results, and each
of these decisions incur costs. For example, should an auditor use (expensive
to obtain) logged-in YouTube accounts while gathering recommendations from the
algorithm to obtain more accurate inferences? We explore the impact of this and
many other decisions and make some startling discoveries about the
methodological choices that impact YouTube's recommendations. Taken all
together, our research suggests auditing configuration compromises that YouTube
auditors and researchers can use to reduce audit overhead, both economically
and computationally, without sacrificing accuracy of their inferences.
Similarly, we also identify several configuration parameters that have a
significant impact on the accuracy of measured inferences and should be
carefully considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Recommend Using Non-Uniform Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanning Chen, Mohsen Bayati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning user preferences for products based on their past purchases or
reviews is at the cornerstone of modern recommendation engines. One
complication in this learning task is that some users are more likely to
purchase products or review them, and some products are more likely to be
purchased or reviewed by the users. This non-uniform pattern degrades the power
of many existing recommendation algorithms, as they assume that the observed
data are sampled uniformly at random among user-product pairs. In addition,
existing literature on modeling non-uniformity either assume user interests are
independent of the products, or lack theoretical understanding. In this paper,
we first model the user-product preferences as a partially observed matrix with
non-uniform observation pattern. Next, building on the literature about
low-rank matrix estimation, we introduce a new weighted trace-norm penalized
regression to predict unobserved values of the matrix. We then prove an upper
bound for the prediction error of our proposed approach. Our upper bound is a
function of a number of parameters that are based on a certain weight matrix
that depends on the joint distribution of users and products. Utilizing this
observation, we introduce a new optimization problem to select a weight matrix
that minimizes the upper bound on the prediction error. The final product is a
new estimator, NU-Recommend, that outperforms existing methods in both
synthetic and real datasets. Our approach aims at accurate predictions for all
users while prioritizing fairness. To achieve this, we employ a bias-variance
tradeoff mechanism that ensures good overall prediction performance without
compromising the predictive accuracy for less active users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a survey of the state of the art of hybrid language
models architectures and strategies for "complex" question-answering (QA, CQA,
CPS). Very large language models are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, tasks, methods,
sensitive data, performance, human approval and versatile feedback... This
survey extends findings from the robust community edited research papers BIG,
BLOOM and HELM which open source, benchmark and analyze limits and challenges
of large language models in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). It identifies the key
elements used with Large Language Models (LLM) to solve complex questions or
problems. Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of language models in complex QA. Hybridizing these models with
different components could allow to overcome these different limits and go much
further. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form QA,
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, QA explainability and truthfulness, time dimension.
Therefore we review current solutions and promising strategies, using elements
such as hybrid LLM architectures, human-in-the-loop reinforcement learning,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, and others. We analyze existing solutions and provide an
overview of the current research and trends in the area of complex QA.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-03-14T05:24:33.308511067Z">
            2023-03-14 05:24:33 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
